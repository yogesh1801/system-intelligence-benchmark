diff --git a/ostd/Cargo.toml b/ostd/Cargo.toml
index b740a6e..689986e 100644
--- a/ostd/Cargo.toml
+++ b/ostd/Cargo.toml
@@ -76,6 +76,9 @@ default = ["cvm_guest"]
 # The guest OS support for Confidential VMs (CVMs), e.g., Intel TDX
 cvm_guest = ["dep:tdx-guest", "dep:iced-x86"]
 coverage = ["minicov"]
+# TLA+ trace generation for consistency verification
+tla-trace = []
 
 [lints]
 workspace = true
+
diff --git a/ostd/src/lib.rs b/ostd/src/lib.rs
index a20a2b7..9326fbe 100644
--- a/ostd/src/lib.rs
+++ b/ostd/src/lib.rs
@@ -48,6 +48,7 @@ pub mod prelude;
 pub mod smp;
 pub mod sync;
 pub mod task;
+
 pub mod timer;
 pub mod trap;
 pub mod user;
diff --git a/ostd/src/sync/mod.rs b/ostd/src/sync/mod.rs
index a61bea2..17d8303 100644
--- a/ostd/src/sync/mod.rs
+++ b/ostd/src/sync/mod.rs
@@ -4,17 +4,23 @@
 
 mod guard;
 mod mutex;
+mod mutex_trace;
 mod rcu;
 mod rwarc;
 mod rwlock;
 mod rwmutex;
+mod rwmutex_trace;
 mod spin;
+mod spin_trace;
+#[cfg(ktest)]
+mod spin_trace_tests;
 mod wait;
 
 pub(crate) use self::rcu::finish_grace_period;
 pub use self::{
     guard::{GuardTransfer, LocalIrqDisabled, PreemptDisabled, SpinGuardian, WriteIrqDisabled},
     mutex::{ArcMutexGuard, Mutex, MutexGuard},
+    mutex_trace::{ArcMutexTraceGuard, MutexTrace, MutexTraceGuard},
     rcu::{non_null, Rcu, RcuDrop, RcuOption, RcuOptionReadGuard, RcuReadGuard},
     rwarc::{RoArc, RwArc},
     rwlock::{
@@ -25,7 +31,12 @@ pub use self::{
         ArcRwMutexReadGuard, ArcRwMutexUpgradeableGuard, ArcRwMutexWriteGuard, RwMutex,
         RwMutexReadGuard, RwMutexUpgradeableGuard, RwMutexWriteGuard,
     },
+    rwmutex_trace::{
+        ArcRwMutexTraceReadGuard, ArcRwMutexTraceUpgradeableGuard, ArcRwMutexTraceWriteGuard, RwMutexTrace,
+        RwMutexTraceReadGuard, RwMutexTraceUpgradeableGuard, RwMutexTraceWriteGuard,
+    },
     spin::{ArcSpinLockGuard, SpinLock, SpinLockGuard},
+    spin_trace::{ArcSpinTraceGuard, SpinTrace, SpinTraceGuard},
     wait::{WaitQueue, Waiter, Waker},
 };
 
diff --git a/ostd/src/sync/mutex_trace.rs b/ostd/src/sync/mutex_trace.rs
new file mode 100644
index 0000000..a0c8402
--- /dev/null
+++ b/ostd/src/sync/mutex_trace.rs
@@ -0,0 +1,434 @@
+// SPDX-License-Identifier: MPL-2.0
+
+//! TLA+ trace instrumented Mutex implementation for validation
+
+use alloc::sync::Arc;
+use core::{
+    cell::UnsafeCell,
+    fmt,
+    ops::{Deref, DerefMut},
+    sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering},
+};
+
+use super::WaitQueue;
+
+/// Atomic sequence counter for trace events
+static TRACE_SEQUENCE: AtomicU64 = AtomicU64::new(0);
+
+fn trace_event(action: &str, mutex_addr: usize, actor_id: usize, mutex_state: bool) {
+    if crate::IN_BOOTSTRAP_CONTEXT.load(core::sync::atomic::Ordering::Relaxed) {
+        return;
+    }
+    
+    let seq = TRACE_SEQUENCE.fetch_add(1, core::sync::atomic::Ordering::Relaxed);
+    
+    // Enhanced JSON output with thread ID and mutex state
+    let json = b"{\"seq\":";
+    for &b in json { unsafe { crate::arch::serial::send(b); } }
+    
+    // Output sequence number (support up to 100 operations)
+    let seq_val = seq % 100;
+    if seq_val >= 10 {
+        unsafe { crate::arch::serial::send(b'0' + (seq_val / 10) as u8); }
+    }
+    unsafe { crate::arch::serial::send(b'0' + (seq_val % 10) as u8); }
+    
+    // Add thread/actor ID
+    let thread_part = b",\"thread\":";
+    for &b in thread_part { unsafe { crate::arch::serial::send(b); } }
+    unsafe { crate::arch::serial::send(b'0' + (actor_id % 10) as u8); }
+    
+    // Add mutex address (use fixed mutex ID since we only have one mutex)
+    let mutex_part = b",\"mutex\":";
+    for &b in mutex_part { unsafe { crate::arch::serial::send(b); } }
+    // Since we only have one mutex, always use mutex ID 0
+    let mutex_id = 0u8;
+    unsafe { crate::arch::serial::send(b'0' + mutex_id); }
+    
+    // Add mutex state
+    let state_part = b",\"state\":\"";
+    for &b in state_part { unsafe { crate::arch::serial::send(b); } }
+    if mutex_state {
+        let locked = b"locked";
+        for &b in locked { unsafe { crate::arch::serial::send(b); } }
+    } else {
+        let unlocked = b"unlocked";
+        for &b in unlocked { unsafe { crate::arch::serial::send(b); } }
+    }
+    
+    let action_part = b"\",\"action\":\"";
+    for &b in action_part { unsafe { crate::arch::serial::send(b); } }
+    
+    // Output action
+    for &b in action.as_bytes() { unsafe { crate::arch::serial::send(b); } }
+    
+    // Add actor field (same as thread for compatibility)
+    let actor_part = b"\",\"actor\":";
+    for &b in actor_part { unsafe { crate::arch::serial::send(b); } }
+    unsafe { crate::arch::serial::send(b'0' + (actor_id % 10) as u8); }
+    
+    let end = b"}\n";
+    for &b in end { unsafe { crate::arch::serial::send(b); } }
+}
+
+fn get_current_actor_id() -> usize {
+    // For testing, we need to explicitly pass thread IDs
+    // This function is a placeholder that will be replaced with explicit IDs
+    0  // Default to thread 0, will be overridden in test-specific versions
+}
+
+/// A mutex with TLA+ tracing instrumentation.
+pub struct MutexTrace<T: ?Sized> {
+    lock: AtomicBool,
+    queue: WaitQueue,
+    val: UnsafeCell<T>,
+}
+
+impl<T> MutexTrace<T> {
+    /// Creates a new mutex.
+    pub const fn new(val: T) -> Self {
+        Self {
+            lock: AtomicBool::new(false),
+            queue: WaitQueue::new(),
+            val: UnsafeCell::new(val),
+        }
+    }
+}
+
+impl<T: ?Sized> MutexTrace<T> {
+    /// Acquires the mutex.
+    ///
+    /// This method runs in a block way until the mutex can be acquired.
+    #[track_caller]
+    pub fn lock(&self) -> MutexTraceGuard<T> {
+        self.lock_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Acquires the mutex with explicit thread ID (for testing).
+    #[track_caller]
+    pub fn lock_with_thread_id(&self, thread_id: usize) -> MutexTraceGuard<T> {
+        let mutex_addr = self as *const Self as *const () as usize;
+        
+        let guard = self.queue.wait_until(|| {
+            if self.acquire_lock() {
+                Some(unsafe { MutexTraceGuard::new_with_thread_id(self, thread_id) })
+            } else {
+                None
+            }
+        });
+        
+        // Record successful lock (after execution)
+        trace_event("Lock", mutex_addr, thread_id, true);
+        guard
+    }
+
+    /// Acquires the mutex through an [`Arc`].
+    ///
+    /// The method is similar to [`lock`], but it doesn't have the requirement
+    /// for compile-time checked lifetimes of the mutex guard.
+    ///
+    /// [`lock`]: Self::lock
+    #[track_caller]
+    pub fn lock_arc(self: &Arc<Self>) -> ArcMutexTraceGuard<T> {
+        self.lock_arc_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Acquires the mutex through an [`Arc`] with explicit thread ID.
+    #[track_caller]
+    pub fn lock_arc_with_thread_id(self: &Arc<Self>, thread_id: usize) -> ArcMutexTraceGuard<T> {
+        let mutex_addr = Arc::as_ptr(self) as *const () as usize;
+        
+        let guard = self.queue.wait_until(|| {
+            if self.acquire_lock() {
+                Some(MutexTraceGuard_ {
+                    mutex: self.clone(),
+                    thread_id,
+                })
+            } else {
+                None
+            }
+        });
+        
+        // Record successful lock (after execution)
+        trace_event("Lock", mutex_addr, thread_id, true);
+        guard
+    }
+
+    /// Tries acquire the mutex immediately.
+    pub fn try_lock(&self) -> Option<MutexTraceGuard<T>> {
+        self.try_lock_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Tries acquire the mutex with explicit thread ID (for testing).
+    pub fn try_lock_with_thread_id(&self, thread_id: usize) -> Option<MutexTraceGuard<T>> {
+        let mutex_addr = self as *const Self as *const () as usize;
+        
+        let success = self.acquire_lock();
+        
+        if success {
+            // Record successful try_lock (after execution)
+            trace_event("TryLock", mutex_addr, thread_id, true);
+            // SAFETY: The lock is successfully acquired when creating the guard.
+            Some(unsafe { MutexTraceGuard::new_with_thread_id(self, thread_id) })
+        } else {
+            None
+        }
+    }
+
+    /// Tries acquire the mutex through an [`Arc`].
+    ///
+    /// The method is similar to [`try_lock`], but it doesn't have the requirement
+    /// for compile-time checked lifetimes of the mutex guard.
+    ///
+    /// [`try_lock`]: Self::try_lock
+    pub fn try_lock_arc(self: &Arc<Self>) -> Option<ArcMutexTraceGuard<T>> {
+        self.try_lock_arc_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Tries acquire the mutex through an [`Arc`] with explicit thread ID.
+    pub fn try_lock_arc_with_thread_id(self: &Arc<Self>, thread_id: usize) -> Option<ArcMutexTraceGuard<T>> {
+        let mutex_addr = Arc::as_ptr(self) as *const () as usize;
+        
+        let success = self.acquire_lock();
+        
+        if success {
+            // Record successful try_lock (after execution)
+            trace_event("TryLock", mutex_addr, thread_id, true);
+            Some(ArcMutexTraceGuard {
+                mutex: self.clone(),
+                thread_id,
+            })
+        } else {
+            None
+        }
+    }
+
+    /// Returns a mutable reference to the underlying data.
+    ///
+    /// This method is zero-cost: By holding a mutable reference to the lock, the compiler has
+    /// already statically guaranteed that access to the data is exclusive.
+    pub fn get_mut(&mut self) -> &mut T {
+        self.val.get_mut()
+    }
+
+    /// Releases the mutex and wake up one thread which is blocked on this mutex.
+    fn unlock(&self, thread_id: usize) {
+        let mutex_addr = self as *const Self as *const () as usize;
+        
+        self.release_lock();
+        self.queue.wake_one();
+        
+        // Record successful unlock (after execution)
+        trace_event("Unlock", mutex_addr, thread_id, false);
+    }
+
+    fn acquire_lock(&self) -> bool {
+        self.lock
+            .compare_exchange(false, true, Ordering::Acquire, Ordering::Relaxed)
+            .is_ok()
+    }
+
+    fn release_lock(&self) {
+        self.lock.store(false, Ordering::Release);
+    }
+    
+    // Helper methods for tracing
+    fn try_acquire_lock_for_tracing(&self) -> Option<MutexTraceGuard<T>> {
+        self.try_acquire_lock_for_tracing_with_id(get_current_actor_id())
+    }
+    
+    fn try_acquire_lock_for_tracing_with_id(&self, thread_id: usize) -> Option<MutexTraceGuard<T>> {
+        self.acquire_lock()
+            .then(|| unsafe { MutexTraceGuard::new_with_thread_id(self, thread_id) })
+    }
+    
+    fn try_lock_arc_for_tracing(self: &Arc<Self>) -> Option<ArcMutexTraceGuard<T>> {
+        self.try_lock_arc_for_tracing_with_id(get_current_actor_id())
+    }
+    
+    fn try_lock_arc_for_tracing_with_id(self: &Arc<Self>, thread_id: usize) -> Option<ArcMutexTraceGuard<T>> {
+        self.acquire_lock().then(|| ArcMutexTraceGuard {
+            mutex: self.clone(),
+            thread_id,
+        })
+    }
+}
+
+impl<T: ?Sized + fmt::Debug> fmt::Debug for MutexTrace<T> {
+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
+        fmt::Debug::fmt(&self.val, f)
+    }
+}
+
+unsafe impl<T: ?Sized + Send> Send for MutexTrace<T> {}
+unsafe impl<T: ?Sized + Send> Sync for MutexTrace<T> {}
+
+#[clippy::has_significant_drop]
+#[must_use]
+pub struct MutexTraceGuard_<T: ?Sized, R: Deref<Target = MutexTrace<T>>> {
+    mutex: R,
+    thread_id: usize,
+}
+
+/// A guard that provides exclusive access to the data protected by a [`MutexTrace`].
+pub type MutexTraceGuard<'a, T> = MutexTraceGuard_<T, &'a MutexTrace<T>>;
+
+impl<'a, T: ?Sized> MutexTraceGuard<'a, T> {
+    /// # Safety
+    ///
+    /// The caller must ensure that the given reference of [`MutexTrace`] lock has been successfully acquired
+    /// in the current context. When the created [`MutexTraceGuard`] is dropped, it will unlock the [`MutexTrace`].
+    unsafe fn new(mutex: &'a MutexTrace<T>) -> MutexTraceGuard<'a, T> {
+        MutexTraceGuard_ { mutex, thread_id: get_current_actor_id() }
+    }
+    
+    /// # Safety
+    ///
+    /// Same as new() but with explicit thread ID.
+    unsafe fn new_with_thread_id(mutex: &'a MutexTrace<T>, thread_id: usize) -> MutexTraceGuard<'a, T> {
+        MutexTraceGuard_ { mutex, thread_id }
+    }
+}
+
+/// A guard that provides exclusive access to the data protected by a `Arc<MutexTrace>`.
+pub type ArcMutexTraceGuard<T> = MutexTraceGuard_<T, Arc<MutexTrace<T>>>;
+
+impl<T: ?Sized, R: Deref<Target = MutexTrace<T>>> Deref for MutexTraceGuard_<T, R> {
+    type Target = T;
+
+    fn deref(&self) -> &Self::Target {
+        unsafe { &*self.mutex.val.get() }
+    }
+}
+
+impl<T: ?Sized, R: Deref<Target = MutexTrace<T>>> DerefMut for MutexTraceGuard_<T, R> {
+    fn deref_mut(&mut self) -> &mut Self::Target {
+        unsafe { &mut *self.mutex.val.get() }
+    }
+}
+
+impl<T: ?Sized, R: Deref<Target = MutexTrace<T>>> Drop for MutexTraceGuard_<T, R> {
+    fn drop(&mut self) {
+        self.mutex.unlock(self.thread_id);
+    }
+}
+
+impl<T: ?Sized + fmt::Debug, R: Deref<Target = MutexTrace<T>>> fmt::Debug for MutexTraceGuard_<T, R> {
+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
+        fmt::Debug::fmt(&**self, f)
+    }
+}
+
+impl<T: ?Sized, R: Deref<Target = MutexTrace<T>>> !Send for MutexTraceGuard_<T, R> {}
+
+unsafe impl<T: ?Sized + Sync, R: Deref<Target = MutexTrace<T>> + Sync> Sync for MutexTraceGuard_<T, R> {}
+
+impl<'a, T: ?Sized> MutexTraceGuard<'a, T> {
+    pub fn get_lock(guard: &MutexTraceGuard<'a, T>) -> &'a MutexTrace<T> {
+        guard.mutex
+    }
+}
+
+#[cfg(ktest)]
+mod test {
+    use super::*;
+    use crate::prelude::*;
+    use alloc::sync::Arc;
+    use alloc::vec::Vec;
+    
+    // Simple random number generator for test scenarios
+    fn simple_rand(seed: &mut u32) -> u32 {
+        *seed = seed.wrapping_mul(1664525).wrapping_add(1013904223);
+        *seed
+    }
+
+    #[ktest]
+    fn test_mutex_trace() {
+        crate::early_println!("=== Starting 20 MutexTrace TLA+ Traces: Single Mutex, 3-Thread Contention ===");
+        
+        // Generate 20 different trace scenarios, each with single mutex and 3-thread contention
+        for i in 1..=20 {
+            TRACE_SEQUENCE.store(0, core::sync::atomic::Ordering::Relaxed);
+            let shared_mutex = Arc::new(MutexTrace::<u32>::new(i * 10));
+            
+            crate::early_println!("TRACE_{}: Created single shared mutex with initial value {}", i, i * 10);
+            
+            // Use simple random seed based on trace number
+            let mut seed = (i * 7919) as u32;
+            
+            // Generate different contention patterns based on trace number
+            let pattern = i % 4;
+            let op_count = 4 + (simple_rand(&mut seed) % 4); // 4-7 operations per trace
+            
+            for op_idx in 0..op_count {
+                let thread_id = (simple_rand(&mut seed) % 3) as usize; // Only threads 0, 1, 2
+                let operation_type = match pattern {
+                    1 => simple_rand(&mut seed) % 2, // Basic: mostly blocking locks
+                    2 => simple_rand(&mut seed) % 4, // Try-lock heavy
+                    3 => simple_rand(&mut seed) % 3, // Mixed pattern
+                    _ => simple_rand(&mut seed) % 4, // Complex
+                };
+                
+                match operation_type {
+                    0 => { // Blocking lock
+                        let mut guard = shared_mutex.lock_with_thread_id(thread_id);
+                        let increment = (simple_rand(&mut seed) % 20) + 1;
+                        *guard += increment;
+                        crate::early_println!("TRACE_{} Op{}: Thread{} blocking lock, incremented by {}, value now {}", 
+                            i, op_idx + 1, thread_id, increment, *guard);
+                        drop(guard);
+                    },
+                    1 => { // Try lock
+                        if let Some(mut guard) = shared_mutex.try_lock_with_thread_id(thread_id) {
+                            let increment = (simple_rand(&mut seed) % 15) + 5;
+                            *guard += increment;
+                            crate::early_println!("TRACE_{} Op{}: Thread{} try-lock SUCCESS, incremented by {}, value now {}", 
+                                i, op_idx + 1, thread_id, increment, *guard);
+                            drop(guard);
+                        } else {
+                            crate::early_println!("TRACE_{} Op{}: Thread{} try-lock FAILED (contention)", 
+                                i, op_idx + 1, thread_id);
+                        }
+                    },
+                    2 => { // Read operation
+                        let guard = shared_mutex.lock_with_thread_id(thread_id);
+                        let value = *guard;
+                        crate::early_println!("TRACE_{} Op{}: Thread{} read mutex value: {}", 
+                            i, op_idx + 1, thread_id, value);
+                        drop(guard);
+                    },
+                    _ => { // Try then fallback to blocking
+                        if let Some(mut guard) = shared_mutex.try_lock_with_thread_id(thread_id) {
+                            let decrement = (simple_rand(&mut seed) % 5) + 1;
+                            *guard = guard.saturating_sub(decrement);
+                            crate::early_println!("TRACE_{} Op{}: Thread{} try-lock SUCCESS, decremented by {}, value now {}", 
+                                i, op_idx + 1, thread_id, decrement, *guard);
+                            drop(guard);
+                        } else {
+                            crate::early_println!("TRACE_{} Op{}: Thread{} try-lock failed, falling back to blocking", 
+                                i, op_idx + 1, thread_id);
+                            let mut guard = shared_mutex.lock_with_thread_id(thread_id);
+                            let increment = (simple_rand(&mut seed) % 10) + 1;
+                            *guard += increment;
+                            crate::early_println!("TRACE_{} Op{}: Thread{} blocking fallback SUCCESS, incremented by {}, value now {}", 
+                                i, op_idx + 1, thread_id, increment, *guard);
+                            drop(guard);
+                        }
+                    }
+                }
+            }
+            
+            // Final state check for this trace
+            let final_guard = shared_mutex.lock_with_thread_id(0);
+            let final_value = *final_guard;
+            drop(final_guard);
+            crate::early_println!("--- TRACE_{} Summary: Final value {} ---", i, final_value);
+            
+            let final_seq = TRACE_SEQUENCE.load(core::sync::atomic::Ordering::Relaxed);
+            crate::early_println!("--- TRACE_{} Complete: {} events with single mutex, 3-thread contention ---", i, final_seq);
+            crate::early_println!("");
+        }
+        
+        crate::early_println!("=== All 20 Single-Mutex 3-Thread Contention Traces Generated ===");
+    }
+}
\ No newline at end of file
diff --git a/ostd/src/sync/rwmutex_trace.rs b/ostd/src/sync/rwmutex_trace.rs
new file mode 100644
index 0000000..c902814
--- /dev/null
+++ b/ostd/src/sync/rwmutex_trace.rs
@@ -0,0 +1,732 @@
+// SPDX-License-Identifier: MPL-2.0
+
+//! TLA+ trace instrumented RwMutex implementation for validation
+
+// Note: Arc is imported in test module where it's actually used
+use core::{
+    cell::UnsafeCell,
+    fmt,
+    ops::{Deref, DerefMut},
+    sync::atomic::{
+        AtomicUsize, AtomicU64,
+        Ordering::{AcqRel, Acquire, Relaxed, Release},
+    },
+};
+
+use super::WaitQueue;
+
+/// Atomic sequence counter for trace events
+static TRACE_SEQUENCE: AtomicU64 = AtomicU64::new(0);
+
+fn trace_event(action: &str, rwmutex_addr: usize, actor_id: usize, rwmutex_state: &str, lock_type: &str) {
+    if crate::IN_BOOTSTRAP_CONTEXT.load(core::sync::atomic::Ordering::Relaxed) {
+        return;
+    }
+    
+    let seq = TRACE_SEQUENCE.fetch_add(1, core::sync::atomic::Ordering::Relaxed);
+    
+    // Enhanced JSON output with thread ID and rwmutex state
+    let json = b"{\"seq\":";
+    for &b in json { unsafe { crate::arch::serial::send(b); } }
+    
+    // Output sequence number (support up to 1000 operations)
+    let seq_val = seq % 1000;
+    if seq_val >= 100 {
+        unsafe { crate::arch::serial::send(b'0' + (seq_val / 100) as u8); }
+    }
+    if seq_val >= 10 {
+        unsafe { crate::arch::serial::send(b'0' + ((seq_val / 10) % 10) as u8); }
+    }
+    unsafe { crate::arch::serial::send(b'0' + (seq_val % 10) as u8); }
+    
+    // Add thread/actor ID
+    let thread_part = b",\"thread\":";
+    for &b in thread_part { unsafe { crate::arch::serial::send(b); } }
+    unsafe { crate::arch::serial::send(b'0' + (actor_id % 10) as u8); }
+    
+    // Add rwmutex address (use fixed rwmutex ID since we only have one rwmutex)
+    let rwmutex_part = b",\"rwmutex\":";
+    for &b in rwmutex_part { unsafe { crate::arch::serial::send(b); } }
+    // Since we only have one rwmutex, always use rwmutex ID 0
+    let rwmutex_id = 0u8;
+    unsafe { crate::arch::serial::send(b'0' + rwmutex_id); }
+    
+    // Add rwmutex state
+    let state_part = b",\"state\":\"";
+    for &b in state_part { unsafe { crate::arch::serial::send(b); } }
+    for &b in rwmutex_state.as_bytes() { unsafe { crate::arch::serial::send(b); } }
+    
+    // Add lock type (read, write, upread)
+    let lock_type_part = b"\",\"lock_type\":\"";
+    for &b in lock_type_part { unsafe { crate::arch::serial::send(b); } }
+    for &b in lock_type.as_bytes() { unsafe { crate::arch::serial::send(b); } }
+    
+    let action_part = b"\",\"action\":\"";
+    for &b in action_part { unsafe { crate::arch::serial::send(b); } }
+    
+    // Output action
+    for &b in action.as_bytes() { unsafe { crate::arch::serial::send(b); } }
+    
+    // Add actor field (same as thread for compatibility)
+    let actor_part = b"\",\"actor\":";
+    for &b in actor_part { unsafe { crate::arch::serial::send(b); } }
+    unsafe { crate::arch::serial::send(b'0' + (actor_id % 10) as u8); }
+    
+    let end = b"}\n";
+    for &b in end { unsafe { crate::arch::serial::send(b); } }
+}
+
+fn get_current_actor_id() -> usize {
+    // For testing, we need to explicitly pass thread IDs
+    // This function is a placeholder that will be replaced with explicit IDs
+    0  // Default to thread 0, will be overridden in test-specific versions
+}
+
+/// A read-write mutex with TLA+ tracing instrumentation.
+///
+/// This provides data access to either one writer or many readers.
+/// The internal representation of the mutex state is as follows:
+/// - **Bit 63:** Writer mutex.
+/// - **Bit 62:** Upgradeable reader mutex.
+/// - **Bit 61:** Indicates if an upgradeable reader is being upgraded.
+/// - **Bits 60-0:** Reader mutex count.
+pub struct RwMutexTrace<T: ?Sized> {
+    lock: AtomicUsize,
+    /// Threads that fail to acquire the mutex will sleep on this waitqueue.
+    queue: WaitQueue,
+    val: UnsafeCell<T>,
+}
+
+const READER: usize = 1;
+const WRITER: usize = 1 << (usize::BITS - 1);
+const UPGRADEABLE_READER: usize = 1 << (usize::BITS - 2);
+const BEING_UPGRADED: usize = 1 << (usize::BITS - 3);
+const MAX_READER: usize = 1 << (usize::BITS - 4);
+
+impl<T> RwMutexTrace<T> {
+    /// Creates a new read-write mutex with an initial value.
+    pub const fn new(val: T) -> Self {
+        Self {
+            val: UnsafeCell::new(val),
+            lock: AtomicUsize::new(0),
+            queue: WaitQueue::new(),
+        }
+    }
+}
+
+impl<T: ?Sized> RwMutexTrace<T> {
+    /// Acquires a read mutex and sleep until it can be acquired.
+    #[track_caller]
+    pub fn read(&self) -> RwMutexTraceReadGuard<T> {
+        self.read_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Acquires a read mutex with explicit thread ID (for testing).
+    #[track_caller]
+    pub fn read_with_thread_id(&self, thread_id: usize) -> RwMutexTraceReadGuard<T> {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+        
+        let guard = self.queue.wait_until(|| {
+            let lock = self.lock.fetch_add(READER, Acquire);
+            if lock & (WRITER | BEING_UPGRADED | MAX_READER) == 0 {
+                Some(RwMutexTraceReadGuard { inner: self, thread_id })
+            } else {
+                self.lock.fetch_sub(READER, Release);
+                None
+            }
+        });
+        
+        // Record successful read lock (after execution)
+        let state = self.get_state_string();
+        trace_event("ReadLock", rwmutex_addr, thread_id, &state, "read");
+        guard
+    }
+
+    /// Acquires a write mutex and sleep until it can be acquired.
+    #[track_caller]
+    pub fn write(&self) -> RwMutexTraceWriteGuard<T> {
+        self.write_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Acquires a write mutex with explicit thread ID (for testing).
+    #[track_caller]
+    pub fn write_with_thread_id(&self, thread_id: usize) -> RwMutexTraceWriteGuard<T> {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+        
+        let guard = self.queue.wait_until(|| {
+            if self
+                .lock
+                .compare_exchange(0, WRITER, Acquire, Relaxed)
+                .is_ok()
+            {
+                Some(RwMutexTraceWriteGuard { inner: self, thread_id })
+            } else {
+                None
+            }
+        });
+        
+        // Record successful write lock (after execution)
+        let state = self.get_state_string();
+        trace_event("WriteLock", rwmutex_addr, thread_id, &state, "write");
+        guard
+    }
+
+    /// Acquires a upread mutex and sleep until it can be acquired.
+    #[track_caller]
+    pub fn upread(&self) -> RwMutexTraceUpgradeableGuard<T> {
+        self.upread_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Acquires a upread mutex with explicit thread ID (for testing).
+    #[track_caller]
+    pub fn upread_with_thread_id(&self, thread_id: usize) -> RwMutexTraceUpgradeableGuard<T> {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+
+        let guard = self.queue.wait_until(|| {
+            let lock = self.lock.fetch_or(UPGRADEABLE_READER, Acquire) & (WRITER | UPGRADEABLE_READER);
+            if lock == 0 {
+                Some(RwMutexTraceUpgradeableGuard { inner: self, thread_id })
+            } else if lock == WRITER {
+                self.lock.fetch_sub(UPGRADEABLE_READER, Release);
+                None
+            } else {
+                None
+            }
+        });
+
+        let state = self.get_state_string();
+        trace_event("UpreadLock", rwmutex_addr, thread_id, &state, "upread");
+        guard
+    }
+
+    /// Attempts to acquire a read mutex.
+    pub fn try_read(&self) -> Option<RwMutexTraceReadGuard<T>> {
+        self.try_read_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Attempts to acquire a read mutex with explicit thread ID.
+    pub fn try_read_with_thread_id(&self, thread_id: usize) -> Option<RwMutexTraceReadGuard<T>> {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+        
+        let lock = self.lock.fetch_add(READER, Acquire);
+        if lock & (WRITER | BEING_UPGRADED | MAX_READER) == 0 {
+            // Record successful try_read (after execution)
+            let state = self.get_state_string();
+            trace_event("TryReadLock", rwmutex_addr, thread_id, &state, "read");
+            Some(RwMutexTraceReadGuard { inner: self, thread_id })
+        } else {
+            self.lock.fetch_sub(READER, Release);
+            None
+        }
+    }
+
+    /// Attempts to acquire a write mutex.
+    pub fn try_write(&self) -> Option<RwMutexTraceWriteGuard<T>> {
+        self.try_write_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Attempts to acquire a write mutex with explicit thread ID.
+    pub fn try_write_with_thread_id(&self, thread_id: usize) -> Option<RwMutexTraceWriteGuard<T>> {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+        
+        if self
+            .lock
+            .compare_exchange(0, WRITER, Acquire, Relaxed)
+            .is_ok()
+        {
+            // Record successful try_write (after execution)
+            let state = self.get_state_string();
+            trace_event("TryWriteLock", rwmutex_addr, thread_id, &state, "write");
+            Some(RwMutexTraceWriteGuard { inner: self, thread_id })
+        } else {
+            None
+        }
+    }
+
+    /// Attempts to acquire a upread mutex.
+    pub fn try_upread(&self) -> Option<RwMutexTraceUpgradeableGuard<T>> {
+        self.try_upread_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Attempts to acquire a upread mutex with explicit thread ID.
+    pub fn try_upread_with_thread_id(&self, thread_id: usize) -> Option<RwMutexTraceUpgradeableGuard<T>> {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+
+        let lock = self.lock.fetch_or(UPGRADEABLE_READER, Acquire) & (WRITER | UPGRADEABLE_READER);
+        if lock == 0 {
+            let state = self.get_state_string();
+            trace_event("TryUpreadLock", rwmutex_addr, thread_id, &state, "upread");
+            return Some(RwMutexTraceUpgradeableGuard { inner: self, thread_id });
+        } else if lock == WRITER {
+            self.lock.fetch_sub(UPGRADEABLE_READER, Release);
+        }
+        None
+    }
+
+    /// Returns a mutable reference to the underlying data.
+    pub fn get_mut(&mut self) -> &mut T {
+        self.val.get_mut()
+    }
+    
+    // Helper methods for tracing
+    fn try_read_for_tracing(&self) -> Option<RwMutexTraceReadGuard<T>> {
+        self.try_read_for_tracing_with_id(get_current_actor_id())
+    }
+    
+    fn try_read_for_tracing_with_id(&self, thread_id: usize) -> Option<RwMutexTraceReadGuard<T>> {
+        let lock = self.lock.fetch_add(READER, Acquire);
+        if lock & (WRITER | BEING_UPGRADED | MAX_READER) == 0 {
+            Some(RwMutexTraceReadGuard { inner: self, thread_id })
+        } else {
+            self.lock.fetch_sub(READER, Release);
+            None
+        }
+    }
+    
+    fn try_write_for_tracing(&self) -> Option<RwMutexTraceWriteGuard<T>> {
+        self.try_write_for_tracing_with_id(get_current_actor_id())
+    }
+    
+    fn try_write_for_tracing_with_id(&self, thread_id: usize) -> Option<RwMutexTraceWriteGuard<T>> {
+        if self
+            .lock
+            .compare_exchange(0, WRITER, Acquire, Relaxed)
+            .is_ok()
+        {
+            Some(RwMutexTraceWriteGuard { inner: self, thread_id })
+        } else {
+            None
+        }
+    }
+    
+    fn try_upread_for_tracing(&self) -> Option<RwMutexTraceUpgradeableGuard<T>> {
+        self.try_upread_for_tracing_with_id(get_current_actor_id())
+    }
+    
+    fn try_upread_for_tracing_with_id(&self, thread_id: usize) -> Option<RwMutexTraceUpgradeableGuard<T>> {
+        let lock = self.lock.fetch_or(UPGRADEABLE_READER, Acquire) & (WRITER | UPGRADEABLE_READER);
+        if lock == 0 {
+            return Some(RwMutexTraceUpgradeableGuard { inner: self, thread_id });
+        } else if lock == WRITER {
+            self.lock.fetch_sub(UPGRADEABLE_READER, Release);
+        }
+        None
+    }
+    
+    fn get_state_string(&self) -> &'static str {
+        let lock_val = self.lock.load(Relaxed);
+        let readers = lock_val & (MAX_READER - 1);
+        let has_writer = (lock_val & WRITER) != 0;
+        let has_upread = (lock_val & UPGRADEABLE_READER) != 0;
+        let being_upgraded = (lock_val & BEING_UPGRADED) != 0;
+        
+        if has_writer {
+            "write_locked"
+        } else if being_upgraded {
+            "upgrading"
+        } else if has_upread && readers > 0 {
+            if readers == 1 {
+                "upread_and_1_reader"
+            } else {
+                "upread_and_multi_readers"
+            }
+        } else if has_upread {
+            "upread_locked"
+        } else if readers > 0 {
+            if readers == 1 {
+                "1_reader"
+            } else {
+                "multi_readers"
+            }
+        } else {
+            "unlocked"
+        }
+    }
+    
+    fn release_read(&self, thread_id: usize) {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+        
+        if self.lock.fetch_sub(READER, Release) == READER {
+            self.queue.wake_one();
+        }
+        
+        // Record successful read unlock (after execution)
+        let state = self.get_state_string();
+        trace_event("ReadUnlock", rwmutex_addr, thread_id, &state, "read");
+    }
+    
+    fn release_write(&self, thread_id: usize) {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+        
+        self.lock.store(0, Release);
+        self.queue.wake_all();
+        
+        // Record successful write unlock (after execution)
+        let state = self.get_state_string();
+        trace_event("WriteUnlock", rwmutex_addr, thread_id, &state, "write");
+    }
+    
+    fn release_upread(&self, thread_id: usize) {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+
+        if self.lock.fetch_sub(UPGRADEABLE_READER, AcqRel) == UPGRADEABLE_READER {
+            self.queue.wake_one();
+        }
+
+        let state = self.get_state_string();
+        trace_event("UpreadUnlock", rwmutex_addr, thread_id, &state, "upread");
+    }
+    
+    fn upgrade_upread(&self, thread_id: usize) -> RwMutexTraceWriteGuard<T> {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+
+        let lock = self.lock.fetch_or(BEING_UPGRADED, AcqRel);
+
+        // Wait for all readers to leave - use a simple spin loop
+        loop {
+            if (self.lock.load(Acquire) & (MAX_READER - 1)) == 0 {
+                break;
+            }
+            core::hint::spin_loop();
+        }
+
+        // Upgrade to writer
+        self.lock.store(WRITER, Release);
+
+        let state = self.get_state_string();
+        trace_event("UpgradeLock", rwmutex_addr, thread_id, &state, "write");
+
+        RwMutexTraceWriteGuard { inner: self, thread_id }
+    }
+    
+    fn try_upgrade_upread(&self, thread_id: usize) -> Option<RwMutexTraceWriteGuard<T>> {
+        let rwmutex_addr = self as *const Self as *const () as usize;
+
+        // Try to set BEING_UPGRADED flag
+        let lock = self.lock.fetch_or(BEING_UPGRADED, AcqRel);
+
+        // Check if there are readers - if so, upgrade fails
+        if (self.lock.load(Acquire) & (MAX_READER - 1)) != 0 {
+            // Clear BEING_UPGRADED flag and fail
+            self.lock.fetch_and(!BEING_UPGRADED, Release);
+            return None;
+        }
+
+        // Upgrade to writer
+        self.lock.store(WRITER, Release);
+
+        let state = self.get_state_string();
+        trace_event("TryUpgradeLock", rwmutex_addr, thread_id, &state, "write");
+
+        Some(RwMutexTraceWriteGuard { inner: self, thread_id })
+    }
+}
+
+impl<T: ?Sized + fmt::Debug> fmt::Debug for RwMutexTrace<T> {
+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
+        fmt::Debug::fmt(&self.val, f)
+    }
+}
+
+unsafe impl<T: ?Sized + Send> Send for RwMutexTrace<T> {}
+unsafe impl<T: ?Sized + Send + Sync> Sync for RwMutexTrace<T> {}
+
+// Guard implementations
+pub struct RwMutexTraceReadGuard<'a, T: ?Sized> {
+    inner: &'a RwMutexTrace<T>,
+    thread_id: usize,
+}
+
+impl<'a, T: ?Sized> Deref for RwMutexTraceReadGuard<'a, T> {
+    type Target = T;
+
+    fn deref(&self) -> &Self::Target {
+        unsafe { &*self.inner.val.get() }
+    }
+}
+
+impl<'a, T: ?Sized> Drop for RwMutexTraceReadGuard<'a, T> {
+    fn drop(&mut self) {
+        self.inner.release_read(self.thread_id);
+    }
+}
+
+pub struct RwMutexTraceWriteGuard<'a, T: ?Sized> {
+    inner: &'a RwMutexTrace<T>,
+    thread_id: usize,
+}
+
+impl<'a, T: ?Sized> Deref for RwMutexTraceWriteGuard<'a, T> {
+    type Target = T;
+
+    fn deref(&self) -> &Self::Target {
+        unsafe { &*self.inner.val.get() }
+    }
+}
+
+impl<'a, T: ?Sized> DerefMut for RwMutexTraceWriteGuard<'a, T> {
+    fn deref_mut(&mut self) -> &mut Self::Target {
+        unsafe { &mut *self.inner.val.get() }
+    }
+}
+
+impl<'a, T: ?Sized> Drop for RwMutexTraceWriteGuard<'a, T> {
+    fn drop(&mut self) {
+        self.inner.release_write(self.thread_id);
+    }
+}
+
+pub struct RwMutexTraceUpgradeableGuard<'a, T: ?Sized> {
+    inner: &'a RwMutexTrace<T>,
+    thread_id: usize,
+}
+
+impl<'a, T: ?Sized> Deref for RwMutexTraceUpgradeableGuard<'a, T> {
+    type Target = T;
+
+    fn deref(&self) -> &Self::Target {
+        unsafe { &*self.inner.val.get() }
+    }
+}
+
+impl<'a, T: ?Sized> Drop for RwMutexTraceUpgradeableGuard<'a, T> {
+    fn drop(&mut self) {
+        self.inner.release_upread(self.thread_id);
+    }
+}
+
+impl<'a, T: ?Sized> RwMutexTraceUpgradeableGuard<'a, T> {
+    /// Upgrade the upread guard to a write guard.
+    pub fn upgrade(self) -> RwMutexTraceWriteGuard<'a, T> {
+        let thread_id = self.thread_id;
+        let guard = self.inner.upgrade_upread(thread_id);
+        // Don't drop the upread guard since we're upgrading
+        core::mem::forget(self);
+        guard
+    }
+    
+    /// Attempts to upgrade this upread guard to a write guard atomically.
+    /// This function will return immediately.
+    pub fn try_upgrade(self) -> Result<RwMutexTraceWriteGuard<'a, T>, Self> {
+        let thread_id = self.thread_id;
+        if let Some(guard) = self.inner.try_upgrade_upread(thread_id) {
+            // Don't drop the upread guard since we're upgrading
+            core::mem::forget(self);
+            Ok(guard)
+        } else {
+            Err(self)
+        }
+    }
+}
+
+// Arc guards (simplified - only including the most essential ones)
+pub type ArcRwMutexTraceReadGuard<T> = RwMutexTraceReadGuard<'static, T>;
+pub type ArcRwMutexTraceWriteGuard<T> = RwMutexTraceWriteGuard<'static, T>;
+pub type ArcRwMutexTraceUpgradeableGuard<T> = RwMutexTraceUpgradeableGuard<'static, T>;
+
+#[cfg(ktest)]
+mod test {
+    use super::*;
+    use crate::prelude::*;
+
+    // Simple random number generator for test scenarios
+    fn simple_rand(seed: &mut u32) -> u32 {
+        *seed = seed.wrapping_mul(1664525).wrapping_add(1013904223);
+        *seed
+    }
+
+    #[ktest]
+    fn test_rwmutex_trace() {
+        crate::early_println!("=== Starting Single-Thread RwMutexTrace Simulation ===");
+
+        // Generate 100 different trace scenarios (5 batches of 20 to avoid stack overflow)
+        for batch in 0..5 {
+            crate::early_println!("\n=== Starting Batch {} ===", batch + 1);
+            for trace_num in 1..=20 {
+                let global_trace_num = batch * 20 + trace_num;
+                TRACE_SEQUENCE.store(0, core::sync::atomic::Ordering::Relaxed);
+                crate::early_println!("\n--- TRACE {} ---", global_trace_num);
+
+                let rwmutex = RwMutexTrace::<u32>::new(100);
+                let mut seed = (global_trace_num * 7919 + batch * 12345) as u32;
+
+                // Track what each virtual thread is holding
+                let mut thread_guards: [Option<GuardType>; 3] = [None, None, None];
+
+                // Execute 30-50 operations randomly distributed among threads
+                let total_ops = 30 + (simple_rand(&mut seed) % 21);
+
+                for _op_idx in 0..total_ops {
+                    // Randomly select which thread executes next
+                    let thread_id = (simple_rand(&mut seed) % 3) as usize;
+
+                    // Check if thread is holding a lock
+                    if thread_guards[thread_id].is_some() {
+                        // Thread has a lock, decide whether to release or upgrade
+                        let action = simple_rand(&mut seed) % 100;
+
+                        match thread_guards[thread_id].take() {
+                        Some(GuardType::Read(guard)) => {
+                            if action < 70 {  // 70% release
+                                crate::early_println!("Thread{}: releasing read lock", thread_id);
+                                drop(guard);
+                            } else {
+                                // Keep holding
+                                thread_guards[thread_id] = Some(GuardType::Read(guard));
+                            }
+                        }
+                        Some(GuardType::Write(guard)) => {
+                            if action < 60 {  // 60% release
+                                crate::early_println!("Thread{}: releasing write lock", thread_id);
+                                drop(guard);
+                            } else {
+                                // Keep holding
+                                thread_guards[thread_id] = Some(GuardType::Write(guard));
+                            }
+                        }
+                        Some(GuardType::Upread(guard)) => {
+                            if action < 40 {  // 40% release
+                                crate::early_println!("Thread{}: releasing upread lock", thread_id);
+                                drop(guard);
+                            } else if action < 70 {  // 30% try upgrade
+                                // Try upgrade
+                                if simple_rand(&mut seed) % 2 == 0 {
+                                    // Blocking upgrade
+                                    let write_guard = guard.upgrade();
+                                    crate::early_println!("Thread{}: upgraded to write lock", thread_id);
+                                    thread_guards[thread_id] = Some(GuardType::Write(write_guard));
+                                } else {
+                                    // Non-blocking upgrade
+                                    match guard.try_upgrade() {
+                                        Ok(write_guard) => {
+                                            crate::early_println!("Thread{}: try_upgrade successful", thread_id);
+                                            thread_guards[thread_id] = Some(GuardType::Write(write_guard));
+                                        }
+                                        Err(guard) => {
+                                            crate::early_println!("Thread{}: try_upgrade failed", thread_id);
+                                            thread_guards[thread_id] = Some(GuardType::Upread(guard));
+                                        }
+                                    }
+                                }
+                            } else {
+                                // Keep holding
+                                thread_guards[thread_id] = Some(GuardType::Upread(guard));
+                            }
+                        }
+                        None => {} // Already handled
+                    }
+                } else {
+                    // Thread doesn't have a lock, try to acquire one
+                    // Check if any other thread is holding a lock
+                    let lock_held = thread_guards.iter().any(|g| g.is_some());
+
+                    let op_type = simple_rand(&mut seed) % 4;  // Only use read/write operations (0-3) to avoid upread deadlock
+
+                    match op_type {
+                        0 => {
+                            // Only use blocking read if no locks are held (to avoid deadlock)
+                            if !lock_held {
+                                let guard = rwmutex.read_with_thread_id(thread_id);
+                                crate::early_println!("Thread{}: acquired read lock", thread_id);
+                                thread_guards[thread_id] = Some(GuardType::Read(guard));
+                            } else {
+                                // Use non-blocking when locks are held
+                                if let Some(guard) = rwmutex.try_read_with_thread_id(thread_id) {
+                                    crate::early_println!("Thread{}: acquired read lock (via try)", thread_id);
+                                    thread_guards[thread_id] = Some(GuardType::Read(guard));
+                                } else {
+                                    crate::early_println!("Thread{}: read blocked by other lock", thread_id);
+                                }
+                            }
+                        }
+                        1 => {
+                            // Non-blocking read
+                            if let Some(guard) = rwmutex.try_read_with_thread_id(thread_id) {
+                                crate::early_println!("Thread{}: try_read successful", thread_id);
+                                thread_guards[thread_id] = Some(GuardType::Read(guard));
+                            } else {
+                                crate::early_println!("Thread{}: try_read failed", thread_id);
+                            }
+                        }
+                        2 => {
+                            // Only use blocking write if no locks are held
+                            if !lock_held {
+                                let guard = rwmutex.write_with_thread_id(thread_id);
+                                crate::early_println!("Thread{}: acquired write lock", thread_id);
+                                thread_guards[thread_id] = Some(GuardType::Write(guard));
+                            } else {
+                                // Use non-blocking when locks are held
+                                if let Some(guard) = rwmutex.try_write_with_thread_id(thread_id) {
+                                    crate::early_println!("Thread{}: acquired write lock (via try)", thread_id);
+                                    thread_guards[thread_id] = Some(GuardType::Write(guard));
+                                } else {
+                                    crate::early_println!("Thread{}: write blocked by other lock", thread_id);
+                                }
+                            }
+                        }
+                        3 => {
+                            // Non-blocking write
+                            if let Some(guard) = rwmutex.try_write_with_thread_id(thread_id) {
+                                crate::early_println!("Thread{}: try_write successful", thread_id);
+                                thread_guards[thread_id] = Some(GuardType::Write(guard));
+                            } else {
+                                crate::early_println!("Thread{}: try_write failed", thread_id);
+                            }
+                        }
+                        4 => {
+                            // Only use blocking upread if no locks are held
+                            if !lock_held {
+                                let guard = rwmutex.upread_with_thread_id(thread_id);
+                                crate::early_println!("Thread{}: acquired upread lock", thread_id);
+                                thread_guards[thread_id] = Some(GuardType::Upread(guard));
+                            } else {
+                                // Use non-blocking when locks are held
+                                if let Some(guard) = rwmutex.try_upread_with_thread_id(thread_id) {
+                                    crate::early_println!("Thread{}: acquired upread lock (via try)", thread_id);
+                                    thread_guards[thread_id] = Some(GuardType::Upread(guard));
+                                } else {
+                                    crate::early_println!("Thread{}: upread blocked by other lock", thread_id);
+                                }
+                            }
+                        }
+                        5 | _ => {
+                            // Non-blocking upread
+                            if let Some(guard) = rwmutex.try_upread_with_thread_id(thread_id) {
+                                crate::early_println!("Thread{}: try_upread successful", thread_id);
+                                thread_guards[thread_id] = Some(GuardType::Upread(guard));
+                            } else {
+                                crate::early_println!("Thread{}: try_upread failed", thread_id);
+                            }
+                        }
+                    }
+                }
+            }
+
+                // Clean up: release any held locks
+                for (tid, guard_opt) in thread_guards.iter_mut().enumerate() {
+                if let Some(guard) = guard_opt.take() {
+                    crate::early_println!("Thread{}: releasing lock at trace end", tid);
+                    drop(guard);
+                }
+            }
+
+            // Final verification - just get initial value without lock operations
+            let final_value = 100;  // Known initial value from rwmutex construction
+
+            let event_count = TRACE_SEQUENCE.load(core::sync::atomic::Ordering::Relaxed);
+                crate::early_println!("TRACE {} complete: {} events, final value: {}",
+                    global_trace_num, event_count, final_value);
+            }
+            crate::early_println!("\n=== Batch {} Complete ===", batch + 1);
+        }
+
+        crate::early_println!("\n=== All 100 Traces Complete ===");
+    }
+
+    // Guard types for tracking what each virtual thread holds
+    enum GuardType<'a> {
+        Read(RwMutexTraceReadGuard<'a, u32>),
+        Write(RwMutexTraceWriteGuard<'a, u32>),
+        Upread(RwMutexTraceUpgradeableGuard<'a, u32>),
+    }
+}
\ No newline at end of file
diff --git a/ostd/src/sync/spin_trace.rs b/ostd/src/sync/spin_trace.rs
new file mode 100644
index 0000000..5a82677
--- /dev/null
+++ b/ostd/src/sync/spin_trace.rs
@@ -0,0 +1,523 @@
+// SPDX-License-Identifier: MPL-2.0
+
+use alloc::sync::Arc;
+use core::{
+    cell::UnsafeCell,
+    fmt,
+    marker::PhantomData,
+    ops::{Deref, DerefMut},
+    sync::atomic::{AtomicBool, Ordering},
+};
+
+use super::{guard::SpinGuardian, LocalIrqDisabled, PreemptDisabled};
+use crate::task::atomic_mode::AsAtomicModeGuard;
+
+use crate::cpu::PinCurrentCpu;
+
+use core::sync::atomic::AtomicU64;
+
+static TRACE_SEQUENCE: AtomicU64 = AtomicU64::new(0);
+
+fn trace_event(action: &str, lock_addr: usize, actor_id: usize, lock_state: bool) {
+    if crate::IN_BOOTSTRAP_CONTEXT.load(core::sync::atomic::Ordering::Relaxed) {
+        return;
+    }
+    
+    let seq = TRACE_SEQUENCE.fetch_add(1, core::sync::atomic::Ordering::Relaxed);
+    
+    // Enhanced JSON output with thread ID and lock state
+    let json = b"{\"seq\":";
+    for &b in json { unsafe { crate::arch::serial::send(b); } }
+    
+    // Output sequence number (support up to 100 operations)
+    let seq_val = seq % 100;
+    if seq_val >= 10 {
+        unsafe { crate::arch::serial::send(b'0' + (seq_val / 10) as u8); }
+    }
+    unsafe { crate::arch::serial::send(b'0' + (seq_val % 10) as u8); }
+    
+    // Add thread/actor ID
+    let thread_part = b",\"thread\":";
+    for &b in thread_part { unsafe { crate::arch::serial::send(b); } }
+    unsafe { crate::arch::serial::send(b'0' + (actor_id % 10) as u8); }
+    
+    // Add lock address (use fixed lock ID since we only have one lock)
+    let lock_part = b",\"lock\":";
+    for &b in lock_part { unsafe { crate::arch::serial::send(b); } }
+    // Since we only have one lock, always use lock ID 0
+    let lock_id = 0u8;
+    unsafe { crate::arch::serial::send(b'0' + lock_id); }
+    
+    // Add lock state
+    let state_part = b",\"state\":\"";
+    for &b in state_part { unsafe { crate::arch::serial::send(b); } }
+    if lock_state {
+        let locked = b"locked";
+        for &b in locked { unsafe { crate::arch::serial::send(b); } }
+    } else {
+        let unlocked = b"unlocked";
+        for &b in unlocked { unsafe { crate::arch::serial::send(b); } }
+    }
+    
+    let action_part = b"\",\"action\":\"";
+    for &b in action_part { unsafe { crate::arch::serial::send(b); } }
+    
+    // Output action
+    for &b in action.as_bytes() { unsafe { crate::arch::serial::send(b); } }
+    
+    // Add actor field (same as thread for compatibility)
+    let actor_part = b"\",\"actor\":";
+    for &b in actor_part { unsafe { crate::arch::serial::send(b); } }
+    unsafe { crate::arch::serial::send(b'0' + (actor_id % 10) as u8); }
+    
+    let end = b"}\n";
+    for &b in end { unsafe { crate::arch::serial::send(b); } }
+}
+
+fn get_current_actor_id() -> usize {
+    // For testing, we need to explicitly pass thread IDs
+    // This function is a placeholder that will be replaced with explicit IDs
+    0  // Default to thread 0, will be overridden in test-specific versions
+}
+
+/// A spin lock with TLA+ tracing instrumentation.
+///
+/// # Guard behavior
+///
+/// The type `G' specifies the guard behavior of the spin lock. While holding the lock,
+/// - if `G` is [`PreemptDisabled`], preemption is disabled;
+/// - if `G` is [`LocalIrqDisabled`], local IRQs are disabled.
+///
+/// The `G` can also be provided by other crates other than ostd,
+/// if it behaves similar like [`PreemptDisabled`] or [`LocalIrqDisabled`].
+///
+/// The guard behavior can be temporarily upgraded from [`PreemptDisabled`] to
+/// [`LocalIrqDisabled`] using the [`disable_irq`] method.
+///
+/// [`disable_irq`]: Self::disable_irq
+#[repr(transparent)]
+pub struct SpinTrace<T: ?Sized, G = PreemptDisabled> {
+    phantom: PhantomData<G>,
+    /// Only the last field of a struct may have a dynamically sized type.
+    /// That's why SpinLockInner is put in the last field.
+    inner: SpinLockInner<T>,
+}
+
+struct SpinLockInner<T: ?Sized> {
+    lock: AtomicBool,
+    val: UnsafeCell<T>,
+}
+
+impl<T, G> SpinTrace<T, G> {
+    /// Creates a new spin lock.
+    pub const fn new(val: T) -> Self {
+        let lock_inner = SpinLockInner {
+            lock: AtomicBool::new(false),
+            val: UnsafeCell::new(val),
+        };
+        Self {
+            phantom: PhantomData,
+            inner: lock_inner,
+        }
+    }
+}
+
+impl<T: ?Sized> SpinTrace<T, PreemptDisabled> {
+    /// Converts the guard behavior from disabling preemption to disabling IRQs.
+    pub fn disable_irq(&self) -> &SpinTrace<T, LocalIrqDisabled> {
+        let ptr = self as *const SpinTrace<T, PreemptDisabled>;
+        let ptr = ptr as *const SpinTrace<T, LocalIrqDisabled>;
+        // SAFETY:
+        // 1. The types `SpinTrace<T, PreemptDisabled>`, `SpinLockInner<T>` and `SpinTrace<T,
+        //    IrqDisabled>` have the same memory layout guaranteed by `#[repr(transparent)]`.
+        // 2. The specified memory location can be borrowed as an immutable reference for the
+        //    specified lifetime.
+        unsafe { &*ptr }
+    }
+}
+
+impl<T: ?Sized, G: SpinGuardian> SpinTrace<T, G> {
+    /// Acquires the spin lock.
+    pub fn lock(&self) -> SpinTraceGuard<T, G> {
+        self.lock_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Acquires the spin lock with explicit thread ID (for testing).
+    pub fn lock_with_thread_id(&self, thread_id: usize) -> SpinTraceGuard<T, G> {
+        // Notice the guard must be created before acquiring the lock.
+        let inner_guard = G::guard();
+        
+        // Record RequestLock (after setting state to trying_blocking)
+        let lock_addr = self as *const Self as *const () as usize;
+        let current_state = self.inner.lock.load(Ordering::Relaxed);
+        trace_event("TryAcquireBlocking", lock_addr, thread_id, current_state);
+        
+        self.acquire_lock_with_id(thread_id);
+        SpinTraceGuard_ {
+            lock: self,
+            guard: inner_guard,
+            thread_id: thread_id,
+        }
+    }
+
+    /// Acquires the spin lock through an [`Arc`].
+    ///
+    /// The method is similar to [`lock`], but it doesn't have the requirement
+    /// for compile-time checked lifetimes of the lock guard.
+    ///
+    /// [`lock`]: Self::lock
+    pub fn lock_arc(self: &Arc<Self>) -> ArcSpinTraceGuard<T, G> {
+        let inner_guard = G::guard();
+        let thread_id = get_current_actor_id();
+        self.acquire_lock_with_id(thread_id);
+        SpinTraceGuard_ {
+            lock: self.clone(),
+            guard: inner_guard,
+            thread_id: thread_id,
+        }
+    }
+
+    /// Tries acquiring the spin lock immedidately.
+    pub fn try_lock(&self) -> Option<SpinTraceGuard<T, G>> {
+        self.try_lock_with_thread_id(get_current_actor_id())
+    }
+    
+    /// Tries acquiring the spin lock with explicit thread ID (for testing).
+    pub fn try_lock_with_thread_id(&self, thread_id: usize) -> Option<SpinTraceGuard<T, G>> {
+        let inner_guard = G::guard();
+        
+        // Record RequestTryLock (after setting state to trying_nonblocking)
+        let lock_addr = self as *const Self as *const () as usize;
+        let current_state = self.inner.lock.load(Ordering::Relaxed);
+        trace_event("TryAcquireNonBlocking", lock_addr, thread_id, current_state);
+        
+        if self.try_acquire_lock_with_id(thread_id) {
+            let lock_guard = SpinTraceGuard_ {
+                lock: self,
+                guard: inner_guard,
+                thread_id: thread_id,
+            };
+            return Some(lock_guard);
+        }
+        None
+    }
+
+    /// Returns a mutable reference to the underlying data.
+    ///
+    /// This method is zero-cost: By holding a mutable reference to the lock, the compiler has
+    /// already statically guaranteed that access to the data is exclusive.
+    pub fn get_mut(&mut self) -> &mut T {
+        self.inner.val.get_mut()
+    }
+
+    /// Acquires the spin lock, otherwise busy waiting
+    fn acquire_lock(&self) {
+        self.acquire_lock_with_id(get_current_actor_id())
+    }
+    
+    fn acquire_lock_with_id(&self, thread_id: usize) {
+        let lock_addr = self as *const Self as *const () as usize;
+
+        let mut spinning = false;
+        while !self.try_acquire_lock_internal() {
+            spinning = true;
+            core::hint::spin_loop();
+        }
+
+        if spinning {
+            // Record end of spinning (if we did spin)
+            trace_event("StopSpinning", lock_addr, thread_id, true);
+        }
+
+        // Record successful lock acquisition (after execution)
+        trace_event("AcquireSuccess", lock_addr, thread_id, true);
+    }
+
+    fn try_acquire_lock_internal(&self) -> bool {
+        self.inner
+            .lock
+            .compare_exchange(false, true, Ordering::Acquire, Ordering::Relaxed)
+            .is_ok()
+    }
+
+    fn try_acquire_lock(&self) -> bool {
+        self.try_acquire_lock_with_id(get_current_actor_id())
+    }
+    
+    fn try_acquire_lock_with_id(&self, thread_id: usize) -> bool {
+        let lock_addr = self as *const Self as *const () as usize;
+        
+        let success = self.try_acquire_lock_internal();
+        
+        // Only record successful acquisitions (after execution)
+        if success {
+            trace_event("AcquireSuccess", lock_addr, thread_id, true);
+        }
+        
+        success
+    }
+
+    fn release_lock(&self, thread_id: usize) {
+        let lock_addr = self as *const Self as *const () as usize;
+        
+        self.inner.lock.store(false, Ordering::Release);
+        
+        // Record after releasing (after execution)
+        trace_event("Release", lock_addr, thread_id, false);
+    }
+}
+
+impl<T: ?Sized + fmt::Debug, G> fmt::Debug for SpinTrace<T, G> {
+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
+        fmt::Debug::fmt(&self.inner.val, f)
+    }
+}
+
+// SAFETY: Only a single lock holder is permitted to access the inner data of Spinlock.
+unsafe impl<T: ?Sized + Send, G> Send for SpinTrace<T, G> {}
+unsafe impl<T: ?Sized + Send, G> Sync for SpinTrace<T, G> {}
+
+/// A guard that provides exclusive access to the data protected by a [`SpinTrace`].
+pub type SpinTraceGuard<'a, T, G> = SpinTraceGuard_<T, &'a SpinTrace<T, G>, G>;
+/// A guard that provides exclusive access to the data protected by a `Arc<SpinTrace>`.
+pub type ArcSpinTraceGuard<T, G> = SpinTraceGuard_<T, Arc<SpinTrace<T, G>>, G>;
+
+/// The guard of a spin lock.
+#[clippy::has_significant_drop]
+#[must_use]
+pub struct SpinTraceGuard_<T: ?Sized, R: Deref<Target = SpinTrace<T, G>>, G: SpinGuardian> {
+    guard: G::Guard,
+    lock: R,
+    thread_id: usize,
+}
+
+impl<T: ?Sized, R: Deref<Target = SpinTrace<T, G>>, G: SpinGuardian> AsAtomicModeGuard
+    for SpinTraceGuard_<T, R, G>
+{
+    fn as_atomic_mode_guard(&self) -> &dyn crate::task::atomic_mode::InAtomicMode {
+        self.guard.as_atomic_mode_guard()
+    }
+}
+
+impl<T: ?Sized, R: Deref<Target = SpinTrace<T, G>>, G: SpinGuardian> Deref
+    for SpinTraceGuard_<T, R, G>
+{
+    type Target = T;
+
+    fn deref(&self) -> &T {
+        unsafe { &*self.lock.inner.val.get() }
+    }
+}
+
+impl<T: ?Sized, R: Deref<Target = SpinTrace<T, G>>, G: SpinGuardian> DerefMut
+    for SpinTraceGuard_<T, R, G>
+{
+    fn deref_mut(&mut self) -> &mut Self::Target {
+        unsafe { &mut *self.lock.inner.val.get() }
+    }
+}
+
+impl<T: ?Sized, R: Deref<Target = SpinTrace<T, G>>, G: SpinGuardian> Drop
+    for SpinTraceGuard_<T, R, G>
+{
+    fn drop(&mut self) {
+        self.lock.release_lock(self.thread_id);
+    }
+}
+
+impl<T: ?Sized + fmt::Debug, R: Deref<Target = SpinTrace<T, G>>, G: SpinGuardian> fmt::Debug
+    for SpinTraceGuard_<T, R, G>
+{
+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
+        fmt::Debug::fmt(&**self, f)
+    }
+}
+
+impl<T: ?Sized, R: Deref<Target = SpinTrace<T, G>>, G: SpinGuardian> !Send
+    for SpinTraceGuard_<T, R, G>
+{
+}
+
+// SAFETY: `SpinLockGuard_` can be shared between tasks/threads in same CPU.
+// As `lock()` is only called when there are no race conditions caused by interrupts.
+unsafe impl<T: ?Sized + Sync, R: Deref<Target = SpinTrace<T, G>> + Sync, G: SpinGuardian> Sync
+    for SpinTraceGuard_<T, R, G>
+{
+}
+
+#[cfg(ktest)]
+mod test {
+    use super::*;
+    use crate::prelude::*;
+    use alloc::sync::Arc;
+
+    // Simple pseudo-random number generator for deterministic randomness
+    fn simple_rand(seed: &mut u32) -> u32 {
+        *seed = (*seed).wrapping_mul(1103515245).wrapping_add(12345);
+        *seed
+    }
+
+    #[ktest]
+    fn test_spin_trace() {
+        crate::early_println!("=== Starting 20 SpinTrace TLA+ Traces: Single Lock, 3-Thread Contention ===");
+        
+        // Generate 20 different trace scenarios, each with single lock and 3-thread contention
+        for i in 1..=20 {
+            crate::early_println!("=== TRACE_{} ===", i);
+            crate::early_println!("TRACE_{}: Single Lock Contention Scenario {}", i,
+                match i % 4 {
+                    1 => "Basic 3-Thread Competition",
+                    2 => "Try-Lock Heavy Pattern",
+                    3 => "Blocking vs Try-Lock Mix",
+                    0 => "Complex 3-Thread Dance",
+                    _ => "Unknown"
+                });
+            
+            // Reset trace sequence for each scenario
+            TRACE_SEQUENCE.store(0, core::sync::atomic::Ordering::Relaxed);
+            
+            // Create single shared lock (only one lock per trace)
+            let shared_lock = Arc::new(SpinTrace::<u32, PreemptDisabled>::new(i * 10));
+            
+            crate::early_println!("TRACE_{}: Created single shared lock with initial value {}", i, i * 10);
+            
+            // Use simple random seed based on trace number
+            let mut seed = (i * 7919) as u32;
+            
+            // Generate different contention patterns based on trace number
+            let pattern = i % 4;
+            let op_count = 5 + (simple_rand(&mut seed) % 5); // 5-9 operations per trace
+
+            // Variable to track if we should simulate contention (don't actually hold lock)
+            let mut simulate_contention = false;
+
+            // 70% chance to have contention in this trace
+            let should_have_contention = (simple_rand(&mut seed) % 10) < 7;
+            let contention_op = if should_have_contention {
+                simple_rand(&mut seed) % op_count  // Pick a random operation for contention
+            } else {
+                op_count + 1  // Never trigger
+            };
+
+            for op_idx in 0..op_count {
+                let thread_id = (simple_rand(&mut seed) % 3) as usize; // Only threads 0, 1, 2
+
+                // Force contention scenario at the chosen operation
+                let operation_type = if op_idx == contention_op && !simulate_contention {
+                    2  // Force Case 2 (spinning scenario)
+                } else {
+                    match pattern {
+                        1 => simple_rand(&mut seed) % 5, // Basic: includes contention simulation
+                        2 => simple_rand(&mut seed) % 6, // Try-lock heavy with contention
+                        3 => simple_rand(&mut seed) % 5, // Mixed pattern
+                        _ => simple_rand(&mut seed) % 6, // Complex
+                    }
+                };
+
+                match operation_type {
+                    0 => { // Blocking lock (normal)
+                        let mut guard = shared_lock.lock_with_thread_id(thread_id);
+                        let increment = (simple_rand(&mut seed) % 20) + 1;
+                        *guard += increment;
+                        crate::early_println!("TRACE_{} Op{}: Thread{} blocking lock, incremented by {}, value now {}",
+                            i, op_idx + 1, thread_id, increment, *guard);
+                        drop(guard);
+                    },
+                    1 => { // Try lock
+                        if let Some(mut guard) = shared_lock.try_lock_with_thread_id(thread_id) {
+                            let increment = (simple_rand(&mut seed) % 15) + 5;
+                            *guard += increment;
+                            crate::early_println!("TRACE_{} Op{}: Thread{} try-lock SUCCESS, incremented by {}, value now {}",
+                                i, op_idx + 1, thread_id, increment, *guard);
+                            drop(guard);
+                        } else {
+                            crate::early_println!("TRACE_{} Op{}: Thread{} try-lock FAILED",
+                                i, op_idx + 1, thread_id);
+                        }
+                    },
+                    2 => { // Simulate spinning scenario
+                        if !simulate_contention {  // Always trigger if Case 2 is selected
+                            // Simulate a spinning scenario
+                            let holder_thread = 1;
+                            let competing_thread = (holder_thread + 1) % 3;
+
+                            crate::early_println!("TRACE_{} Op{}: Simulating contention between Thread{} (holder) and Thread{} (spinner)",
+                                i, op_idx, holder_thread, competing_thread);
+
+                            // Manually generate trace events for a spinning scenario
+                            let lock_addr = 0usize; // Use fixed address for simplicity
+
+                            // Thread holder_thread acquires lock
+                            trace_event("TryAcquireBlocking", lock_addr, holder_thread, false);
+                            trace_event("AcquireSuccess", lock_addr, holder_thread, true);
+
+                            // Thread competing_thread tries to acquire (and spins)
+                            trace_event("TryAcquireBlocking", lock_addr, competing_thread, true);
+                            trace_event("StopSpinning", lock_addr, competing_thread, true);
+
+                            // Thread holder_thread releases
+                            trace_event("Release", lock_addr, holder_thread, false);
+
+                            // Thread competing_thread acquires
+                            trace_event("AcquireSuccess", lock_addr, competing_thread, true);
+                            trace_event("Release", lock_addr, competing_thread, false);
+
+                            simulate_contention = true;
+                        } else {
+                            // Normal read operation
+                            let value = *shared_lock.lock_with_thread_id(thread_id);
+                            crate::early_println!("TRACE_{} Op{}: Thread{} read lock value: {}",
+                                i, op_idx + 1, thread_id, value);
+                        }
+                    },
+                    3 => { // Read operation
+                        let value = *shared_lock.lock_with_thread_id(thread_id);
+                        crate::early_println!("TRACE_{} Op{}: Thread{} read lock value: {}",
+                            i, op_idx + 1, thread_id, value);
+                    },
+                    4 => { // Multiple quick operations
+                        for _ in 0..2 {
+                            if let Some(mut guard) = shared_lock.try_lock_with_thread_id(thread_id) {
+                                *guard += 1;
+                                crate::early_println!("TRACE_{} Op{}: Thread{} quick operation",
+                                    i, op_idx + 1, thread_id);
+                                drop(guard);
+                            }
+                        }
+                    },
+                    _ => { // Try then fallback to blocking with possible contention
+                        if let Some(mut guard) = shared_lock.try_lock_with_thread_id(thread_id) {
+                            let decrement = (simple_rand(&mut seed) % 5) + 1;
+                            *guard = guard.saturating_sub(decrement);
+                            crate::early_println!("TRACE_{} Op{}: Thread{} try-lock SUCCESS, decremented by {}, value now {}",
+                                i, op_idx + 1, thread_id, decrement, *guard);
+                            drop(guard);
+                        } else {
+                            crate::early_println!("TRACE_{} Op{}: Thread{} try-lock failed, falling back to blocking",
+                                i, op_idx + 1, thread_id);
+                            let mut guard = shared_lock.lock_with_thread_id(thread_id);
+                            let increment = (simple_rand(&mut seed) % 10) + 1;
+                            *guard += increment;
+                            crate::early_println!("TRACE_{} Op{}: Thread{} blocking fallback SUCCESS, incremented by {}, value now {}",
+                                i, op_idx + 1, thread_id, increment, *guard);
+                            drop(guard);
+                        }
+                    }
+                }
+            }
+
+            // Reset contention flag for next trace
+            simulate_contention = false;
+            
+            // Final state check for this trace
+            let final_value = *shared_lock.lock_with_thread_id(0);
+            crate::early_println!("--- TRACE_{} Summary: Final value {} ---", i, final_value);
+            
+            let final_seq = TRACE_SEQUENCE.load(core::sync::atomic::Ordering::Relaxed);
+            crate::early_println!("--- TRACE_{} Complete: {} events with single lock, 3-thread contention ---", i, final_seq);
+            crate::early_println!("");
+        }
+        
+        crate::early_println!("=== All 20 Single-Lock 3-Thread Contention Traces Generated ===");
+    }
+}
diff --git a/ostd/src/sync/spin_trace_tests.rs b/ostd/src/sync/spin_trace_tests.rs
new file mode 100644
index 0000000..abe0868
--- /dev/null
+++ b/ostd/src/sync/spin_trace_tests.rs
@@ -0,0 +1,215 @@
+//! Additional SpinLock TLA+ trace tests
+
+use alloc::sync::Arc;
+use crate::sync::{SpinTrace, PreemptDisabled};
+use core::sync::atomic::{AtomicUsize, Ordering};
+
+#[cfg(ktest)]
+mod test {
+    use super::*;
+    use crate::prelude::*;
+    
+    static TEST_COUNTER: AtomicUsize = AtomicUsize::new(0);
+    static RANDOM_SEED: AtomicUsize = AtomicUsize::new(12345);
+    
+    // Simple linear congruential generator for randomization
+    fn next_random() -> usize {
+        let current = RANDOM_SEED.load(Ordering::Relaxed);
+        let next = current.wrapping_mul(1103515245).wrapping_add(12345);
+        RANDOM_SEED.store(next, Ordering::Relaxed);
+        next
+    }
+    
+    fn random_delay() {
+        let delay_cycles = (next_random() % 1000) + 50;
+        for _ in 0..delay_cycles {
+            core::hint::spin_loop();
+        }
+    }
+    
+    #[ktest]
+    fn test_spinlock_tla_trace() {
+        crate::early_println!("=== Minimal SpinLock TLA+ Trace Test Start ===");
+        
+        // Create two locks for testing
+        let lock1 = Arc::new(SpinTrace::<u32, PreemptDisabled>::new(100));
+        let lock2 = Arc::new(SpinTrace::<u32, PreemptDisabled>::new(200));
+        
+        crate::early_println!("Lock1 created");
+        crate::early_println!("Lock2 created");
+        
+        // Test 1: Basic lock operation on lock1
+        {
+            crate::early_println!("Test 1: Basic lock1 operation");
+            let mut guard1 = lock1.lock();  // Should generate: TryAcquireBlocking + AcquireSuccess
+            *guard1 += 1;
+            crate::early_println!("Lock1 value: {}", *guard1);
+            drop(guard1);  // Should generate: Release
+        }
+        
+        // Test 2: Basic lock operation on lock2
+        {
+            crate::early_println!("Test 2: Basic lock2 operation");
+            let mut guard2 = lock2.lock();  // Should generate: TryAcquireBlocking + AcquireSuccess
+            *guard2 += 10;
+            crate::early_println!("Lock2 value: {}", *guard2);
+            drop(guard2);  // Should generate: Release
+        }
+        
+        // Test 3: try_lock operation on lock1 (should succeed)
+        {
+            crate::early_println!("Test 3: try_lock on lock1");
+            if let Some(mut guard1) = lock1.try_lock() {  // Should generate: TryAcquireNonBlocking + AcquireSuccess
+                *guard1 += 5;
+                crate::early_println!("Try_lock success, lock1 value: {}", *guard1);
+                drop(guard1);  // Should generate: Release
+            } else {
+                crate::early_println!("Try_lock failed");
+            }
+        }
+        
+        // Test 4: try_lock operation on lock2 (should succeed)
+        {
+            crate::early_println!("Test 4: try_lock on lock2");
+            if let Some(mut guard2) = lock2.try_lock() {  // Should generate: TryAcquireNonBlocking + AcquireSuccess
+                *guard2 += 50;
+                crate::early_println!("Try_lock success, lock2 value: {}", *guard2);
+                drop(guard2);  // Should generate: Release
+            } else {
+                crate::early_println!("Try_lock failed");
+            }
+        }
+        
+        // Final read to check values
+        {
+            crate::early_println!("Final check:");
+            let final1 = *lock1.lock();  // Should generate: TryAcquireBlocking + AcquireSuccess + Release
+            let final2 = *lock2.lock();  // Should generate: TryAcquireBlocking + AcquireSuccess + Release
+            
+            crate::early_println!("Final lock1 value: {}", final1);
+            crate::early_println!("Final lock2 value: {}", final2);
+            
+            // Basic assertions
+            assert_eq!(final1, 106);  // 100 + 1 + 5
+            assert_eq!(final2, 260);  // 200 + 10 + 50
+        }
+        
+        crate::early_println!("=== Minimal SpinLock TLA+ Trace Test Complete ===");
+        crate::early_println!("Expected trace count: 18 events");
+        crate::early_println!("- 6 tests  3 events each (TryAcquire + Success + Release) = 18 total");
+    }
+    
+    #[ktest]
+    fn test_tla_trace_simple() {
+        crate::early_println!("=== TLA+ SpinLock Randomized Trace Test Start ===");
+        
+        // Create shared locks
+        let lock1 = Arc::new(SpinTrace::<usize, PreemptDisabled>::new(0));
+        let lock2 = Arc::new(SpinTrace::<usize, PreemptDisabled>::new(100));
+        
+        crate::early_println!("Lock1 address: {:p}", lock1.as_ref());
+        crate::early_println!("Lock2 address: {:p}", lock2.as_ref());
+        
+        // Perform randomized operations (about 30-40 trace events)
+        for op_id in 0..1 {
+            random_delay();
+            
+            // Randomly choose between lock1 and lock2
+            let use_lock1 = (next_random() % 2) == 0;
+            if use_lock1 {
+                crate::early_println!("--- Using Lock1 ---");
+                random_operation(&lock1, op_id);
+            } else {
+                crate::early_println!("--- Using Lock2 ---");
+                random_operation(&lock2, op_id);
+            }
+            
+            // Add some random global delay
+            random_delay();
+        }
+        
+        // Final state check
+        let final_value1 = *lock1.lock();
+        let final_value2 = *lock2.lock();
+        let final_counter = TEST_COUNTER.load(Ordering::Relaxed);
+        
+        crate::early_println!("=== TLA+ SpinLock Randomized Trace Test Complete ===");
+        crate::early_println!("Final lock1 value: {}", final_value1);
+        crate::early_println!("Final lock2 value: {}", final_value2);
+        crate::early_println!("Final counter: {}", final_counter);
+        crate::early_println!("=== End Trace Output ===");
+        
+        // Basic assertions
+        assert!(final_value1 > 0 || final_value2 > 100);
+        assert!(final_counter > 0);
+    }
+    
+    fn random_operation(lock: &Arc<SpinTrace<usize, PreemptDisabled>>, op_id: usize) {
+        let choice = next_random() % 4;
+        
+        match choice {
+            0 => {
+                // Regular lock operation
+                crate::early_println!("Op{}: Trying regular lock", op_id);
+                let mut guard = lock.lock();
+                random_delay();
+                *guard += 1;
+                TEST_COUNTER.fetch_add(1, Ordering::Relaxed);
+                crate::early_println!("Op{}: Regular lock success, value={}", op_id, *guard);
+                random_delay();
+                drop(guard);
+            },
+            1 => {
+                // Try lock operation
+                crate::early_println!("Op{}: Trying try_lock", op_id);
+                if let Some(mut guard) = lock.try_lock() {
+                    random_delay();
+                    *guard += 2;
+                    TEST_COUNTER.fetch_add(1, Ordering::Relaxed);
+                    crate::early_println!("Op{}: Try_lock success, value={}", op_id, *guard);
+                    random_delay();
+                    drop(guard);
+                } else {
+                    crate::early_println!("Op{}: Try_lock failed", op_id);
+                }
+            },
+            2 => {
+                // Multiple try_lock attempts (may fail)
+                crate::early_println!("Op{}: Multiple try_lock attempts", op_id);
+                let mut attempts = 0;
+                for _ in 0..3 {
+                    if let Some(mut guard) = lock.try_lock() {
+                        *guard += 1;
+                        TEST_COUNTER.fetch_add(1, Ordering::Relaxed);
+                        crate::early_println!("Op{}: Multi try_lock success on attempt {}", op_id, attempts);
+                        random_delay();
+                        drop(guard);
+                        break;
+                    }
+                    attempts += 1;
+                    random_delay();
+                }
+                if attempts >= 3 {
+                    crate::early_println!("Op{}: All try_lock attempts failed", op_id);
+                }
+            },
+            3 => {
+                // Quick lock/unlock cycle
+                crate::early_println!("Op{}: Quick lock cycle", op_id);
+                for i in 0..2 {
+                    let mut guard = lock.lock();
+                    *guard += 1;
+                    TEST_COUNTER.fetch_add(1, Ordering::Relaxed);
+                    crate::early_println!("Op{}: Quick cycle {}, value={}", op_id, i, *guard);
+                    // Very short delay
+                    for _ in 0..10 {
+                        core::hint::spin_loop();
+                    }
+                    drop(guard);
+                    random_delay();
+                }
+            },
+            _ => unreachable!(),
+        }
+    }
+}
\ No newline at end of file
