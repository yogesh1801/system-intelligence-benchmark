{"task_id": "system_lab_1", "task_name": "problems/system_lab_1.md", "task": "# Problem Context\n## Introduction\nIn this lab you'll build a MapReduce system. You'll implement a worker process that calls application Map and Reduce functions and handles reading and writing files, and a coordinator process that hands out tasks to workers and copes with failed workers. You'll be building something similar to the [MapReduce paper](http://research.google.com/archive/mapreduce-osdi04.pdf). (Note: this lab uses \"coordinator\" instead of the paper's \"master\".)\n## Getiting Started\nYou need to [setup Go](http://nil.csail.mit.edu/6.5840/2024/labs/go.html) to do the labs.\n\nFetch the initial lab software with [git](https://git-scm.com/) (a version control system). To learn more about git, look at the [Pro Git book](https://git-scm.com/book/en/v2) or the [git user's manual](http://www.kernel.org/pub/software/scm/git/docs/user-manual.html).\n\n```\n$ git clone git://g.csail.mit.edu/6.5840-golabs-2024 6.5840\n$ cd 6.5840\n$ ls\nMakefile src\n$\n```\n\nWe supply you with a simple sequential mapreduce implementation in `src/main/mrsequential.go`. It runs the maps and reduces one at a time, in a single process. We also provide you with a couple of MapReduce applications: word-count in `mrapps/wc.go`, and a text indexer in `mrapps/indexer.go`. You can run word count sequentially as follows:\n\n```\n$ cd ~/6.5840\n$ cd src/main\n$ go build -buildmode=plugin ../mrapps/wc.go\n$ rm mr-out*\n$ go run mrsequential.go wc.so pg*.txt\n$ more mr-out-0\nA 509\nABOUT 2\nACT 8\n...\n```\n\n`mrsequential.go` leaves its output in the file `mr-out-0`. The input is from the text files named `pg-xxx.txt`.\n\nFeel free to borrow code from `mrsequential.go`. You should also have a look at `mrapps/wc.go` to see what MapReduce application code looks like.\n\nFor this lab and all the others, we might issue updates to the code we provide you. To ensure that you can fetch those updates and easily merge them using `git pull`, it's best to leave the code we provide in the original files. You can add to the code we provide as directed in the lab write-ups; just don't move it. It's OK to put your own new functions in new files.\n## The Code\n\n# Your Task \nYour job is to implement a distributed MapReduce, consisting of two programs, the coordinator and the worker. There will be just one coordinator process, and one or more worker processes executing in parallel. In a real system the workers would run on a bunch of different machines, but for this lab you'll run them all on a single machine. The workers will talk to the coordinator via RPC. Each worker process will, in a loop, ask the coordinator for a task, read the task's input from one or more files, execute the task, write the task's output to one or more files, and again ask the coordinator for a new task. The coordinator should notice if a worker hasn't completed its task in a reasonable amount of time (for this lab, use ten seconds), and give the same task to a different worker.\n\nWe have given you a little code to start you off. The \"main\" routines for the coordinator and worker are in `main/mrcoordinator.go` and `main/mrworker.go`; don't change these files. You should put your implementation in `mr/coordinator.go`, `mr/worker.go`, and `mr/rpc.go`.\n\nHere's how to run your code on the word-count MapReduce application. First, make sure the word-count plugin is freshly built:\n\n```\n$ go build -buildmode=plugin ../mrapps/wc.go\n```\n\nIn the `main` directory, run the coordinator.\n\n```\n$ rm mr-out*\n$ go run mrcoordinator.go pg-*.txt\n```\n\nThe `pg-*.txt` arguments to `mrcoordinator.go` are the input files; each file corresponds to one \"split\", and is the input to one Map task.\n\nIn one or more other windows, run some workers:\n\n```\n$ go run mrworker.go wc.so\n```\n\nWhen the workers and coordinator have finished, look at the output in `mr-out-*`. When you've completed the lab, the sorted union of the output files should match the sequential output, like this:\n\n```\n$ cat mr-out-* | sort | more\nA 509\nABOUT 2\nACT 8\n...\n```\n\nWe supply you with a test script in `main/test-mr.sh`. The tests check that the `wc` and `indexer` MapReduce applications produce the correct output when given the `pg-xxx.txt` files as input. The tests also check that your implementation runs the Map and Reduce tasks in parallel, and that your implementation recovers from workers that crash while running tasks.\n\nIf you run the test script now, it will hang because the coordinator never finishes:\n\n```\n$ cd ~/6.5840/src/main\n$ bash test-mr.sh\n*** Starting wc test.\n```\n\nYou can change `ret := false` to true in the Done function in `mr/coordinator.go` so that the coordinator exits immediately. Then:\n\n```\n$ bash test-mr.sh\n*** Starting wc test.\nsort: No such file or directory\ncmp: EOF on mr-wc-all\n--- wc output is not the same as mr-correct-wc.txt\n--- wc test: FAIL\n$\n```\n\nThe test script expects to see output in files named `mr-out-X`, one for each reduce task. The empty implementations of `mr/coordinator.go` and `mr/worker.go` don't produce those files (or do much of anything else), so the test fails.\n\nWhen you've finished, the test script output should look like this:\n\n```\n$ bash test-mr.sh\n*** Starting wc test.\n--- wc test: PASS\n*** Starting indexer test.\n--- indexer test: PASS\n*** Starting map parallelism test.\n--- map parallelism test: PASS\n*** Starting reduce parallelism test.\n--- reduce parallelism test: PASS\n*** Starting job count test.\n--- job count test: PASS\n*** Starting early exit test.\n--- early exit test: PASS\n*** Starting crash test.\n--- crash test: PASS\n*** PASSED ALL TESTS\n$\n```\n\nYou may see some errors from the Go RPC package that look like\n\n```\n2019/12/16 13:27:09 rpc.Register: method \"Done\" has 1 input parameters; needs exactly three\n```\n\nIgnore these messages; registering the coordinator as an [RPC server](https://golang.org/src/net/rpc/server.go) checks if all its methods are suitable for RPCs (have 3 inputs); we know that `Done` is not called via RPC.\n\nAdditionally, depending on your strategy for terminating worker processes, you may see some errors of the form\n\n```\n2024/02/11 16:21:32 dialing:dial unix /var/tmp/5840-mr-501: connect: connection refused\n```\n\nIt is fine to see a handful of these messages per test; they arise when the worker is unable to contact the coordinator RPC server after the coordinator has exited.\n\n### A few rules:\n\n- The map phase should divide the intermediate keys into buckets for `nReduce` reduce tasks, where `nReduce` is the number of reduce tasks -- the argument that `main/mrcoordinator.go` passes to `MakeCoordinator()`. Each mapper should create `nReduce` intermediate files for consumption by the reduce tasks.\n- The worker implementation should put the output of the X'th reduce task in the file `mr-out-X`.\n- A `mr-out-X` file should contain one line per Reduce function output. The line should be generated with the Go `\"%v %v\"` format, called with the key and value. Have a look in `main/mrsequential.go` for the line commented \"this is the correct format\". The test script will fail if your implementation deviates too much from this format.\n- You can modify `mr/worker.go`, `mr/coordinator.go`, and `mr/rpc.go`. You can temporarily modify other files for testing, but make sure your code works with the original versions; we'll test with the original versions.\n- The worker should put intermediate Map output in files in the current directory, where your worker can later read them as input to Reduce tasks.\n- `main/mrcoordinator.go` expects `mr/coordinator.go` to implement a `Done()` method that returns true when the MapReduce job is completely finished; at that point, `mrcoordinator.go` will exit.\n- When the job is completely finished, the worker processes should exit. A simple way to implement this is to use the return value from `call()`: if the worker fails to contact the coordinator, it can assume that the coordinator has exited because the job is done, so the worker can terminate too. Depending on your design, you might also find it helpful to have a \"please exit\" pseudo-task that the coordinator can give to workers.\n\n### Hints\n\n- The [Guidance page](http://nil.csail.mit.edu/6.5840/2024/labs/guidance.html) has some tips on developing and debugging.\n\n- One way to get started is to modify `mr/worker.go`'s `Worker()` to send an RPC to the coordinator asking for a task. Then modify the coordinator to respond with the file name of an as-yet-unstarted map task. Then modify the worker to read that file and call the application Map function, as in `mrsequential.go`.\n\n- The application Map and Reduce functions are loaded at run-time using the Go plugin package, from files whose names end in `.so`.\n\n- If you change anything in the `mr/` directory, you will probably have to re-build any MapReduce plugins you use, with something like `go build -buildmode=plugin ../mrapps/wc.go`\n\n- This lab relies on the workers sharing a file system. That's straightforward when all workers run on the same machine, but would require a global filesystem like GFS if the workers ran on different machines.\n\n- A reasonable naming convention for intermediate files is `mr-X-Y`, where X is the Map task number, and Y is the reduce task number.\n\n- The worker's map task code will need a way to store intermediate key/value pairs in files in a way that can be correctly read back during reduce tasks. One possibility is to use Go's encoding/json package. To write key/value pairs in JSON format to an open file:\n\n  ```\n    enc := json.NewEncoder(file)\n    for _, kv := ... {\n      err := enc.Encode(&kv)\n  ```\n\n  and to read such a file back:\n\n  ```\n    dec := json.NewDecoder(file)\n    for {\n      var kv KeyValue\n      if err := dec.Decode(&kv); err != nil {\n        break\n      }\n      kva = append(kva, kv)\n    }\n  ```\n\n- The map part of your worker can use the `ihash(key)` function (in `worker.go`) to pick the reduce task for a given key.\n\n- You can steal some code from `mrsequential.go` for reading Map input files, for sorting intermediate key/value pairs between the Map and Reduce, and for storing Reduce output in files.\n\n- The coordinator, as an RPC server, will be concurrent; don't forget to lock shared data.\n\n- Use Go's race detector, with `go run -race`. `test-mr.sh` has a comment at the start that tells you how to run it with `-race`. When we grade your labs, we will **not** use the race detector. Nevertheless, if your code has races, there's a good chance it will fail when we test it even without the race detector.\n\n- Workers will sometimes need to wait, e.g. reduces can't start until the last map has finished. One possibility is for workers to periodically ask the coordinator for work, sleeping with `time.Sleep()` between each request. Another possibility is for the relevant RPC handler in the coordinator to have a loop that waits, either with `time.Sleep()` or `sync.Cond`. Go runs the handler for each RPC in its own thread, so the fact that one handler is waiting needn't prevent the coordinator from processing other RPCs.\n\n- The coordinator can't reliably distinguish between crashed workers, workers that are alive but have stalled for some reason, and workers that are executing but too slowly to be useful. The best you can do is have the coordinator wait for some amount of time, and then give up and re-issue the task to a different worker. For this lab, have the coordinator wait for ten seconds; after that the coordinator should assume the worker has died (of course, it might not have).\n\n- If you choose to implement Backup Tasks (Section 3.6), note that we test that your code doesn't schedule extraneous tasks when workers execute tasks without crashing. Backup tasks should only be scheduled after some relatively long period of time (e.g., 10s).\n\n- To test crash recovery, you can use the `mrapps/crash.go` application plugin. It randomly exits in the Map and Reduce functions.\n\n- To ensure that nobody observes partially written files in the presence of crashes, the MapReduce paper mentions the trick of using a temporary file and atomically renaming it once it is completely written. You can use `ioutil.TempFile` (or `os.CreateTemp` if you are running Go 1.17 or later) to create a temporary file and `os.Rename` to atomically rename it.\n\n- `test-mr.sh` runs all its processes in the sub-directory `mr-tmp`, so if something goes wrong and you want to look at intermediate or output files, look there. Feel free to temporarily modify `test-mr.sh` to `exit` after the failing test, so the script does not continue testing (and overwrite the output files).\n\n- `test-mr-many.sh` runs `test-mr.sh` many times in a row, which you may want to do in order to spot low-probability bugs. It takes as an argument the number of times to run the tests. You should not run several `test-mr.sh` instances in parallel because the coordinator will reuse the same socket, causing conflicts.\n\n- Go RPC sends only struct fields whose names start with capital letters. Sub-structures must also have capitalized field names.\n\n- When calling the RPC call() function, the reply struct should contain all default values. RPC calls should look like this:\n\n  ```\n    reply := SomeType{}\n    call(..., &reply)\n  ```\n\n  without setting any fields of reply before the call. If you pass reply structures that have non-default fields, the RPC system may silently return incorrect values.\n", "test_method": "cd src/main && bash test-mr.sh", "test_results": "*** Starting wc test.\n--- wc test: PASS\n*** Starting indexer test.\n--- indexer test: PASS\n*** Starting map parallelism test.\n--- map parallelism test: PASS\n*** Starting reduce parallelism test.\n--- reduce parallelism test: PASS\n*** Starting job count test.\n--- job count test: PASS\n*** Starting early exit test.\n--- early exit test: PASS\n*** Starting crash test.\n--- crash test: PASS\n*** PASSED ALL TESTS", "difficulty": "moderate/hard", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-mr.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_2", "task_name": "problems/system_lab_2.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a key/value server for a single machine that ensures that each operation is executed exactly once despite network failures and that the operations are [linearizable](http://nil.csail.mit.edu/6.5840/2024/papers/linearizability-faq.txt). Later labs will replicate a server like this one to handle server crashes.\n\nClients can send three different RPCs to the key/value server: `Put(key, value)`, `Append(key, arg)`, and `Get(key)`. The server maintains an in-memory map of key/value pairs. Keys and values are strings. `Put(key, value)` installs or replaces the value for a particular key in the map, `Append(key, arg)` appends arg to key's value *and* returns the old value, and `Get(key)` fetches the current value for the key. A `Get` for a non-existent key should return an empty string. An `Append` to a non-existent key should act as if the existing value were a zero-length string. Each client talks to the server through a `Clerk` with Put/Append/Get methods. A `Clerk` manages RPC interactions with the server.\n\nYour server must arrange that application calls to `Clerk` Get/Put/Append methods be linearizable. If client requests aren't concurrent, each client Get/Put/Append call should observe the modifications to the state implied by the preceding sequence of calls. For concurrent calls, the return values and final state must be the same as if the operations had executed one at a time in some order. Calls are concurrent if they overlap in time: for example, if client X calls `Clerk.Put()`, and client Y calls `Clerk.Append()`, and then client X's call returns. A call must observe the effects of all calls that have completed before the call starts.\n\nLinearizability is convenient for applications because it's the behavior you'd see from a single server that processes requests one at a time. For example, if one client gets a successful response from the server for an update request, subsequently launched reads from other clients are guaranteed to see the effects of that update. Providing linearizability is relatively easy for a single server.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvsrv`. You will need to modify `kvsrv/client.go`, `kvsrv/server.go`, and `kvsrv/common.go`.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/kvsrv\n$ go test\n...\n$\n```\n\n## The Code\n\n# Your Task \nYour first task is to implement a solution that works when there are no dropped messages.\n\nYou'll need to add RPC-sending code to the Clerk Put/Append/Get methods in `client.go`, and implement `Put`, `Append()` and `Get()` RPC handlers in `server.go`.\n\nYou have completed this task when you pass the first two tests in the test suite: \"one client\" and \"many clients\".\n\n- Check that your code is race-free using `go test -race`.", "test_method": "cd src/kvsr && go test", "test_results": "", "difficulty": "easy", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-kvsrv.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_3", "task_name": "problems/system_lab_3.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a key/value server for a single machine that ensures that each operation is executed exactly once despite network failures and that the operations are [linearizable](http://nil.csail.mit.edu/6.5840/2024/papers/linearizability-faq.txt). Later labs will replicate a server like this one to handle server crashes.\n\nClients can send three different RPCs to the key/value server: `Put(key, value)`, `Append(key, arg)`, and `Get(key)`. The server maintains an in-memory map of key/value pairs. Keys and values are strings. `Put(key, value)` installs or replaces the value for a particular key in the map, `Append(key, arg)` appends arg to key's value *and* returns the old value, and `Get(key)` fetches the current value for the key. A `Get` for a non-existent key should return an empty string. An `Append` to a non-existent key should act as if the existing value were a zero-length string. Each client talks to the server through a `Clerk` with Put/Append/Get methods. A `Clerk` manages RPC interactions with the server.\n\nYour server must arrange that application calls to `Clerk` Get/Put/Append methods be linearizable. If client requests aren't concurrent, each client Get/Put/Append call should observe the modifications to the state implied by the preceding sequence of calls. For concurrent calls, the return values and final state must be the same as if the operations had executed one at a time in some order. Calls are concurrent if they overlap in time: for example, if client X calls `Clerk.Put()`, and client Y calls `Clerk.Append()`, and then client X's call returns. A call must observe the effects of all calls that have completed before the call starts.\n\nLinearizability is convenient for applications because it's the behavior you'd see from a single server that processes requests one at a time. For example, if one client gets a successful response from the server for an update request, subsequently launched reads from other clients are guaranteed to see the effects of that update. Providing linearizability is relatively easy for a single server.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvsrv`. You will need to modify `kvsrv/client.go`, `kvsrv/server.go`, and `kvsrv/common.go`.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/kvsrv\n$ go test\n...\n$\n```\n\n## The Code\n\n# Your Task \nNow you should modify your solution to continue in the face of dropped messages (e.g., RPC requests and RPC replies). If a message was lost, then the client's `ck.server.Call()` will return `false` (more precisely, `Call()` waits for a reply message for a timeout interval, and returns false if no reply arrives within that time). One problem you'll face is that a `Clerk` may have to send an RPC multiple times until it succeeds. Each call to `Clerk.Put()` or `Clerk.Append()`, however, should result in just a *single* execution, so you will have to ensure that the re-send doesn't result in the server executing the request twice.\n\nAdd code to `Clerk` to retry if doesn't receive a reply, and to `server.go` to filter duplicates if the operation requires it. These notes include guidance on [duplicate detection](http://nil.csail.mit.edu/6.5840/2024/notes/l-raft-QA.txt).\n\n- You will need to uniquely identify client operations to ensure that the key/value server executes each one just once.\n- You will have to think carefully about what state the server must maintain for handling duplicate `Get()`, `Put()`, and `Append()` requests, if any at all.\n- Your scheme for duplicate detection should free server memory quickly, for example by having each RPC imply that the client has seen the reply for its previous RPC. It's OK to assume that a client will make only one call into a Clerk at a time.\n\nYour code should now pass all tests, like this:\n\n```\n$ go test\nTest: one client ...\n  ... Passed -- t  3.8 nrpc 31135 ops 31135\nTest: many clients ...\n  ... Passed -- t  4.7 nrpc 102853 ops 102853\nTest: unreliable net, many clients ...\n  ... Passed -- t  4.1 nrpc   580 ops  496\nTest: concurrent append to same key, unreliable ...\n  ... Passed -- t  0.6 nrpc    61 ops   52\nTest: memory use get ...\n  ... Passed -- t  0.4 nrpc     4 ops    0\nTest: memory use put ...\n  ... Passed -- t  0.2 nrpc     2 ops    0\nTest: memory use append ...\n  ... Passed -- t  0.4 nrpc     2 ops    0\nTest: memory use many puts ...\n  ... Passed -- t 11.5 nrpc 100000 ops    0\nTest: memory use many gets ...\n  ... Passed -- t 12.2 nrpc 100001 ops    0\nPASS\nok      6.5840/kvsrv    39.000s\n```\n\nThe numbers after each `Passed` are real time in seconds, number of RPCs sent (including client RPCs), and number of key/value operations executed (`Clerk` Get/Put/Append calls).\n", "test_method": "cd src/kvsr && go test", "test_results": "Test: one client ...\n  ... Passed -- t  3.8 nrpc 31135 ops 31135\nTest: many clients ...\n  ... Passed -- t  4.7 nrpc 102853 ops 102853\nTest: unreliable net, many clients ...\n  ... Passed -- t  4.1 nrpc   580 ops  496\nTest: concurrent append to same key, unreliable ...\n  ... Passed -- t  0.6 nrpc    61 ops   52\nTest: memory use get ...\n  ... Passed -- t  0.4 nrpc     4 ops    0\nTest: memory use put ...\n  ... Passed -- t  0.2 nrpc     2 ops    0\nTest: memory use append ...\n  ... Passed -- t  0.4 nrpc     2 ops    0\nTest: memory use many puts ...\n  ... Passed -- t 11.5 nrpc 100000 ops    0\nTest: memory use many gets ...\n  ... Passed -- t 12.2 nrpc 100001 ops    0\nPASS\nok      6.5840/kvsrv    39.000s", "difficulty": "easy", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-kvsrv.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_4", "task_name": "problems/system_lab_4.md", "task": "# Problem Context\n## Introduction\nThis is the first in a series of labs in which you'll build a fault-tolerant key/value storage system. In this lab you'll implement Raft, a replicated state machine protocol. In the next lab you'll build a key/value service on top of Raft. Then you will \u00c2\u0093shard\u00c2\u0094 your service over multiple replicated state machines for higher performance.\n\nA replicated service achieves fault tolerance by storing complete copies of its state (i.e., data) on multiple replica servers. Replication allows the service to continue operating even if some of its servers experience failures (crashes or a broken or flaky network). The challenge is that failures may cause the replicas to hold differing copies of the data.\n\nRaft organizes client requests into a sequence, called the log, and ensures that all the replica servers see the same log. Each replica executes client requests in log order, applying them to its local copy of the service's state. Since all the live replicas see the same log contents, they all execute the same requests in the same order, and thus continue to have identical service state. If a server fails but later recovers, Raft takes care of bringing its log up to date. Raft will continue to operate as long as at least a majority of the servers are alive and can talk to each other. If there is no such majority, Raft will make no progress, but will pick up where it left off as soon as a majority can communicate again.\n\nIn this lab you'll implement Raft as a Go object type with associated methods, meant to be used as a module in a larger service. A set of Raft instances talk to each other with RPC to maintain replicated logs. Your Raft interface will support an indefinite sequence of numbered commands, also called log entries. The entries are numbered with *index numbers*. The log entry with a given index will eventually be committed. At that point, your Raft should send the log entry to the larger service for it to execute.\n\nYou should follow the design in the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2024/papers/raft-extended.pdf), with particular attention to Figure 2. You'll implement most of what's in the paper, including saving persistent state and reading it after a node fails and then restarts. You will not implement cluster membership changes (Section 6).\n\nThis lab is due in four parts. You must submit each part on the corresponding due date.\n## Getiting Started\nIf you have done Lab 1, you already have a copy of the lab source code. If not, you can find directions for obtaining the source via git in the [Lab 1 instructions](http://nil.csail.mit.edu/6.5840/2024/labs/lab-mr.html).\n\nWe supply you with skeleton code `src/raft/raft.go`. We also supply a set of tests, which you should use to drive your implementation efforts, and which we'll use to grade your submitted lab. The tests are in `src/raft/test_test.go`.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should check that your code does not have races, by running the tests with the `-race` flag as you develop your solution.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/raft\n$ go test\nTest (3A): initial election ...\n--- FAIL: TestInitialElection3A (5.04s)\n        config.go:326: expected one leader, got none\nTest (3A): election after network failure ...\n--- FAIL: TestReElection3A (5.03s)\n        config.go:326: expected one leader, got none\n...\n$\n```\n## The Code\nImplement Raft by adding code to `raft/raft.go`. In that file you'll find skeleton code, plus examples of how to send and receive RPCs.\n\nYour implementation must support the following interface, which the tester and (eventually) your key/value server will use. You'll find more details in comments in `raft.go`.\n\n```\n// create a new Raft server instance:\nrf := Make(peers, me, persister, applyCh)\n\n// start agreement on a new log entry:\nrf.Start(command interface{}) (index, term, isleader)\n\n// ask a Raft for its current term, and whether it thinks it is leader\nrf.GetState() (term, isLeader)\n\n// each time a new entry is committed to the log, each Raft peer\n// should send an ApplyMsg to the service (or tester).\ntype ApplyMsg\n```\n\nA service calls `Make(peers,me,\u00c2\u0085)` to create a Raft peer. The peers argument is an array of network identifiers of the Raft peers (including this one), for use with RPC. The `me` argument is the index of this peer in the peers array. `Start(command)` asks Raft to start the processing to append the command to the replicated log. `Start()` should return immediately, without waiting for the log appends to complete. The service expects your implementation to send an `ApplyMsg` for each newly committed log entry to the `applyCh` channel argument to `Make()`.\n\n`raft.go` contains example code that sends an RPC (`sendRequestVote()`) and that handles an incoming RPC (`RequestVote()`). Your Raft peers should exchange RPCs using the labrpc Go package (source in `src/labrpc`). The tester can tell `labrpc` to delay RPCs, re-order them, and discard them to simulate various network failures. While you can temporarily modify `labrpc`, make sure your Raft works with the original `labrpc`, since that's what we'll use to test and grade your lab. Your Raft instances must interact only with RPC; for example, they are not allowed to communicate using shared Go variables or files.\n\nSubsequent labs build on this lab, so it is important to give yourself enough time to write solid code.\n# Your Task \nImplement Raft leader election and heartbeats (`AppendEntries` RPCs with no log entries). The goal for Part 3A is for a single leader to be elected, for the leader to remain the leader if there are no failures, and for a new leader to take over if the old leader fails or if packets to/from the old leader are lost. Run `go test -run 3A `to test your 3A code.\n\n- You can't easily run your Raft implementation directly; instead you should run it by way of the tester, i.e. `go test -run 3A `.\n- Follow the paper's Figure 2. At this point you care about sending and receiving RequestVote RPCs, the Rules for Servers that relate to elections, and the State related to leader election,\n- Add the Figure 2 state for leader election to the `Raft` struct in `raft.go`. You'll also need to define a struct to hold information about each log entry.\n- Fill in the `RequestVoteArgs` and `RequestVoteReply` structs. Modify `Make()` to create a background goroutine that will kick off leader election periodically by sending out `RequestVote` RPCs when it hasn't heard from another peer for a while. Implement the `RequestVote()` RPC handler so that servers will vote for one another.\n- To implement heartbeats, define an `AppendEntries` RPC struct (though you may not need all the arguments yet), and have the leader send them out periodically. Write an `AppendEntries` RPC handler method.\n- The tester requires that the leader send heartbeat RPCs no more than ten times per second.\n- The tester requires your Raft to elect a new leader within five seconds of the failure of the old leader (if a majority of peers can still communicate).\n- The paper's Section 5.2 mentions election timeouts in the range of 150 to 300 milliseconds. Such a range only makes sense if the leader sends heartbeats considerably more often than once per 150 milliseconds (e.g., once per 10 milliseconds). Because the tester limits you tens of heartbeats per second, you will have to use an election timeout larger than the paper's 150 to 300 milliseconds, but not too large, because then you may fail to elect a leader within five seconds.\n- You may find Go's [rand](https://golang.org/pkg/math/rand/) useful.\n- You'll need to write code that takes actions periodically or after delays in time. The easiest way to do this is to create a goroutine with a loop that calls [time.Sleep()](https://golang.org/pkg/time/#Sleep); see the `ticker()` goroutine that `Make()` creates for this purpose. Don't use Go's `time.Timer` or `time.Ticker`, which are difficult to use correctly.\n- If your code has trouble passing the tests, read the paper's Figure 2 again; the full logic for leader election is spread over multiple parts of the figure.\n- Don't forget to implement `GetState()`.\n- The tester calls your Raft's `rf.Kill()` when it is permanently shutting down an instance. You can check whether `Kill()` has been called using `rf.killed()`. You may want to do this in all loops, to avoid having dead Raft instances print confusing messages.\n- Go RPC sends only struct fields whose names start with capital letters. Sub-structures must also have capitalized field names (e.g. fields of log records in an array). The `labgob` package will warn you about this; don't ignore the warnings.\n- The most challenging part of this lab may be the debugging. Spend some time making your implementation easy to debug. Refer to the [Guidance](https://pdos.csail.mit.edu/6.824/labs/guidance.html) page for debugging tips.\n- If you fail a test, the tester produces a file that visualizes a timeline with events marked along it, including network partitions, crashed servers, and checks performed. Here's an [example of the visualization](https://pdos.csail.mit.edu/6.824/labs/vis.html). Further, you can add your own annotations by writing, for example, `tester.Annotate(\"Server 0\", \"short description\", \"details\")`. This is a new feature we added this year, so if you have any feedback regarding the visualizer (e.g., bug reports, what annotation APIs that you think might be helpful, what information you want the visualizer to show, etc.), please let us know!\n\nBe sure you pass the 3A tests before submitting Part 3A, so that you see something like this:\n\n```\n$ go test -run 3A\nTest (3A): initial election (reliable network)...\n  ... Passed --   3.6  3   106    0\nTest (3A): election after network failure (reliable network)...\n  ... Passed --   7.6  3   304    0\nTest (3A): multiple elections (reliable network)...\n  ... Passed --   8.4  7   954    0\nPASS\nok      6.5840/raft1    19.834sak\n$\n```\n\nEach \"Passed\" line contains five numbers; these are the time that the test took in seconds, the number of Raft peers, the number of RPCs sent during the test, the total number of bytes in the RPC messages, and the number of log entries that Raft reports were committed. Your numbers will differ from those shown here. You can ignore the numbers if you like, but they may help you sanity-check the number of RPCs that your implementation sends. For all of labs 3, 4, and 5, the grading script will fail your solution if it takes more than 600 seconds for all of the tests (`go test`), or if any individual test takes more than 120 seconds.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should make sure that your code consistently passes the tests with the `-race` flag.\n", "test_method": "cd src/raft && go test -run 3A", "test_results": "Test (3A): initial election ...\n  ... Passed --   3.5  3   58   16840    0\nTest (3A): election after network failure ...\n  ... Passed --   5.4  3  118   25269    0\nTest (3A): multiple elections ...\n  ... Passed --   7.3  7  624  138014    0\nPASS\nok  \t6.5840/raft\t16.265s", "difficulty": "moderate", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-raft.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_5", "task_name": "problems/system_lab_5.md", "task": "# Problem Context\n## Introduction\nThis is the first in a series of labs in which you'll build a fault-tolerant key/value storage system. In this lab you'll implement Raft, a replicated state machine protocol. In the next lab you'll build a key/value service on top of Raft. Then you will \u00c2\u0093shard\u00c2\u0094 your service over multiple replicated state machines for higher performance.\n\nA replicated service achieves fault tolerance by storing complete copies of its state (i.e., data) on multiple replica servers. Replication allows the service to continue operating even if some of its servers experience failures (crashes or a broken or flaky network). The challenge is that failures may cause the replicas to hold differing copies of the data.\n\nRaft organizes client requests into a sequence, called the log, and ensures that all the replica servers see the same log. Each replica executes client requests in log order, applying them to its local copy of the service's state. Since all the live replicas see the same log contents, they all execute the same requests in the same order, and thus continue to have identical service state. If a server fails but later recovers, Raft takes care of bringing its log up to date. Raft will continue to operate as long as at least a majority of the servers are alive and can talk to each other. If there is no such majority, Raft will make no progress, but will pick up where it left off as soon as a majority can communicate again.\n\nIn this lab you'll implement Raft as a Go object type with associated methods, meant to be used as a module in a larger service. A set of Raft instances talk to each other with RPC to maintain replicated logs. Your Raft interface will support an indefinite sequence of numbered commands, also called log entries. The entries are numbered with *index numbers*. The log entry with a given index will eventually be committed. At that point, your Raft should send the log entry to the larger service for it to execute.\n\nYou should follow the design in the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2024/papers/raft-extended.pdf), with particular attention to Figure 2. You'll implement most of what's in the paper, including saving persistent state and reading it after a node fails and then restarts. You will not implement cluster membership changes (Section 6).\n\nThis lab is due in four parts. You must submit each part on the corresponding due date.\n## Getiting Started\nIf you have done Lab 1, you already have a copy of the lab source code. If not, you can find directions for obtaining the source via git in the [Lab 1 instructions](http://nil.csail.mit.edu/6.5840/2024/labs/lab-mr.html).\n\nWe supply you with skeleton code `src/raft/raft.go`. We also supply a set of tests, which you should use to drive your implementation efforts, and which we'll use to grade your submitted lab. The tests are in `src/raft/test_test.go`.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should check that your code does not have races, by running the tests with the `-race` flag as you develop your solution.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/raft\n$ go test\nTest (3A): initial election ...\n--- FAIL: TestInitialElection3A (5.04s)\n        config.go:326: expected one leader, got none\nTest (3A): election after network failure ...\n--- FAIL: TestReElection3A (5.03s)\n        config.go:326: expected one leader, got none\n...\n$\n```\n## The Code\nImplement Raft by adding code to `raft/raft.go`. In that file you'll find skeleton code, plus examples of how to send and receive RPCs.\n\nYour implementation must support the following interface, which the tester and (eventually) your key/value server will use. You'll find more details in comments in `raft.go`.\n\n```\n// create a new Raft server instance:\nrf := Make(peers, me, persister, applyCh)\n\n// start agreement on a new log entry:\nrf.Start(command interface{}) (index, term, isleader)\n\n// ask a Raft for its current term, and whether it thinks it is leader\nrf.GetState() (term, isLeader)\n\n// each time a new entry is committed to the log, each Raft peer\n// should send an ApplyMsg to the service (or tester).\ntype ApplyMsg\n```\n\nA service calls `Make(peers,me,\u00c2\u0085)` to create a Raft peer. The peers argument is an array of network identifiers of the Raft peers (including this one), for use with RPC. The `me` argument is the index of this peer in the peers array. `Start(command)` asks Raft to start the processing to append the command to the replicated log. `Start()` should return immediately, without waiting for the log appends to complete. The service expects your implementation to send an `ApplyMsg` for each newly committed log entry to the `applyCh` channel argument to `Make()`.\n\n`raft.go` contains example code that sends an RPC (`sendRequestVote()`) and that handles an incoming RPC (`RequestVote()`). Your Raft peers should exchange RPCs using the labrpc Go package (source in `src/labrpc`). The tester can tell `labrpc` to delay RPCs, re-order them, and discard them to simulate various network failures. While you can temporarily modify `labrpc`, make sure your Raft works with the original `labrpc`, since that's what we'll use to test and grade your lab. Your Raft instances must interact only with RPC; for example, they are not allowed to communicate using shared Go variables or files.\n\nSubsequent labs build on this lab, so it is important to give yourself enough time to write solid code.\n# Your Task \nImplement the leader and follower code to append new log entries, so that the `go test -run 3B `tests pass.\n\n- Run `git pull` to get the latest lab software.\n- Your first goal should be to pass `TestBasicAgree3B()`. Start by implementing `Start()`, then write the code to send and receive new log entries via `AppendEntries` RPCs, following Figure 2. Send each newly committed entry on `applyCh` on each peer.\n- You will need to implement the election restriction (section 5.4.1 in the paper).\n- Your code may have loops that repeatedly check for certain events. Don't have these loops execute continuously without pausing, since that will slow your implementation enough that it fails tests. Use Go's [condition variables](https://golang.org/pkg/sync/#Cond), or insert a `time.Sleep(10 * time.Millisecond)` in each loop iteration.\n- Do yourself a favor for future labs and write (or re-write) code that's clean and clear. For ideas, re-visit our the [Guidance page](http://nil.csail.mit.edu/6.5840/2024/labs/guidance.html) with tips on how to develop and debug your code.\n- If you fail a test, look at `test_test.go` and `config.go` to understand what's being tested. `config.go` also illustrates how the tester uses the Raft API.\n\nThe tests for upcoming labs may fail your code if it runs too slowly. You can check how much real time and CPU time your solution uses with the time command. Here's typical output:\n\n```\n$ time go test -run 3B\nTest (3B): basic agreement ...\n  ... Passed --   0.9  3   16    4572    3\nTest (3B): RPC byte count ...\n  ... Passed --   1.7  3   48  114536   11\nTest (3B): agreement after follower reconnects ...\n  ... Passed --   3.6  3   78   22131    7\nTest (3B): no agreement if too many followers disconnect ...\n  ... Passed --   3.8  5  172   40935    3\nTest (3B): concurrent Start()s ...\n  ... Passed --   1.1  3   24    7379    6\nTest (3B): rejoin of partitioned leader ...\n  ... Passed --   5.1  3  152   37021    4\nTest (3B): leader backs up quickly over incorrect follower logs ...\n  ... Passed --  17.2  5 2080 1587388  102\nTest (3B): RPC counts aren't too high ...\n  ... Passed --   2.2  3   60   20119   12\nPASS\nok  \t6.5840/raft\t35.557s\n\nreal\t0m35.899s\nuser\t0m2.556s\nsys\t0m1.458s\n$\n```\n\nThe \"ok 6.5840/raft 35.557s\" means that Go measured the time taken for the 3B tests to be 35.557 seconds of real (wall-clock) time. The \"user 0m2.556s\" means that the code consumed 2.556 seconds of CPU time, or time spent actually executing instructions (rather than waiting or sleeping). If your solution uses much more than a minute of real time for the 3B tests, or much more than 5 seconds of CPU time, you may run into trouble later on. Look for time spent sleeping or waiting for RPC timeouts, loops that run without sleeping or waiting for conditions or channel messages, or large numbers of RPCs sent.\n\n### ", "test_method": "cd src/raft && time go test -run 3B", "test_results": "Test (3B): basic agreement ...\n  ... Passed --   0.9  3   16    4572    3\nTest (3B): RPC byte count ...\n  ... Passed --   1.7  3   48  114536   11\nTest (3B): agreement after follower reconnects ...\n  ... Passed --   3.6  3   78   22131    7\nTest (3B): no agreement if too many followers disconnect ...\n  ... Passed --   3.8  5  172   40935    3\nTest (3B): concurrent Start()s ...\n  ... Passed --   1.1  3   24    7379    6\nTest (3B): rejoin of partitioned leader ...\n  ... Passed --   5.1  3  152   37021    4\nTest (3B): leader backs up quickly over incorrect follower logs ...\n  ... Passed --  17.2  5 2080 1587388  102\nTest (3B): RPC counts aren't too high ...\n  ... Passed --   2.2  3   60   20119   12\nPASS\nok  \t6.5840/raft\t35.557s\n\nreal\t0m35.899s\nuser\t0m2.556s\nsys\t0m1.458s", "difficulty": "hard", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-raft.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_6", "task_name": "problems/system_lab_6.md", "task": "# Problem Context\n## Introduction\nThis is the first in a series of labs in which you'll build a fault-tolerant key/value storage system. In this lab you'll implement Raft, a replicated state machine protocol. In the next lab you'll build a key/value service on top of Raft. Then you will \u00c2\u0093shard\u00c2\u0094 your service over multiple replicated state machines for higher performance.\n\nA replicated service achieves fault tolerance by storing complete copies of its state (i.e., data) on multiple replica servers. Replication allows the service to continue operating even if some of its servers experience failures (crashes or a broken or flaky network). The challenge is that failures may cause the replicas to hold differing copies of the data.\n\nRaft organizes client requests into a sequence, called the log, and ensures that all the replica servers see the same log. Each replica executes client requests in log order, applying them to its local copy of the service's state. Since all the live replicas see the same log contents, they all execute the same requests in the same order, and thus continue to have identical service state. If a server fails but later recovers, Raft takes care of bringing its log up to date. Raft will continue to operate as long as at least a majority of the servers are alive and can talk to each other. If there is no such majority, Raft will make no progress, but will pick up where it left off as soon as a majority can communicate again.\n\nIn this lab you'll implement Raft as a Go object type with associated methods, meant to be used as a module in a larger service. A set of Raft instances talk to each other with RPC to maintain replicated logs. Your Raft interface will support an indefinite sequence of numbered commands, also called log entries. The entries are numbered with *index numbers*. The log entry with a given index will eventually be committed. At that point, your Raft should send the log entry to the larger service for it to execute.\n\nYou should follow the design in the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2024/papers/raft-extended.pdf), with particular attention to Figure 2. You'll implement most of what's in the paper, including saving persistent state and reading it after a node fails and then restarts. You will not implement cluster membership changes (Section 6).\n\nThis lab is due in four parts. You must submit each part on the corresponding due date.\n## Getiting Started\nIf you have done Lab 1, you already have a copy of the lab source code. If not, you can find directions for obtaining the source via git in the [Lab 1 instructions](http://nil.csail.mit.edu/6.5840/2024/labs/lab-mr.html).\n\nWe supply you with skeleton code `src/raft/raft.go`. We also supply a set of tests, which you should use to drive your implementation efforts, and which we'll use to grade your submitted lab. The tests are in `src/raft/test_test.go`.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should check that your code does not have races, by running the tests with the `-race` flag as you develop your solution.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/raft\n$ go test\nTest (3A): initial election ...\n--- FAIL: TestInitialElection3A (5.04s)\n        config.go:326: expected one leader, got none\nTest (3A): election after network failure ...\n--- FAIL: TestReElection3A (5.03s)\n        config.go:326: expected one leader, got none\n...\n$\n```\n## The Code\nImplement Raft by adding code to `raft/raft.go`. In that file you'll find skeleton code, plus examples of how to send and receive RPCs.\n\nYour implementation must support the following interface, which the tester and (eventually) your key/value server will use. You'll find more details in comments in `raft.go`.\n\n```\n// create a new Raft server instance:\nrf := Make(peers, me, persister, applyCh)\n\n// start agreement on a new log entry:\nrf.Start(command interface{}) (index, term, isleader)\n\n// ask a Raft for its current term, and whether it thinks it is leader\nrf.GetState() (term, isLeader)\n\n// each time a new entry is committed to the log, each Raft peer\n// should send an ApplyMsg to the service (or tester).\ntype ApplyMsg\n```\n\nA service calls `Make(peers,me,\u00c2\u0085)` to create a Raft peer. The peers argument is an array of network identifiers of the Raft peers (including this one), for use with RPC. The `me` argument is the index of this peer in the peers array. `Start(command)` asks Raft to start the processing to append the command to the replicated log. `Start()` should return immediately, without waiting for the log appends to complete. The service expects your implementation to send an `ApplyMsg` for each newly committed log entry to the `applyCh` channel argument to `Make()`.\n\n`raft.go` contains example code that sends an RPC (`sendRequestVote()`) and that handles an incoming RPC (`RequestVote()`). Your Raft peers should exchange RPCs using the labrpc Go package (source in `src/labrpc`). The tester can tell `labrpc` to delay RPCs, re-order them, and discard them to simulate various network failures. While you can temporarily modify `labrpc`, make sure your Raft works with the original `labrpc`, since that's what we'll use to test and grade your lab. Your Raft instances must interact only with RPC; for example, they are not allowed to communicate using shared Go variables or files.\n\nSubsequent labs build on this lab, so it is important to give yourself enough time to write solid code.\n# Your Task \nIf a Raft-based server reboots it should resume service where it left off. This requires that Raft keep persistent state that survives a reboot. The paper's Figure 2 mentions which state should be persistent.\n\nA real implementation would write Raft's persistent state to disk each time it changed, and would read the state from disk when restarting after a reboot. Your implementation won't use the disk; instead, it will save and restore persistent state from a `Persister` object (see `persister.go`). Whoever calls `Raft.Make()` supplies a `Persister` that initially holds Raft's most recently persisted state (if any). Raft should initialize its state from that `Persister`, and should use it to save its persistent state each time the state changes. Use the `Persister`'s `ReadRaftState()` and `Save()` methods.\n\nComplete the functions `persist()` and `readPersist()` in `raft.go` by adding code to save and restore persistent state. You will need to encode (or \"serialize\") the state as an array of bytes in order to pass it to the `Persister`. Use the `labgob` encoder; see the comments in `persist()` and `readPersist()`. `labgob` is like Go's `gob` encoder but prints error messages if you try to encode structures with lower-case field names. For now, pass `nil` as the second argument to `persister.Save()`. Insert calls to `persist()` at the points where your implementation changes persistent state. Once you've done this, and if the rest of your implementation is correct, you should pass all of the 3C tests.\n\nYou will probably need the optimization that backs up nextIndex by more than one entry at a time. Look at the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2024/papers/raft-extended.pdf) starting at the bottom of page 7 and top of page 8 (marked by a gray line). The paper is vague about the details; you will need to fill in the gaps. One possibility is to have a rejection message include:\n\n```\n    XTerm:  term in the conflicting entry (if any)\n    XIndex: index of first entry with that term (if any)\n    XLen:   log length\n```\n\nThen the leader's logic can be something like:\n\n```\n  Case 1: leader doesn't have XTerm:\n    nextIndex = XIndex\n  Case 2: leader has XTerm:\n    nextIndex = leader's last entry for XTerm\n  Case 3: follower's log is too short:\n    nextIndex = XLen\n```\n\nA few other hints:\n\n- Run `git pull` to get the latest lab software.\n- The 3C tests are more demanding than those for 3A or 3B, and failures may be caused by problems in your code for 3A or 3B.\n\nYour code should pass all the 3C tests (as shown below), as well as the 3A and 3B tests.\n\n```\n$ go test -run 3C\nTest (3C): basic persistence ...\n  ... Passed --   5.0  3   86   22849    6\nTest (3C): more persistence ...\n  ... Passed --  17.6  5  952  218854   16\nTest (3C): partitioned leader and one follower crash, leader restarts ...\n  ... Passed --   2.0  3   34    8937    4\nTest (3C): Figure 8 ...\n  ... Passed --  31.2  5  580  130675   32\nTest (3C): unreliable agreement ...\n  ... Passed --   1.7  5 1044  366392  246\nTest (3C): Figure 8 (unreliable) ...\n  ... Passed --  33.6  5 10700 33695245  308\nTest (3C): churn ...\n  ... Passed --  16.1  5 8864 44771259 1544\nTest (3C): unreliable churn ...\n  ... Passed --  16.5  5 4220 6414632  906\nPASS\nok  \t6.5840/raft\t123.564s\n$\n```\n\nIt is a good idea to run the tests multiple times before submitting and check that each run prints `PASS`.\n\n```\n$ for i in {0..10}; do go test; done\n```", "test_method": "cd src/raft && go test -run 3C", "test_results": "Test (3C): basic persistence ...\n  ... Passed --   5.0  3   86   22849    6\nTest (3C): more persistence ...\n  ... Passed --  17.6  5  952  218854   16\nTest (3C): partitioned leader and one follower crash, leader restarts ...\n  ... Passed --   2.0  3   34    8937    4\nTest (3C): Figure 8 ...\n  ... Passed --  31.2  5  580  130675   32\nTest (3C): unreliable agreement ...\n  ... Passed --   1.7  5 1044  366392  246\nTest (3C): Figure 8 (unreliable) ...\n  ... Passed --  33.6  5 10700 33695245  308\nTest (3C): churn ...\n  ... Passed --  16.1  5 8864 44771259 1544\nTest (3C): unreliable churn ...\n  ... Passed --  16.5  5 4220 6414632  906\nPASS\nok  \t6.5840/raft\t123.564s", "difficulty": "hard", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-raft.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_7", "task_name": "problems/system_lab_7.md", "task": "# Problem Context\n## Introduction\nThis is the first in a series of labs in which you'll build a fault-tolerant key/value storage system. In this lab you'll implement Raft, a replicated state machine protocol. In the next lab you'll build a key/value service on top of Raft. Then you will \u00c2\u0093shard\u00c2\u0094 your service over multiple replicated state machines for higher performance.\n\nA replicated service achieves fault tolerance by storing complete copies of its state (i.e., data) on multiple replica servers. Replication allows the service to continue operating even if some of its servers experience failures (crashes or a broken or flaky network). The challenge is that failures may cause the replicas to hold differing copies of the data.\n\nRaft organizes client requests into a sequence, called the log, and ensures that all the replica servers see the same log. Each replica executes client requests in log order, applying them to its local copy of the service's state. Since all the live replicas see the same log contents, they all execute the same requests in the same order, and thus continue to have identical service state. If a server fails but later recovers, Raft takes care of bringing its log up to date. Raft will continue to operate as long as at least a majority of the servers are alive and can talk to each other. If there is no such majority, Raft will make no progress, but will pick up where it left off as soon as a majority can communicate again.\n\nIn this lab you'll implement Raft as a Go object type with associated methods, meant to be used as a module in a larger service. A set of Raft instances talk to each other with RPC to maintain replicated logs. Your Raft interface will support an indefinite sequence of numbered commands, also called log entries. The entries are numbered with *index numbers*. The log entry with a given index will eventually be committed. At that point, your Raft should send the log entry to the larger service for it to execute.\n\nYou should follow the design in the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2024/papers/raft-extended.pdf), with particular attention to Figure 2. You'll implement most of what's in the paper, including saving persistent state and reading it after a node fails and then restarts. You will not implement cluster membership changes (Section 6).\n\nThis lab is due in four parts. You must submit each part on the corresponding due date.\n## Getiting Started\nIf you have done Lab 1, you already have a copy of the lab source code. If not, you can find directions for obtaining the source via git in the [Lab 1 instructions](http://nil.csail.mit.edu/6.5840/2024/labs/lab-mr.html).\n\nWe supply you with skeleton code `src/raft/raft.go`. We also supply a set of tests, which you should use to drive your implementation efforts, and which we'll use to grade your submitted lab. The tests are in `src/raft/test_test.go`.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should check that your code does not have races, by running the tests with the `-race` flag as you develop your solution.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/raft\n$ go test\nTest (3A): initial election ...\n--- FAIL: TestInitialElection3A (5.04s)\n        config.go:326: expected one leader, got none\nTest (3A): election after network failure ...\n--- FAIL: TestReElection3A (5.03s)\n        config.go:326: expected one leader, got none\n...\n$\n```\n## The Code\nImplement Raft by adding code to `raft/raft.go`. In that file you'll find skeleton code, plus examples of how to send and receive RPCs.\n\nYour implementation must support the following interface, which the tester and (eventually) your key/value server will use. You'll find more details in comments in `raft.go`.\n\n```\n// create a new Raft server instance:\nrf := Make(peers, me, persister, applyCh)\n\n// start agreement on a new log entry:\nrf.Start(command interface{}) (index, term, isleader)\n\n// ask a Raft for its current term, and whether it thinks it is leader\nrf.GetState() (term, isLeader)\n\n// each time a new entry is committed to the log, each Raft peer\n// should send an ApplyMsg to the service (or tester).\ntype ApplyMsg\n```\n\nA service calls `Make(peers,me,\u00c2\u0085)` to create a Raft peer. The peers argument is an array of network identifiers of the Raft peers (including this one), for use with RPC. The `me` argument is the index of this peer in the peers array. `Start(command)` asks Raft to start the processing to append the command to the replicated log. `Start()` should return immediately, without waiting for the log appends to complete. The service expects your implementation to send an `ApplyMsg` for each newly committed log entry to the `applyCh` channel argument to `Make()`.\n\n`raft.go` contains example code that sends an RPC (`sendRequestVote()`) and that handles an incoming RPC (`RequestVote()`). Your Raft peers should exchange RPCs using the labrpc Go package (source in `src/labrpc`). The tester can tell `labrpc` to delay RPCs, re-order them, and discard them to simulate various network failures. While you can temporarily modify `labrpc`, make sure your Raft works with the original `labrpc`, since that's what we'll use to test and grade your lab. Your Raft instances must interact only with RPC; for example, they are not allowed to communicate using shared Go variables or files.\n\nSubsequent labs build on this lab, so it is important to give yourself enough time to write solid code.\n# Your Task \nAs things stand now, a rebooting server replays the complete Raft log in order to restore its state. However, it's not practical for a long-running service to remember the complete Raft log forever. Instead, you'll modify Raft to cooperate with services that persistently store a \"snapshot\" of their state from time to time, at which point Raft discards log entries that precede the snapshot. The result is a smaller amount of persistent data and faster restart. However, it's now possible for a follower to fall so far behind that the leader has discarded the log entries it needs to catch up; the leader must then send a snapshot plus the log starting at the time of the snapshot. Section 7 of the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2024/papers/raft-extended.pdf) outlines the scheme; you will have to design the details.\n\nYour Raft must provide the following function that the service can call with a serialized snapshot of its state:\n\n```\nSnapshot(index int, snapshot []byte)\n```\n\nIn Lab 3D, the tester calls `Snapshot()` periodically. In Lab 4, you will write a key/value server that calls `Snapshot()`; the snapshot will contain the complete table of key/value pairs. The service layer calls `Snapshot()` on every peer (not just on the leader).\n\nThe `index` argument indicates the highest log entry that's reflected in the snapshot. Raft should discard its log entries before that point. You'll need to revise your Raft code to operate while storing only the tail of the log.\n\nYou'll need to implement the `InstallSnapshot` RPC discussed in the paper that allows a Raft leader to tell a lagging Raft peer to replace its state with a snapshot. You will likely need to think through how InstallSnapshot should interact with the state and rules in Figure 2.\n\nWhen a follower's Raft code receives an InstallSnapshot RPC, it can use the `applyCh` to send the snapshot to the service in an `ApplyMsg`. The `ApplyMsg` struct definition already contains the fields you will need (and which the tester expects). Take care that these snapshots only advance the service's state, and don't cause it to move backwards.\n\nIf a server crashes, it must restart from persisted data. Your Raft should persist both Raft state and the corresponding snapshot. Use the second argument to `persister.Save()` to save the snapshot. If there's no snapshot, pass `nil` as the second argument.\n\nWhen a server restarts, the application layer reads the persisted snapshot and restores its saved state.\n\nImplement `Snapshot()` and the InstallSnapshot RPC, as well as the changes to Raft to support these (e.g, operation with a trimmed log). Your solution is complete when it passes the 3D tests (and all the previous Lab 3 tests).\n\n- `git pull` to make sure you have the latest software.\n- A good place to start is to modify your code to so that it is able to store just the part of the log starting at some index X. Initially you can set X to zero and run the 3B/3C tests. Then make `Snapshot(index)` discard the log before `index`, and set X equal to `index`. If all goes well you should now pass the first 3D test.\n- Next: have the leader send an InstallSnapshot RPC if it doesn't have the log entries required to bring a follower up to date.\n- Send the entire snapshot in a single InstallSnapshot RPC. Don't implement Figure 13's `offset` mechanism for splitting up the snapshot.\n- Raft must discard old log entries in a way that allows the Go garbage collector to free and re-use the memory; this requires that there be no reachable references (pointers) to the discarded log entries.\n- A reasonable amount of time to consume for the full set of Lab 3 tests (3A+3B+3C+3D) without `-race` is 6 minutes of real time and one minute of CPU time. When running with `-race`, it is about 10 minutes of real time and two minutes of CPU time.\n\nYour code should pass all the 3D tests (as shown below), as well as the 3A, 3B, and 3C tests.\n\n```\n$ go test -run 3D\nTest (3D): snapshots basic ...\n  ... Passed --  11.6  3  176   61716  192\nTest (3D): install snapshots (disconnect) ...\n  ... Passed --  64.2  3  878  320610  336\nTest (3D): install snapshots (disconnect+unreliable) ...\n  ... Passed --  81.1  3 1059  375850  341\nTest (3D): install snapshots (crash) ...\n  ... Passed --  53.5  3  601  256638  339\nTest (3D): install snapshots (unreliable+crash) ...\n  ... Passed --  63.5  3  687  288294  336\nTest (3D): crash and restart all servers ...\n  ... Passed --  19.5  3  268   81352   58\nPASS\nok      6.5840/raft      293.456s\n```", "test_method": "cd src/raft && go test -run 3D", "test_results": "Test (3D): snapshots basic ...\n  ... Passed --  11.6  3  176   61716  192\nTest (3D): install snapshots (disconnect) ...\n  ... Passed --  64.2  3  878  320610  336\nTest (3D): install snapshots (disconnect+unreliable) ...\n  ... Passed --  81.1  3 1059  375850  341\nTest (3D): install snapshots (crash) ...\n  ... Passed --  53.5  3  601  256638  339\nTest (3D): install snapshots (unreliable+crash) ...\n  ... Passed --  63.5  3  687  288294  336\nTest (3D): crash and restart all servers ...\n  ... Passed --  19.5  3  268   81352   58\nPASS\nok      6.5840/raft      293.456s", "difficulty": "hard", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-raft.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_8", "task_name": "problems/system_lab_8.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a fault-tolerant key/value storage service using your Raft library from [Lab 3](http://nil.csail.mit.edu/6.5840/2024/labs/lab-raft.html). Your key/value service will be a replicated state machine, consisting of several key/value servers that each maintain a database of key/value pairs, as in [Lab 2](http://nil.csail.mit.edu/6.5840/2024/labs/lab-raft.html), but additionally use Raft for replication. Your key/value service should continue to process client requests as long as a majority of the servers are alive and can communicate, in spite of other failures or network partitions. After Lab 4, you will have implemented all parts (Clerk, Service, and Raft) shown in the [diagram of Raft interactions](http://nil.csail.mit.edu/6.5840/2024/notes/raft_diagram.pdf).\n\nClients will interact with your key/value service in much the same way as Lab 2. In particular, clients can send three different RPCs to the key/value service:\n\n- `Put(key, value)`: replaces the value for a particular key in the database\n- `Append(key, arg)`: appends arg to key's value (treating the existing value as an empty string if the key is non-existent)\n- `Get(key)`: fetches the current value of the key (returning the empty string for non-existent keys)\n\nKeys and values are strings. Note that unlike in Lab 2, neither `Put` nor `Append` should return a value to the client. Each client talks to the service through a `Clerk` with Put/Append/Get methods. The `Clerk` manages RPC interactions with the servers.\n\nYour service must arrange that application calls to `Clerk` Get/Put/Append methods be linearizable. If called one at a time, the Get/Put/Append methods should act as if the system had only one copy of its state, and each call should observe the modifications to the state implied by the preceding sequence of calls. For concurrent calls, the return values and final state must be the same as if the operations had executed one at a time in some order. Calls are concurrent if they overlap in time: for example, if client X calls `Clerk.Put()`, and client Y calls `Clerk.Append()`, and then client X's call returns. A call must observe the effects of all calls that have completed before the call starts.\n\nProviding linearizability is relatively easy for a single server. It is harder if the service is replicated, since all servers must choose the same execution order for concurrent requests, must avoid replying to clients using state that isn't up to date, and must recover their state after a failure in a way that preserves all acknowledged client updates.\n\nThis lab has two parts. In part A, you will implement a replicated key/value service using your Raft implementation, but without using snapshots. In part B, you will use your snapshot implementation from Lab 3D, which will allow Raft to discard old log entries. Please submit each part by the respective deadline.\n\nYou should review the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2024/papers/raft-extended.pdf), in particular Sections 7 and 8. For a wider perspective, have a look at Chubby, Paxos Made Live, Spanner, Zookeeper, Harp, Viewstamped Replication, and [Bolosky et al.](http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf)\n\nStart early.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvraft`. You will need to modify `kvraft/client.go`, `kvraft/server.go`, and perhaps `kvraft/common.go`.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/kvraft\n$ go test\n...\n$\n```\n\n## The Code\n\n# Your Task \nEach of your key/value servers (\"kvservers\") will have an associated Raft peer. Clerks send `Put()`, `Append()`, and `Get()` RPCs to the kvserver whose associated Raft is the leader. The kvserver code submits the Put/Append/Get operation to Raft, so that the Raft log holds a sequence of Put/Append/Get operations. All of the kvservers execute operations from the Raft log in order, applying the operations to their key/value databases; the intent is for the servers to maintain identical replicas of the key/value database.\n\nA `Clerk` sometimes doesn't know which kvserver is the Raft leader. If the `Clerk` sends an RPC to the wrong kvserver, or if it cannot reach the kvserver, the `Clerk` should re-try by sending to a different kvserver. If the key/value service commits the operation to its Raft log (and hence applies the operation to the key/value state machine), the leader reports the result to the `Clerk` by responding to its RPC. If the operation failed to commit (for example, if the leader was replaced), the server reports an error, and the `Clerk` retries with a different server.\n\nYour kvservers should not directly communicate; they should only interact with each other through Raft.\n\nYour first task is to implement a solution that works when there are no dropped messages, and no failed servers.\n\nFeel free to copy over your client code from Lab 2 (`kvsrv/client.go`) into `kvraft/client.go`. You will need to add logic for deciding which kvserver to send each RPC to. Recall that `Append()` no longer returns a value to the Clerk.\n\nYou'll also need to implement `Put()`, `Append()`, and `Get()` RPC handlers in `server.go`. These handlers should enter an `Op` in the Raft log using `Start()`; you should fill in the `Op` struct definition in `server.go` so that it describes a Put/Append/Get operation. Each server should execute `Op` commands as Raft commits them, i.e. as they appear on the `applyCh`. An RPC handler should notice when Raft commits its `Op`, and then reply to the RPC.\n\nYou have completed this task when you **reliably** pass the first test in the test suite: \"One client\".\n\n- After calling `Start()`, your kvservers will need to wait for Raft to complete agreement. Commands that have been agreed upon arrive on the `applyCh`. Your code will need to keep reading `applyCh` while `Put()`, `Append()`, and `Get()` handlers submit commands to the Raft log using `Start()`. Beware of deadlock between the kvserver and its Raft library.\n- A kvserver should not complete a `Get()` RPC if it is not part of a majority (so that it does not serve stale data). A simple solution is to enter every `Get()` (as well as each `Put()` and `Append()`) in the Raft log. You don't have to implement the optimization for read-only operations that is described in Section 8.\n- You should not need to add any fields to to the Raft `ApplyMsg`, or to Raft RPCs such as `AppendEntries`, but you are allowed to do so.\n- It's best to add locking from the start because the need to avoid deadlocks sometimes affects overall code design. Check that your code is race-free using `go test -race`.\n\nNow you should modify your solution to continue in the face of network and server failures. One problem you'll face is that a `Clerk` may have to send an RPC multiple times until it finds a kvserver that replies positively. If a leader fails just after committing an entry to the Raft log, the `Clerk` may not receive a reply, and thus may re-send the request to another leader. Each call to `Clerk.Put()` or `Clerk.Append()` should result in just a single execution, so you will have to ensure that the re-send doesn't result in the servers executing the request twice.\n\nAdd code to handle failures, and to cope with duplicate `Clerk` requests, including situations where the `Clerk` sends a request to a kvserver leader in one term, times out waiting for a reply, and re-sends the request to a new leader in another term. The request should execute just once. These notes include guidance on [duplicate detection](http://nil.csail.mit.edu/6.5840/2024/notes/l-raft-QA.txt). Your code should pass the `go test -run 4A` tests.\n\n- Your solution needs to handle a leader that has called Start() for a Clerk's RPC, but loses its leadership before the request is committed to the log. In this case you should arrange for the Clerk to re-send the request to other servers until it finds the new leader. One way to do this is for the server to detect that it has lost leadership, by noticing that Raft's term has changed or a different request has appeared at the index returned by Start(). If the ex-leader is partitioned by itself, it won't know about new leaders; but any client in the same partition won't be able to talk to a new leader either, so it's OK in this case for the server and client to wait indefinitely until the partition heals.\n- You will probably have to modify your Clerk to remember which server turned out to be the leader for the last RPC, and send the next RPC to that server first. This will avoid wasting time searching for the leader on every RPC, which may help you pass some of the tests quickly enough.\n- You should use a duplicate detection scheme similar to Lab 2. It should free server memory quickly, for example by having each RPC imply that the client has seen the reply for its previous RPC. It's OK to assume that a client will make only one call into a Clerk at a time. You may find that you need to make changes to what information you store in your duplicate detection table from Lab 2.\n\nYour code should now pass the Lab 4A tests, like this:\n\n```\n$ go test -run 4A\nTest: one client (4A) ...\n  ... Passed --  15.5  5  4576  903\nTest: ops complete fast enough (4A) ...\n  ... Passed --  15.7  3  3022    0\nTest: many clients (4A) ...\n  ... Passed --  15.9  5  5884 1160\nTest: unreliable net, many clients (4A) ...\n  ... Passed --  19.2  5  3083  441\nTest: concurrent append to same key, unreliable (4A) ...\n  ... Passed --   2.5  3   218   52\nTest: progress in majority (4A) ...\n  ... Passed --   1.7  5   103    2\nTest: no progress in minority (4A) ...\n  ... Passed --   1.0  5   102    3\nTest: completion after heal (4A) ...\n  ... Passed --   1.2  5    70    3\nTest: partitions, one client (4A) ...\n  ... Passed --  23.8  5  4501  765\nTest: partitions, many clients (4A) ...\n  ... Passed --  23.5  5  5692  974\nTest: restarts, one client (4A) ...\n  ... Passed --  22.2  5  4721  908\nTest: restarts, many clients (4A) ...\n  ... Passed --  22.5  5  5490 1033\nTest: unreliable net, restarts, many clients (4A) ...\n  ... Passed --  26.5  5  3532  474\nTest: restarts, partitions, many clients (4A) ...\n  ... Passed --  29.7  5  6122 1060\nTest: unreliable net, restarts, partitions, many clients (4A) ...\n  ... Passed --  32.9  5  2967  317\nTest: unreliable net, restarts, partitions, random keys, many clients (4A) ...\n  ... Passed --  35.0  7  8249  746\nPASS\nok  \t6.5840/kvraft\t290.184s\n```\n\nThe numbers after each `Passed` are real time in seconds, number of peers, number of RPCs sent (including client RPCs), and number of key/value operations executed (`Clerk` Get/Put/Append calls).", "test_method": "cd src/kvraft && go test -run 4A", "test_results": "Test: one client (4A) ...\n  ... Passed --  15.5  5  4576  903\nTest: ops complete fast enough (4A) ...\n  ... Passed --  15.7  3  3022    0\nTest: many clients (4A) ...\n  ... Passed --  15.9  5  5884 1160\nTest: unreliable net, many clients (4A) ...\n  ... Passed --  19.2  5  3083  441\nTest: concurrent append to same key, unreliable (4A) ...\n  ... Passed --   2.5  3   218   52\nTest: progress in majority (4A) ...\n  ... Passed --   1.7  5   103    2\nTest: no progress in minority (4A) ...\n  ... Passed --   1.0  5   102    3\nTest: completion after heal (4A) ...\n  ... Passed --   1.2  5    70    3\nTest: partitions, one client (4A) ...\n  ... Passed --  23.8  5  4501  765\nTest: partitions, many clients (4A) ...\n  ... Passed --  23.5  5  5692  974\nTest: restarts, one client (4A) ...\n  ... Passed --  22.2  5  4721  908\nTest: restarts, many clients (4A) ...\n  ... Passed --  22.5  5  5490 1033\nTest: unreliable net, restarts, many clients (4A) ...\n  ... Passed --  26.5  5  3532  474\nTest: restarts, partitions, many clients (4A) ...\n  ... Passed --  29.7  5  6122 1060\nTest: unreliable net, restarts, partitions, many clients (4A) ...\n  ... Passed --  32.9  5  2967  317\nTest: unreliable net, restarts, partitions, random keys, many clients (4A) ...\n  ... Passed --  35.0  7  8249  746\nPASS\nok  \t6.5840/kvraft\t290.184s", "difficulty": "moderate/hard", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-kvraft.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_9", "task_name": "problems/system_lab_9.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a fault-tolerant key/value storage service using your Raft library from [Lab 3](http://nil.csail.mit.edu/6.5840/2024/labs/lab-raft.html). Your key/value service will be a replicated state machine, consisting of several key/value servers that each maintain a database of key/value pairs, as in [Lab 2](http://nil.csail.mit.edu/6.5840/2024/labs/lab-raft.html), but additionally use Raft for replication. Your key/value service should continue to process client requests as long as a majority of the servers are alive and can communicate, in spite of other failures or network partitions. After Lab 4, you will have implemented all parts (Clerk, Service, and Raft) shown in the [diagram of Raft interactions](http://nil.csail.mit.edu/6.5840/2024/notes/raft_diagram.pdf).\n\nClients will interact with your key/value service in much the same way as Lab 2. In particular, clients can send three different RPCs to the key/value service:\n\n- `Put(key, value)`: replaces the value for a particular key in the database\n- `Append(key, arg)`: appends arg to key's value (treating the existing value as an empty string if the key is non-existent)\n- `Get(key)`: fetches the current value of the key (returning the empty string for non-existent keys)\n\nKeys and values are strings. Note that unlike in Lab 2, neither `Put` nor `Append` should return a value to the client. Each client talks to the service through a `Clerk` with Put/Append/Get methods. The `Clerk` manages RPC interactions with the servers.\n\nYour service must arrange that application calls to `Clerk` Get/Put/Append methods be linearizable. If called one at a time, the Get/Put/Append methods should act as if the system had only one copy of its state, and each call should observe the modifications to the state implied by the preceding sequence of calls. For concurrent calls, the return values and final state must be the same as if the operations had executed one at a time in some order. Calls are concurrent if they overlap in time: for example, if client X calls `Clerk.Put()`, and client Y calls `Clerk.Append()`, and then client X's call returns. A call must observe the effects of all calls that have completed before the call starts.\n\nProviding linearizability is relatively easy for a single server. It is harder if the service is replicated, since all servers must choose the same execution order for concurrent requests, must avoid replying to clients using state that isn't up to date, and must recover their state after a failure in a way that preserves all acknowledged client updates.\n\nThis lab has two parts. In part A, you will implement a replicated key/value service using your Raft implementation, but without using snapshots. In part B, you will use your snapshot implementation from Lab 3D, which will allow Raft to discard old log entries. Please submit each part by the respective deadline.\n\nYou should review the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2024/papers/raft-extended.pdf), in particular Sections 7 and 8. For a wider perspective, have a look at Chubby, Paxos Made Live, Spanner, Zookeeper, Harp, Viewstamped Replication, and [Bolosky et al.](http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf)\n\nStart early.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvraft`. You will need to modify `kvraft/client.go`, `kvraft/server.go`, and perhaps `kvraft/common.go`.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/kvraft\n$ go test\n...\n$\n```\n\n## The Code\n\n# Your Task \nAs things stand now, your key/value server doesn't call your Raft library's `Snapshot()` method, so a rebooting server has to replay the complete persisted Raft log in order to restore its state. Now you'll modify kvserver to cooperate with Raft to save log space, and reduce restart time, using Raft's `Snapshot()` from Lab 3D.\n\nThe tester passes `maxraftstate` to your `StartKVServer()`. `maxraftstate` indicates the maximum allowed size of your persistent Raft state in bytes (including the log, but not including snapshots). You should compare `maxraftstate` to `persister.RaftStateSize()`. Whenever your key/value server detects that the Raft state size is approaching this threshold, it should save a snapshot by calling Raft's `Snapshot`. If `maxraftstate` is -1, you do not have to snapshot. `maxraftstate` applies to the GOB-encoded bytes your Raft passes as the first argument to to `persister.Save()`.\n\nModify your kvserver so that it detects when the persisted Raft state grows too large, and then hands a snapshot to Raft. When a kvserver server restarts, it should read the snapshot from `persister` and restore its state from the snapshot.\n\n- Think about when a kvserver should snapshot its state and what should be included in the snapshot. Raft stores each snapshot in the persister object using `Save()`, along with corresponding Raft state. You can read the latest stored snapshot using `ReadSnapshot()`.\n- Your kvserver must be able to detect duplicated operations in the log across checkpoints, so any state you are using to detect them must be included in the snapshots.\n- Capitalize all fields of structures stored in the snapshot.\n- You may have bugs in your Raft library that this lab exposes. If you make changes to your Raft implementation make sure it continues to pass all of the Lab 3 tests.\n- A reasonable amount of time to take for the Lab 4 tests is 400 seconds of real time and 700 seconds of CPU time. Further, `go test -run TestSnapshotSize` should take less than 20 seconds of real time.\n\nYour code should pass the 4B tests (as in the example here) as well as the 4A tests (and your Raft must continue to pass the Lab 3 tests).\n\n```\n$ go test -run 4B\nTest: InstallSnapshot RPC (4B) ...\n  ... Passed --   4.0  3   289   63\nTest: snapshot size is reasonable (4B) ...\n  ... Passed --   2.6  3  2418  800\nTest: ops complete fast enough (4B) ...\n  ... Passed --   3.2  3  3025    0\nTest: restarts, snapshots, one client (4B) ...\n  ... Passed --  21.9  5 29266 5820\nTest: restarts, snapshots, many clients (4B) ...\n  ... Passed --  21.5  5 33115 6420\nTest: unreliable net, snapshots, many clients (4B) ...\n  ... Passed --  17.4  5  3233  482\nTest: unreliable net, restarts, snapshots, many clients (4B) ...\n  ... Passed --  22.7  5  3337  471\nTest: unreliable net, restarts, partitions, snapshots, many clients (4B) ...\n  ... Passed --  30.4  5  2725  274\nTest: unreliable net, restarts, partitions, snapshots, random keys, many clients (4B) ...\n  ... Passed --  37.7  7  8378  681\nPASS\nok  \t6.5840/kvraft\t161.538s\n```", "test_method": "cd src/kvraft && go test -run 4B", "test_results": "Test: InstallSnapshot RPC (4B) ...\n  ... Passed --   4.0  3   289   63\nTest: snapshot size is reasonable (4B) ...\n  ... Passed --   2.6  3  2418  800\nTest: ops complete fast enough (4B) ...\n  ... Passed --   3.2  3  3025    0\nTest: restarts, snapshots, one client (4B) ...\n  ... Passed --  21.9  5 29266 5820\nTest: restarts, snapshots, many clients (4B) ...\n  ... Passed --  21.5  5 33115 6420\nTest: unreliable net, snapshots, many clients (4B) ...\n  ... Passed --  17.4  5  3233  482\nTest: unreliable net, restarts, snapshots, many clients (4B) ...\n  ... Passed --  22.7  5  3337  471\nTest: unreliable net, restarts, partitions, snapshots, many clients (4B) ...\n  ... Passed --  30.4  5  2725  274\nTest: unreliable net, restarts, partitions, snapshots, random keys, many clients (4B) ...\n  ... Passed --  37.7  7  8378  681\nPASS\nok  \t6.5840/kvraft\t161.538s", "difficulty": "hard", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-kvraft.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_10", "task_name": "problems/system_lab_10.md", "task": "# Problem Context\n## Introduction\nYou can either do a [final project](http://nil.csail.mit.edu/6.5840/2024/project.html) based on your own ideas, or this lab.\n\nIn this lab you'll build a key/value storage system that \"shards,\" or partitions, the keys over a set of replica groups. A shard is a subset of the key/value pairs; for example, all the keys starting with \"a\" might be one shard, all the keys starting with \"b\" another, etc. The reason for sharding is performance. Each replica group handles puts and gets for just a few of the shards, and the groups operate in parallel; thus total system throughput (puts and gets per unit time) increases in proportion to the number of groups.\n\nYour sharded key/value store will have two main components. First, a set of replica groups. Each replica group is responsible for a subset of the shards, using Raft replication. The second component is the \"shard controller\". The shard controller decides which replica group should serve each shard; this information is called the configuration. The configuration changes over time. Clients consult the shard controller in order to find the replica group for a key, and replica groups consult the controller in order to find out what shards to serve. There is a single shard controller for the whole system, implemented as a fault-tolerant service using Raft.\n\nA sharded storage system must be able to shift shards among replica groups. One reason is that some groups may become more loaded than others, so that shards need to be moved to balance the load. Another reason is that replica groups may join and leave the system: new replica groups may be added to increase capacity, or existing replica groups may be taken offline for repair or retirement.\n\nThe main challenge in this lab will be handling reconfiguration -- changes in the assignment of shards to groups. Within a single replica group, all group members must agree on when a reconfiguration occurs relative to client Put/Append/Get requests. For example, a Put may arrive at about the same time as a reconfiguration that causes the replica group to stop being responsible for the shard holding the Put's key. All replicas in the group must agree on whether the Put occurred before or after the reconfiguration. If before, the Put should take effect and the new owner of the shard will see its effect; if after, the Put won't take effect and client must re-try at the new owner. The recommended approach is to have each replica group use Raft to log not just the sequence of Puts, Appends, and Gets but also the sequence of reconfigurations. You will need to ensure that at most one replica group is serving requests for each shard at any one time.\n\nReconfiguration also requires interaction among the replica groups. For example, in configuration 10 group G1 may be responsible for shard S1. In configuration 11, group G2 may be responsible for shard S1. During the reconfiguration from 10 to 11, G1 and G2 must use RPC to move the contents of shard S1 (the key/value pairs) from G1 to G2.\n\nOnly RPC may be used for interaction among clients and servers. For example, different instances of your server are not allowed to share Go variables or files.\n\nThis lab uses \"configuration\" to refer to the assignment of shards to replica groups. This is not the same as Raft cluster membership changes. You don't have to implement Raft cluster membership changes.\n\nThis lab's general architecture (a configuration service and a set of replica groups) follows the same general pattern as Flat Datacenter Storage, BigTable, Spanner, FAWN, Apache HBase, Rosebud, Spinnaker, and many others. These systems differ in many details from this lab, though, and are also typically more sophisticated and capable. For example, the lab doesn't evolve the sets of peers in each Raft group; its data and query models are very simple; and handoff of shards is slow and doesn't allow concurrent client access.\n\nYour Lab 5 sharded server, Lab 5 shard controller, and Lab 4 kvraft must all use the same Raft implementation.\n## Getiting Started\nDo a `git pull` to get the latest lab software.\n\nWe supply you with skeleton code and tests in `src/shardctrler` and `src/shardkv`.\n\nTo get up and running, execute the following commands:\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/shardctrler\n$ go test\n--- FAIL: TestBasic (0.00s)\n        test_test.go:11: wanted 1 groups, got 0\nFAIL\nexit status 1\nFAIL    shardctrler     0.008s\n$\n```\n\nWhen you're done, your implementation should pass all the tests in the `src/shardctrler` directory, and all the ones in `src/shardkv`.\n## The Code\n\n# Your Task \nFirst you'll implement the shard controller, in `shardctrler/server.go` and `client.go`, and a sharded key/value server that can handle an unchanging (static) configuration. When you're done, your code should pass all the tests in the `shardctrler/` directory, and the `5A` tests in `shardkv/`.\n\n```\n$ cd ~/6.5840/src/shardctrler\n$ go test\nTest: Basic leave/join ...\n  ... Passed\nTest: Historical queries ...\n  ... Passed\nTest: Move ...\n  ... Passed\nTest: Concurrent leave/join ...\n  ... Passed\nTest: Minimal transfers after joins ...\n  ... Passed\nTest: Minimal transfers after leaves ...\n  ... Passed\nTest: Multi-group join/leave ...\n  ... Passed\nTest: Concurrent multi leave/join ...\n  ... Passed\nTest: Minimal transfers after multijoins ...\n  ... Passed\nTest: Minimal transfers after multileaves ...\n  ... Passed\nTest: Check Same config on servers ...\n  ... Passed\nPASS\nok  \t6.5840/shardctrler\t5.863s\n$\n$ cd ../shardkv\n$ go test -run 5A\nTest (5A): static shards ...\n  ... Passed\nTest (5A): rejection ...\n  ... Passed\nPASS\nok      6.5840/shardkv  9.262s\n$\n```\n\nThe shardctrler manages a sequence of numbered configurations. Each configuration describes a set of replica groups and an assignment of shards to replica groups. Whenever this assignment needs to change, the shard controller creates a new configuration with the new assignment. Key/value clients and servers contact the shardctrler when they want to know the current (or a past) configuration.\n\nYour implementation must support the RPC interface described in `shardctrler/common.go`, which consists of `Join`, `Leave`, `Move`, and `Query` RPCs. These RPCs are intended to allow an administrator (and the tests) to control the shardctrler: to add new replica groups, to eliminate replica groups, and to move shards between replica groups.\n\nThe `Join` RPC is used by an administrator to add new replica groups. Its argument is a set of mappings from unique, non-zero replica group identifiers (GIDs) to lists of server names. The shardctrler should react by creating a new configuration that includes the new replica groups. The new configuration should divide the shards as evenly as possible among the full set of groups, and should move as few shards as possible to achieve that goal. The shardctrler should allow re-use of a GID if it's not part of the current configuration (i.e. a GID should be allowed to Join, then Leave, then Join again).\n\nThe `Leave` RPC's argument is a list of GIDs of previously joined groups. The shardctrler should create a new configuration that does not include those groups, and that assigns those groups' shards to the remaining groups. The new configuration should divide the shards as evenly as possible among the groups, and should move as few shards as possible to achieve that goal.\n\nThe `Move` RPC's arguments are a shard number and a GID. The shardctrler should create a new configuration in which the shard is assigned to the group. The purpose of `Move` is to allow us to test your software. A `Join` or `Leave` following a `Move` will likely un-do the `Move`, since `Join` and `Leave` re-balance.\n\nThe `Query` RPC's argument is a configuration number. The shardctrler replies with the configuration that has that number. If the number is -1 or bigger than the biggest known configuration number, the shardctrler should reply with the latest configuration. The result of `Query(-1)` should reflect every `Join`, `Leave`, or `Move` RPC that the shardctrler finished handling before it received the `Query(-1)` RPC.\n\nThe very first configuration should be numbered zero. It should contain no groups, and all shards should be assigned to GID zero (an invalid GID). The next configuration (created in response to a `Join` RPC) should be numbered 1, &c. There will usually be significantly more shards than groups (i.e., each group will serve more than one shard), in order that load can be shifted at a fairly fine granularity.\n\nYou must implement the interface specified above in `client.go` and `server.go` in the `shardctrler/` directory. Your shardctrler must be fault-tolerant, using your Raft library from Lab 3/4. You have completed this task when you pass all the tests in `shardctrler/`.\n\n- Start with a stripped-down copy of your kvraft server.\n- You should implement duplicate client request detection for RPCs to the shard controller. The shardctrler tests don't test this, but the shardkv tests will later use your shardctrler on an unreliable network; you may have trouble passing the shardkv tests if your shardctrler doesn't filter out duplicate RPCs.\n- The code in your state machine that performs the shard rebalancing needs to be deterministic. In Go, map iteration order is [not deterministic](https://blog.golang.org/maps#TOC_7.).\n- Go maps are references. If you assign one variable of type map to another, both variables refer to the same map. Thus if you want to create a new `Config` based on a previous one, you need to create a new map object (with `make()`) and copy the keys and values individually.\n- The Go race detector (go test -race) may help you find bugs.\n\nNext, in the `shardkv/` directory, implement enough of a sharded key/value server to pass the first two tests in `shardkv/`. Again, start by copying code from your existing `kvraft` server. You should be able to get the first test to pass without doing anything special regarding sharding, since the `shardkv/client.go` we give you takes care of sending RPCs to the group that the controller assigns to the key in question.\n\nFor the second `shardkv` test, each k/v replica group must reject requests for keys for shards for which the group is not the assigned group. At this point, it's enough for the k/v servers to periodically ask the controller for the latest configuration, and to check that configuration each time a client Get/Put/Append RPC arrives. Use `key2shard()` (in `client.go`) to find the shard number for a key.\n\nYour server should respond with an `ErrWrongGroup` error to a client RPC with a key that the server isn't responsible for (i.e. for a key whose shard is not assigned to the server's group).\n\nYour server should not call the shard controller's `Join()` handler. The tester will call `Join()` when appropriate.", "test_method": "cd src/shardctrler && go test", "test_results": "Test: Basic leave/join ...\n  ... Passed\nTest: Historical queries ...\n  ... Passed\nTest: Move ...\n  ... Passed\nTest: Concurrent leave/join ...\n  ... Passed\nTest: Minimal transfers after joins ...\n  ... Passed\nTest: Minimal transfers after leaves ...\n  ... Passed\nTest: Multi-group join/leave ...\n  ... Passed\nTest: Concurrent multi leave/join ...\n  ... Passed\nTest: Minimal transfers after multijoins ...\n  ... Passed\nTest: Minimal transfers after multileaves ...\n  ... Passed\nTest: Check Same config on servers ...\n  ... Passed\nPASS\nok  \t6.5840/shardctrler\t5.863s\n$\n$ cd ../shardkv\n$ go test -run 5A\nTest (5A): static shards ...\n  ... Passed\nTest (5A): rejection ...\n  ... Passed\nPASS\nok      6.5840/shardkv  9.262s", "difficulty": "easy", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-shard.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_11", "task_name": "problems/system_lab_11.md", "task": "# Problem Context\n## Introduction\nYou can either do a [final project](http://nil.csail.mit.edu/6.5840/2024/project.html) based on your own ideas, or this lab.\n\nIn this lab you'll build a key/value storage system that \"shards,\" or partitions, the keys over a set of replica groups. A shard is a subset of the key/value pairs; for example, all the keys starting with \"a\" might be one shard, all the keys starting with \"b\" another, etc. The reason for sharding is performance. Each replica group handles puts and gets for just a few of the shards, and the groups operate in parallel; thus total system throughput (puts and gets per unit time) increases in proportion to the number of groups.\n\nYour sharded key/value store will have two main components. First, a set of replica groups. Each replica group is responsible for a subset of the shards, using Raft replication. The second component is the \"shard controller\". The shard controller decides which replica group should serve each shard; this information is called the configuration. The configuration changes over time. Clients consult the shard controller in order to find the replica group for a key, and replica groups consult the controller in order to find out what shards to serve. There is a single shard controller for the whole system, implemented as a fault-tolerant service using Raft.\n\nA sharded storage system must be able to shift shards among replica groups. One reason is that some groups may become more loaded than others, so that shards need to be moved to balance the load. Another reason is that replica groups may join and leave the system: new replica groups may be added to increase capacity, or existing replica groups may be taken offline for repair or retirement.\n\nThe main challenge in this lab will be handling reconfiguration -- changes in the assignment of shards to groups. Within a single replica group, all group members must agree on when a reconfiguration occurs relative to client Put/Append/Get requests. For example, a Put may arrive at about the same time as a reconfiguration that causes the replica group to stop being responsible for the shard holding the Put's key. All replicas in the group must agree on whether the Put occurred before or after the reconfiguration. If before, the Put should take effect and the new owner of the shard will see its effect; if after, the Put won't take effect and client must re-try at the new owner. The recommended approach is to have each replica group use Raft to log not just the sequence of Puts, Appends, and Gets but also the sequence of reconfigurations. You will need to ensure that at most one replica group is serving requests for each shard at any one time.\n\nReconfiguration also requires interaction among the replica groups. For example, in configuration 10 group G1 may be responsible for shard S1. In configuration 11, group G2 may be responsible for shard S1. During the reconfiguration from 10 to 11, G1 and G2 must use RPC to move the contents of shard S1 (the key/value pairs) from G1 to G2.\n\nOnly RPC may be used for interaction among clients and servers. For example, different instances of your server are not allowed to share Go variables or files.\n\nThis lab uses \"configuration\" to refer to the assignment of shards to replica groups. This is not the same as Raft cluster membership changes. You don't have to implement Raft cluster membership changes.\n\nThis lab's general architecture (a configuration service and a set of replica groups) follows the same general pattern as Flat Datacenter Storage, BigTable, Spanner, FAWN, Apache HBase, Rosebud, Spinnaker, and many others. These systems differ in many details from this lab, though, and are also typically more sophisticated and capable. For example, the lab doesn't evolve the sets of peers in each Raft group; its data and query models are very simple; and handoff of shards is slow and doesn't allow concurrent client access.\n\nYour Lab 5 sharded server, Lab 5 shard controller, and Lab 4 kvraft must all use the same Raft implementation.\n## Getiting Started\nDo a `git pull` to get the latest lab software.\n\nWe supply you with skeleton code and tests in `src/shardctrler` and `src/shardkv`.\n\nTo get up and running, execute the following commands:\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/shardctrler\n$ go test\n--- FAIL: TestBasic (0.00s)\n        test_test.go:11: wanted 1 groups, got 0\nFAIL\nexit status 1\nFAIL    shardctrler     0.008s\n$\n```\n\nWhen you're done, your implementation should pass all the tests in the `src/shardctrler` directory, and all the ones in `src/shardkv`.\n## The Code\n\n# Your Task \nDo a `git pull` to get the latest lab software.\n\nThe main task in this part of the lab is to move shards among replica groups when the controller changes the sharding, and do it in a way that provides linearizable k/v client operations.\n\nEach of your shards is only required to make progress when a majority of servers in the shard's Raft replica group is alive and can talk to each other, and can talk to a majority of the `shardctrler` servers. Your implementation must operate (serve requests and be able to re-configure as needed) even if a minority of servers in some replica group(s) are dead, temporarily unavailable, or slow.\n\nA shardkv server is a member of only a single replica group. The set of servers in a given replica group will never change.\n\nWe supply you with `client.go` code that sends each RPC to the replica group responsible for the RPC's key. It re-tries if the replica group says it is not responsible for the key; in that case, the client code asks the shard controller for the latest configuration and tries again. You'll have to modify client.go as part of your support for dealing with duplicate client RPCs, much as in the kvraft lab.\n\nWhen you're done your code should pass all the shardkv tests other than the challenge tests:\n\n```\n$ cd ~/6.5840/src/shardkv\n$ go test\nTest (5A): static shards ...\n  ... Passed\nTest (5A): rejection ...\n  ... Passed\nTest (5B): join then leave ...\n  ... Passed\nTest (5B): snapshots, join, and leave ...\nlabgob warning: Decoding into a non-default variable/field Num may not work\n  ... Passed\nTest (5B): servers miss configuration changes...\n  ... Passed\nTest (5B): concurrent puts and configuration changes...\n  ... Passed\nTest (5B): more concurrent puts and configuration changes...\n  ... Passed\nTest (5B): concurrent configuration change and restart...\n  ... Passed\nTest (5B): unreliable 1...\n  ... Passed\nTest (5B): unreliable 2...\n  ... Passed\nTest (5B): unreliable 3...\n  ... Passed\nTest: shard deletion (challenge 1) ...\n  ... Passed\nTest: unaffected shard access (challenge 2) ...\n  ... Passed\nTest: partial migration shard access (challenge 2) ...\n  ... Passed\nPASS\nok  \t6.5840/shardkv\t173.974s\n$\n```\n\nYou will need to make your servers watch for configuration changes, and when one is detected, to start the shard migration process. If a replica group loses a shard, it must stop serving requests to keys in that shard immediately, and start migrating the data for that shard to the replica group that is taking over ownership. If a replica group gains a shard, it needs to wait for the previous owner to send over the old shard data before accepting requests for that shard.\n\nImplement shard migration during configuration changes. Make sure that all servers in a replica group do the migration at the same point in the sequence of operations they execute, so that they all either accept or reject concurrent client requests. You should focus on passing the second test (\"join then leave\") before working on the later tests. You are done with this task when you pass all tests up to, but not including, `TestDelete`.\n\nYour server will need to periodically poll the shardctrler to learn about new configurations. The tests expect that your code polls roughly every 100 milliseconds; more often is OK, but much less often may cause problems.\n\nServers will need to send RPCs to each other in order to transfer shards during configuration changes. The shardctrler's `Config` struct contains server names, but you need a `labrpc.ClientEnd` in order to send an RPC. You should use the `make_end()` function passed to `StartServer()` to turn a server name into a `ClientEnd`. `shardkv/client.go` contains code that does this.\n\n- Process re-configurations one at a time, in order.\n- If a test fails, check for gob errors (e.g. \"gob: type not registered for interface ...\"). Go doesn't consider gob errors to be fatal, although they are fatal for the lab.\n- You'll need to provide at-most-once semantics (duplicate detection) for client requests across shard movement.\n- Think about how the shardkv client and server should deal with `ErrWrongGroup`. Should the client change the sequence number if it receives `ErrWrongGroup`? Should the server update the client state if it returns `ErrWrongGroup` when executing a `Get`/`Put` request?\n- After a server has moved to a new configuration, it is acceptable for it to continue to store shards that it no longer owns (though this would be regrettable in a real system). This may help simplify your server implementation.\n- When group G1 needs a shard from G2 during a configuration change, does it matter at what point during its processing of log entries G2 sends the shard to G1?\n- You can send an entire map in an RPC request or reply, which may help keep the code for shard transfer simple.\n- If one of your RPC handlers includes in its reply a map (e.g. a key/value map) that's part of your server's state, you may get bugs due to races. The RPC system has to read the map in order to send it to the caller, but it isn't holding a lock that covers the map. Your server, however, may proceed to modify the same map while the RPC system is reading it. The solution is for the RPC handler to include a copy of the map in the reply.\n- If you put a map or a slice in a Raft log entry, and your key/value server subsequently sees the entry on the `applyCh` and saves a reference to the map/slice in your key/value server's state, you may have a race. Make a copy of the map/slice, and store the copy in your key/value server's state. The race is between your key/value server modifying the map/slice and Raft reading it while persisting its log.\n- During a configuration change, a pair of groups may need to move shards in both directions between them. If you see deadlock, this is a possible source.", "test_method": "cd src/shardkv && $ go test", "test_results": "Test (5A): static shards ...\n  ... Passed\nTest (5A): rejection ...\n  ... Passed\nTest (5B): join then leave ...\n  ... Passed\nTest (5B): snapshots, join, and leave ...\nlabgob warning: Decoding into a non-default variable/field Num may not work\n  ... Passed\nTest (5B): servers miss configuration changes...\n  ... Passed\nTest (5B): concurrent puts and configuration changes...\n  ... Passed\nTest (5B): more concurrent puts and configuration changes...\n  ... Passed\nTest (5B): concurrent configuration change and restart...\n  ... Passed\nTest (5B): unreliable 1...\n  ... Passed\nTest (5B): unreliable 2...\n  ... Passed\nTest (5B): unreliable 3...\n  ... Passed\nTest: shard deletion (challenge 1) ...\n  ... Passed\nTest: unaffected shard access (challenge 2) ...\n  ... Passed\nTest: partial migration shard access (challenge 2) ...\n  ... Passed\nPASS\nok  \t6.5840/shardkv\t173.974s", "difficulty": "hard", "link": "http://nil.csail.mit.edu/6.5840/2024/labs/lab-shard.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2024", "repo_path": "projects/6.5840-golabs-2024"}
{"task_id": "system_lab_12", "task_name": "problems/system_lab_12.md", "task": "# Problem Context\n## Introduction\nIn this lab you'll build a MapReduce system. You'll implement a worker process that calls application Map and Reduce functions and handles reading and writing files, and a coordinator process that hands out tasks to workers and copes with failed workers. You'll be building something similar to the [MapReduce paper](http://research.google.com/archive/mapreduce-osdi04.pdf). (Note: this lab uses \"coordinator\" instead of the paper's \"master\".)\n## Getiting Started\nYou need to [setup Go](http://nil.csail.mit.edu/6.5840/2024/labs/go.html) to do the labs.\n\nFetch the initial lab software with [git](https://git-scm.com/) (a version control system). To learn more about git, look at the [Pro Git book](https://git-scm.com/book/en/v2) or the [git user's manual](http://www.kernel.org/pub/software/scm/git/docs/user-manual.html).\n\n```\n$ git clone git://g.csail.mit.edu/6.5840-golabs-2024 6.5840\n$ cd 6.5840\n$ ls\nMakefile src\n$\n```\n\nWe supply you with a simple sequential mapreduce implementation in `src/main/mrsequential.go`. It runs the maps and reduces one at a time, in a single process. We also provide you with a couple of MapReduce applications: word-count in `mrapps/wc.go`, and a text indexer in `mrapps/indexer.go`. You can run word count sequentially as follows:\n\n```\n$ cd ~/6.5840\n$ cd src/main\n$ go build -buildmode=plugin ../mrapps/wc.go\n$ rm mr-out*\n$ go run mrsequential.go wc.so pg*.txt\n$ more mr-out-0\nA 509\nABOUT 2\nACT 8\n...\n```\n\n`mrsequential.go` leaves its output in the file `mr-out-0`. The input is from the text files named `pg-xxx.txt`.\n\nFeel free to borrow code from `mrsequential.go`. You should also have a look at `mrapps/wc.go` to see what MapReduce application code looks like.\n\nFor this lab and all the others, we might issue updates to the code we provide you. To ensure that you can fetch those updates and easily merge them using `git pull`, it's best to leave the code we provide in the original files. You can add to the code we provide as directed in the lab write-ups; just don't move it. It's OK to put your own new functions in new files.\n## The Code\n\n# Your Task \nYour job is to implement a distributed MapReduce, consisting of two programs, the coordinator and the worker. There will be just one coordinator process, and one or more worker processes executing in parallel. In a real system the workers would run on a bunch of different machines, but for this lab you'll run them all on a single machine. The workers will talk to the coordinator via RPC. Each worker process will, in a loop, ask the coordinator for a task, read the task's input from one or more files, execute the task, write the task's output to one or more files, and again ask the coordinator for a new task. The coordinator should notice if a worker hasn't completed its task in a reasonable amount of time (for this lab, use ten seconds), and give the same task to a different worker.\n\nWe have given you a little code to start you off. The \"main\" routines for the coordinator and worker are in `main/mrcoordinator.go` and `main/mrworker.go`; don't change these files. You should put your implementation in `mr/coordinator.go`, `mr/worker.go`, and `mr/rpc.go`.\n\nHere's how to run your code on the word-count MapReduce application. First, make sure the word-count plugin is freshly built:\n\n```\n$ go build -buildmode=plugin ../mrapps/wc.go\n```\n\nIn the `main` directory, run the coordinator.\n\n```\n$ rm mr-out*\n$ go run mrcoordinator.go pg-*.txt\n```\n\nThe `pg-*.txt` arguments to `mrcoordinator.go` are the input files; each file corresponds to one \"split\", and is the input to one Map task.\n\nIn one or more other windows, run some workers:\n\n```\n$ go run mrworker.go wc.so\n```\n\nWhen the workers and coordinator have finished, look at the output in `mr-out-*`. When you've completed the lab, the sorted union of the output files should match the sequential output, like this:\n\n```\n$ cat mr-out-* | sort | more\nA 509\nABOUT 2\nACT 8\n...\n```\n\nWe supply you with a test script in `main/test-mr.sh`. The tests check that the `wc` and `indexer` MapReduce applications produce the correct output when given the `pg-xxx.txt` files as input. The tests also check that your implementation runs the Map and Reduce tasks in parallel, and that your implementation recovers from workers that crash while running tasks.\n\nIf you run the test script now, it will hang because the coordinator never finishes:\n\n```\n$ cd ~/6.5840/src/main\n$ bash test-mr.sh\n*** Starting wc test.\n```\n\nYou can change `ret := false` to true in the Done function in `mr/coordinator.go` so that the coordinator exits immediately. Then:\n\n```\n$ bash test-mr.sh\n*** Starting wc test.\nsort: No such file or directory\ncmp: EOF on mr-wc-all\n--- wc output is not the same as mr-correct-wc.txt\n--- wc test: FAIL\n$\n```\n\nThe test script expects to see output in files named `mr-out-X`, one for each reduce task. The empty implementations of `mr/coordinator.go` and `mr/worker.go` don't produce those files (or do much of anything else), so the test fails.\n\nWhen you've finished, the test script output should look like this:\n\n```\n$ bash test-mr.sh\n*** Starting wc test.\n--- wc test: PASS\n*** Starting indexer test.\n--- indexer test: PASS\n*** Starting map parallelism test.\n--- map parallelism test: PASS\n*** Starting reduce parallelism test.\n--- reduce parallelism test: PASS\n*** Starting job count test.\n--- job count test: PASS\n*** Starting early exit test.\n--- early exit test: PASS\n*** Starting crash test.\n--- crash test: PASS\n*** PASSED ALL TESTS\n$\n```\n\nYou may see some errors from the Go RPC package that look like\n\n```\n2019/12/16 13:27:09 rpc.Register: method \"Done\" has 1 input parameters; needs exactly three\n```\n\nIgnore these messages; registering the coordinator as an [RPC server](https://golang.org/src/net/rpc/server.go) checks if all its methods are suitable for RPCs (have 3 inputs); we know that `Done` is not called via RPC.\n\nAdditionally, depending on your strategy for terminating worker processes, you may see some errors of the form\n\n```\n2025/02/11 16:21:32 dialing:dial unix /var/tmp/5840-mr-501: connect: connection refused\n```\n\nIt is fine to see a handful of these messages per test; they arise when the worker is unable to contact the coordinator RPC server after the coordinator has exited.\n\n\n\n### A few rules:\n\n- The map phase should divide the intermediate keys into buckets for `nReduce` reduce tasks, where `nReduce` is the number of reduce tasks -- the argument that `main/mrcoordinator.go` passes to `MakeCoordinator()`. Each mapper should create `nReduce` intermediate files for consumption by the reduce tasks.\n- The worker implementation should put the output of the X'th reduce task in the file `mr-out-X`.\n- A `mr-out-X` file should contain one line per Reduce function output. The line should be generated with the Go `\"%v %v\"` format, called with the key and value. Have a look in `main/mrsequential.go` for the line commented \"this is the correct format\". The test script will fail if your implementation deviates too much from this format.\n- You can modify `mr/worker.go`, `mr/coordinator.go`, and `mr/rpc.go`. You can temporarily modify other files for testing, but make sure your code works with the original versions; we'll test with the original versions.\n- The worker should put intermediate Map output in files in the current directory, where your worker can later read them as input to Reduce tasks.\n- `main/mrcoordinator.go` expects `mr/coordinator.go` to implement a `Done()` method that returns true when the MapReduce job is completely finished; at that point, `mrcoordinator.go` will exit.\n- When the job is completely finished, the worker processes should exit. A simple way to implement this is to use the return value from `call()`: if the worker fails to contact the coordinator, it can assume that the coordinator has exited because the job is done, so the worker can terminate too. Depending on your design, you might also find it helpful to have a \"please exit\" pseudo-task that the coordinator can give to workers.\n\n### Hints\n\n- The [Guidance page](http://nil.csail.mit.edu/6.5840/2025/labs/guidance.html) has some tips on developing and debugging.\n\n- One way to get started is to modify `mr/worker.go`'s `Worker()` to send an RPC to the coordinator asking for a task. Then modify the coordinator to respond with the file name of an as-yet-unstarted map task. Then modify the worker to read that file and call the application Map function, as in `mrsequential.go`.\n\n- The application Map and Reduce functions are loaded at run-time using the Go plugin package, from files whose names end in `.so`.\n\n- If you change anything in the `mr/` directory, you will probably have to re-build any MapReduce plugins you use, with something like `go build -buildmode=plugin ../mrapps/wc.go`\n\n- This lab relies on the workers sharing a file system. That's straightforward when all workers run on the same machine, but would require a global filesystem like GFS if the workers ran on different machines.\n\n- A reasonable naming convention for intermediate files is `mr-X-Y`, where X is the Map task number, and Y is the reduce task number.\n\n- The worker's map task code will need a way to store intermediate key/value pairs in files in a way that can be correctly read back during reduce tasks. One possibility is to use Go's encoding/json package. To write key/value pairs in JSON format to an open file:\n\n  ```\n    enc := json.NewEncoder(file)\n    for _, kv := ... {\n      err := enc.Encode(&kv)\n  ```\n\n  and to read such a file back:\n\n  ```\n    dec := json.NewDecoder(file)\n    for {\n      var kv KeyValue\n      if err := dec.Decode(&kv); err != nil {\n        break\n      }\n      kva = append(kva, kv)\n    }\n  ```\n\n- The map part of your worker can use the `ihash(key)` function (in `worker.go`) to pick the reduce task for a given key.\n\n- You can steal some code from `mrsequential.go` for reading Map input files, for sorting intermediate key/value pairs between the Map and Reduce, and for storing Reduce output in files.\n\n- The coordinator, as an RPC server, will be concurrent; don't forget to lock shared data.\n\n- Use Go's race detector, with `go run -race`. `test-mr.sh` has a comment at the start that tells you how to run it with `-race`. When we grade your labs, we will **not** use the race detector. Nevertheless, if your code has races, there's a good chance it will fail when we test it even without the race detector.\n\n- Workers will sometimes need to wait, e.g. reduces can't start until the last map has finished. One possibility is for workers to periodically ask the coordinator for work, sleeping with `time.Sleep()` between each request. Another possibility is for the relevant RPC handler in the coordinator to have a loop that waits, either with `time.Sleep()` or `sync.Cond`. Go runs the handler for each RPC in its own thread, so the fact that one handler is waiting needn't prevent the coordinator from processing other RPCs.\n\n- The coordinator can't reliably distinguish between crashed workers, workers that are alive but have stalled for some reason, and workers that are executing but too slowly to be useful. The best you can do is have the coordinator wait for some amount of time, and then give up and re-issue the task to a different worker. For this lab, have the coordinator wait for ten seconds; after that the coordinator should assume the worker has died (of course, it might not have).\n\n- If you choose to implement Backup Tasks (Section 3.6), note that we test that your code doesn't schedule extraneous tasks when workers execute tasks without crashing. Backup tasks should only be scheduled after some relatively long period of time (e.g., 10s).\n\n- To test crash recovery, you can use the `mrapps/crash.go` application plugin. It randomly exits in the Map and Reduce functions.\n\n- To ensure that nobody observes partially written files in the presence of crashes, the MapReduce paper mentions the trick of using a temporary file and atomically renaming it once it is completely written. You can use `ioutil.TempFile` (or `os.CreateTemp` if you are running Go 1.17 or later) to create a temporary file and `os.Rename` to atomically rename it.\n\n- `test-mr.sh` runs all its processes in the sub-directory `mr-tmp`, so if something goes wrong and you want to look at intermediate or output files, look there. Feel free to temporarily modify `test-mr.sh` to `exit` after the failing test, so the script does not continue testing (and overwrite the output files).\n\n- `test-mr-many.sh` runs `test-mr.sh` many times in a row, which you may want to do in order to spot low-probability bugs. It takes as an argument the number of times to run the tests. You should not run several `test-mr.sh` instances in parallel because the coordinator will reuse the same socket, causing conflicts.\n\n- Go RPC sends only struct fields whose names start with capital letters. Sub-structures must also have capitalized field names.\n\n- When calling the RPC call() function, the reply struct should contain all default values. RPC calls should look like this:\n\n  ```\n    reply := SomeType{}\n    call(..., &reply)\n  ```\n\n  without setting any fields of reply before the call. If you pass reply structures that have non-default fields, the RPC system may silently return incorrect values.\n\n\n\n\n\n### \n", "test_method": "cd src/main && bash test-mr.sh", "test_results": "*** Starting wc test.\n--- wc test: PASS\n*** Starting indexer test.\n--- indexer test: PASS\n*** Starting map parallelism test.\n--- map parallelism test: PASS\n*** Starting reduce parallelism test.\n--- reduce parallelism test: PASS\n*** Starting job count test.\n--- job count test: PASS\n*** Starting early exit test.\n--- early exit test: PASS\n*** Starting crash test.\n--- crash test: PASS\n*** PASSED ALL TESTS", "difficulty": "moderate/hard", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-mr.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_13", "task_name": "problems/system_lab_13.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a key/value server for a single machine that ensures that each Put operation is executed *at-most-once* despite network failures and that the operations are *linearizable*. You will use this KV server to implement a lock. Later labs will replicate a server like this one to handle server crashes.\n\n### KV server\n\nEach client interacts with the key/value server using a *Clerk*, which sends RPCs to the server. Clients can send two different RPCs to the server: `Put(key, value, version)` and `Get(key)`. The server maintains an in-memory map that records for each key a (value, version) tuple. Keys and values are strings. The version number records the number of times the key has been written. `Put(key, value, version)` installs or replaces the value for a particular key in the map *only if* the `Put`'s version number matches the server's version number for the key. If the version numbers match, the server also increments the version number of the key. If the version numbers don't match, the server should return `rpc.ErrVersion`. A client can create a new key by invoking `Put` with version number 0 (and the resulting version stored by the server will be 1). If the version number of the `Put` is larger than 0 and the key doesn't exist, the server should return `rpc.ErrNoKey`.\n\n`Get(key)` fetches the current value for the key and its associated version. If the key doesn't exist at the server, the server should return `rpc.ErrNoKey`.\n\nMaintaining a version number for each key will be useful for implementing locks using `Put` and ensuring at-most-once semantics for `Put`'s when the network is unreliable and the client retransmits.\n\nWhen you've finished this lab and passed all the tests, you'll have a *linearizable* key/value service from the point of view of clients calling `Clerk.Get` and `Clerk.Put`. That is, if client operations aren't concurrent, each client `Clerk.Get` and `Clerk.Put` will observe the modifications to the state implied by the preceding sequence of operations. For concurrent operations, the return values and final state will be the same as if the operations had executed one at a time in some order. Operations are concurrent if they overlap in time: for example, if client X calls `Clerk.Put()`, and client Y calls `Clerk.Put()`, and then client X's call returns. An operation must observe the effects of all operations that have completed before the operation starts. See the FAQ on [linearizability](http://nil.csail.mit.edu/6.5840/2025/papers/linearizability-faq.txt) for more background.\n\nLinearizability is convenient for applications because it's the behavior you'd see from a single server that processes requests one at a time. For example, if one client gets a successful response from the server for an update request, subsequently launched reads from other clients are guaranteed to see the effects of that update. Providing linearizability is relatively easy for a single server.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvsrv1`. `kvsrv1/client.go` implements a Clerk that clients use to manage RPC interactions with the server; the Clerk provides `Put` and `Get` methods. `kvsrv1/server.go` contains the server code, including the `Put` and `Get` handlers that implement the server side of RPC requests. You will need to modify `client.go` and `server.go`. The RPC requests, replies, and error values are defined in the `kvsrv1/rpc` package in the file `kvsrv1/rpc/rpc.go`, which you should look at, though you don't have to modify `rpc.go`.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/kvsrv1\n$ go test -v\n=== RUN   TestReliablePut\nOne client and reliable Put (reliable network)...\n    kvsrv_test.go:25: Put err ErrNoKey\n...\n$\n```\n\n## The Code\n\n# Your Task \nYour first task is to implement a solution that works when there are no dropped messages. You'll need to add RPC-sending code to the Clerk Put/Get methods in `client.go`, and implement `Put` and `Get` RPC handlers in `server.go`.\n\nYou have completed this task when you pass the Reliable tests in the test suite:\n\n```\n$ go test -v -run Reliable\n=== RUN   TestReliablePut\nOne client and reliable Put (reliable network)...\n  ... Passed --   0.0  1     5    0\n--- PASS: TestReliablePut (0.00s)\n=== RUN   TestPutConcurrentReliable\nTest: many clients racing to put values to the same key (reliable network)...\ninfo: linearizability check timed out, assuming history is ok\n  ... Passed --   3.1  1 90171 90171\n--- PASS: TestPutConcurrentReliable (3.07s)\n=== RUN   TestMemPutManyClientsReliable\nTest: memory use many put clients (reliable network)...\n  ... Passed --   9.2  1 100000    0\n--- PASS: TestMemPutManyClientsReliable (16.59s)\nPASS\nok  \t6.5840/kvsrv1\t19.681s\n```\n\nThe numbers after each `Passed` are real time in seconds, the constant 1, the number of RPCs sent (including client RPCs), and the number of key/value operations executed (`Clerk` `Get` and `Put` calls).\n\n- Check that your code is race-free using `go test -race`.", "test_method": "cd src/kvsrv1 && go test -v -run Reliable", "test_results": "=== RUN   TestReliablePut\nOne client and reliable Put (reliable network)...\n  ... Passed --   0.0  1     5    0\n--- PASS: TestReliablePut (0.00s)\n=== RUN   TestPutConcurrentReliable\nTest: many clients racing to put values to the same key (reliable network)...\ninfo: linearizability check timed out, assuming history is ok\n  ... Passed --   3.1  1 90171 90171\n--- PASS: TestPutConcurrentReliable (3.07s)\n=== RUN   TestMemPutManyClientsReliable\nTest: memory use many put clients (reliable network)...\n  ... Passed --   9.2  1 100000    0\n--- PASS: TestMemPutManyClientsReliable (16.59s)\nPASS\nok  \t6.5840/kvsrv1\t19.681s", "difficulty": "easy", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvsrv1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_14", "task_name": "problems/system_lab_14.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a key/value server for a single machine that ensures that each Put operation is executed *at-most-once* despite network failures and that the operations are *linearizable*. You will use this KV server to implement a lock. Later labs will replicate a server like this one to handle server crashes.\n\n### KV server\n\nEach client interacts with the key/value server using a *Clerk*, which sends RPCs to the server. Clients can send two different RPCs to the server: `Put(key, value, version)` and `Get(key)`. The server maintains an in-memory map that records for each key a (value, version) tuple. Keys and values are strings. The version number records the number of times the key has been written. `Put(key, value, version)` installs or replaces the value for a particular key in the map *only if* the `Put`'s version number matches the server's version number for the key. If the version numbers match, the server also increments the version number of the key. If the version numbers don't match, the server should return `rpc.ErrVersion`. A client can create a new key by invoking `Put` with version number 0 (and the resulting version stored by the server will be 1). If the version number of the `Put` is larger than 0 and the key doesn't exist, the server should return `rpc.ErrNoKey`.\n\n`Get(key)` fetches the current value for the key and its associated version. If the key doesn't exist at the server, the server should return `rpc.ErrNoKey`.\n\nMaintaining a version number for each key will be useful for implementing locks using `Put` and ensuring at-most-once semantics for `Put`'s when the network is unreliable and the client retransmits.\n\nWhen you've finished this lab and passed all the tests, you'll have a *linearizable* key/value service from the point of view of clients calling `Clerk.Get` and `Clerk.Put`. That is, if client operations aren't concurrent, each client `Clerk.Get` and `Clerk.Put` will observe the modifications to the state implied by the preceding sequence of operations. For concurrent operations, the return values and final state will be the same as if the operations had executed one at a time in some order. Operations are concurrent if they overlap in time: for example, if client X calls `Clerk.Put()`, and client Y calls `Clerk.Put()`, and then client X's call returns. An operation must observe the effects of all operations that have completed before the operation starts. See the FAQ on [linearizability](http://nil.csail.mit.edu/6.5840/2025/papers/linearizability-faq.txt) for more background.\n\nLinearizability is convenient for applications because it's the behavior you'd see from a single server that processes requests one at a time. For example, if one client gets a successful response from the server for an update request, subsequently launched reads from other clients are guaranteed to see the effects of that update. Providing linearizability is relatively easy for a single server.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvsrv1`. `kvsrv1/client.go` implements a Clerk that clients use to manage RPC interactions with the server; the Clerk provides `Put` and `Get` methods. `kvsrv1/server.go` contains the server code, including the `Put` and `Get` handlers that implement the server side of RPC requests. You will need to modify `client.go` and `server.go`. The RPC requests, replies, and error values are defined in the `kvsrv1/rpc` package in the file `kvsrv1/rpc/rpc.go`, which you should look at, though you don't have to modify `rpc.go`.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/kvsrv1\n$ go test -v\n=== RUN   TestReliablePut\nOne client and reliable Put (reliable network)...\n    kvsrv_test.go:25: Put err ErrNoKey\n...\n$\n```\n\n## The Code\n\n# Your Task \nIn many distributed applications, clients running on different machines use a key/value server to coordinate their activities. For example, ZooKeeper and Etcd allow clients to coordinate using a distributed lock, in analogy with how threads in a Go program can coordinate with locks (i.e., `sync.Mutex`). Zookeeper and Etcd implement such a lock with conditional put.\n\nIn this exercise your task is to implement a lock layered on client `Clerk.Put` and `Clerk.Get` calls. The lock supports two methods: `Acquire` and `Release`. The lock's specification is that only one client can successfully acquire the lock at a time; other clients must wait until the first client has released the lock using `Release`.\n\nWe supply you with skeleton code and tests in `src/kvsrv1/lock/`. You will need to modify `src/kvsrv1/lock/lock.go`. Your `Acquire` and `Release` code can talk to your key/value server by calling `lk.ck.Put()` and `lk.ck.Get()`.\n\nIf a client crashes while holding a lock, the lock will never be released. In a design more sophisticated than this lab, the client would attach a [lease](https://en.wikipedia.org/wiki/Lease_(computer_science)#:~:text=Leases are commonly used in,to rely on the resource.) to a lock. When the lease expires, the lock server would release the lock on behalf of the client. In this lab clients don't crash and you can ignore this problem.\n\nImplement `Acquire` and `Release`. You have completed this exercise when your code passes the Reliable tests in the test suite in the lock sub-directory:\n\n```\n$ cd lock\n$ go test -v -run Reliable\n=== RUN   TestOneClientReliable\nTest: 1 lock clients (reliable network)...\n  ... Passed --   2.0  1   974    0\n--- PASS: TestOneClientReliable (2.01s)\n=== RUN   TestManyClientsReliable\nTest: 10 lock clients (reliable network)...\n  ... Passed --   2.1  1 83194    0\n--- PASS: TestManyClientsReliable (2.11s)\nPASS\nok  \t6.5840/kvsrv1/lock\t4.120s\n```\n\nIf you haven't implemented the lock yet, the first test will succeed.\n\nThis exercise requires little code but will require a bit more independent thought than the previous exercise.\n\n- You will need a unique identifier for each lock client; call `kvtest.RandValue(8)` to generate a random string.\n- The lock service should use a specific key to store the \"lock state\" (you would have to decide precisely what the lock state is). The key to be used is passed through the parameter `l` of `MakeLock` in `src/kvsrv1/lock/lock.go`.", "test_method": "cd cd src/kvsrv1/lock &&  go test -v -run Reliable", "test_results": "=== RUN   TestOneClientReliable\nTest: 1 lock clients (reliable network)...\n  ... Passed --   2.0  1   974    0\n--- PASS: TestOneClientReliable (2.01s)\n=== RUN   TestManyClientsReliable\nTest: 10 lock clients (reliable network)...\n  ... Passed --   2.1  1 83194    0\n--- PASS: TestManyClientsReliable (2.11s)\nPASS\nok  \t6.5840/kvsrv1/lock\t4.120s", "difficulty": "moderate", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvsrv1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_15", "task_name": "problems/system_lab_15.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a key/value server for a single machine that ensures that each Put operation is executed *at-most-once* despite network failures and that the operations are *linearizable*. You will use this KV server to implement a lock. Later labs will replicate a server like this one to handle server crashes.\n\n### KV server\n\nEach client interacts with the key/value server using a *Clerk*, which sends RPCs to the server. Clients can send two different RPCs to the server: `Put(key, value, version)` and `Get(key)`. The server maintains an in-memory map that records for each key a (value, version) tuple. Keys and values are strings. The version number records the number of times the key has been written. `Put(key, value, version)` installs or replaces the value for a particular key in the map *only if* the `Put`'s version number matches the server's version number for the key. If the version numbers match, the server also increments the version number of the key. If the version numbers don't match, the server should return `rpc.ErrVersion`. A client can create a new key by invoking `Put` with version number 0 (and the resulting version stored by the server will be 1). If the version number of the `Put` is larger than 0 and the key doesn't exist, the server should return `rpc.ErrNoKey`.\n\n`Get(key)` fetches the current value for the key and its associated version. If the key doesn't exist at the server, the server should return `rpc.ErrNoKey`.\n\nMaintaining a version number for each key will be useful for implementing locks using `Put` and ensuring at-most-once semantics for `Put`'s when the network is unreliable and the client retransmits.\n\nWhen you've finished this lab and passed all the tests, you'll have a *linearizable* key/value service from the point of view of clients calling `Clerk.Get` and `Clerk.Put`. That is, if client operations aren't concurrent, each client `Clerk.Get` and `Clerk.Put` will observe the modifications to the state implied by the preceding sequence of operations. For concurrent operations, the return values and final state will be the same as if the operations had executed one at a time in some order. Operations are concurrent if they overlap in time: for example, if client X calls `Clerk.Put()`, and client Y calls `Clerk.Put()`, and then client X's call returns. An operation must observe the effects of all operations that have completed before the operation starts. See the FAQ on [linearizability](http://nil.csail.mit.edu/6.5840/2025/papers/linearizability-faq.txt) for more background.\n\nLinearizability is convenient for applications because it's the behavior you'd see from a single server that processes requests one at a time. For example, if one client gets a successful response from the server for an update request, subsequently launched reads from other clients are guaranteed to see the effects of that update. Providing linearizability is relatively easy for a single server.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvsrv1`. `kvsrv1/client.go` implements a Clerk that clients use to manage RPC interactions with the server; the Clerk provides `Put` and `Get` methods. `kvsrv1/server.go` contains the server code, including the `Put` and `Get` handlers that implement the server side of RPC requests. You will need to modify `client.go` and `server.go`. The RPC requests, replies, and error values are defined in the `kvsrv1/rpc` package in the file `kvsrv1/rpc/rpc.go`, which you should look at, though you don't have to modify `rpc.go`.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/kvsrv1\n$ go test -v\n=== RUN   TestReliablePut\nOne client and reliable Put (reliable network)...\n    kvsrv_test.go:25: Put err ErrNoKey\n...\n$\n```\n\n## The Code\n\n# Your Task \nThe main challenge in this exercise is that the network may re-order, delay, or discard RPC requests and/or replies. To recover from discarded requests/replies, the Clerk must keep re-trying each RPC until it receives a reply from the server.\n\nIf the network discards an RPC request message, then the client re-sending the request will solve the problem: the server will receive and execute just the re-sent request.\n\nHowever, the network might instead discard an RPC reply message. The client does not know which message was discarded; the client only observes that it received no reply. If it was the reply that was discarded, and the client re-sends the RPC request, then the server will receive two copies of the request. That's OK for a `Get`, since `Get` doesn't modify the server state. It is safe to resend a `Put` RPC with the same version number, since the server executes `Put` conditionally on the version number; if the server received and executed a `Put` RPC, it will respond to a re-transmitted copy of that RPC with `rpc.ErrVersion` rather than executing the Put a second time.\n\nA tricky case is if the server replies with an `rpc.ErrVersion` in a response to an RPC that the Clerk retried. In this case, the Clerk cannot know if the Clerk's `Put` was executed by the server or not: the first RPC might have been executed by the server but the network may have discarded the successful response from the server, so that the server sent `rpc.ErrVersion` only for the retransmitted RPC. Or, it might be that another Clerk updated the key before the Clerk's first RPC arrived at the server, so that the server executed neither of the Clerk's RPCs and replied `rpc.ErrVersion` to both. Therefore, if a Clerk receives `rpc.ErrVersion` for a retransmitted Put RPC, `Clerk.Put` must return `rpc.ErrMaybe` to the application instead of `rpc.ErrVersion` since the request may have been executed. It is then up to the application to handle this case. If the server responds to an initial (not retransmitted) Put RPC with `rpc.ErrVersion`, then the Clerk should return `rpc.ErrVersion` to the application, since the RPC was definitely not executed by the server.\n\nIt would be more convenient for application developers if `Put`'s were exactly-once (i.e., no `rpc.ErrMaybe` errors) but that is difficult to guarantee without maintaining state at the server for each Clerk. In the last exercise of this lab, you will implement a lock using your Clerk to explore how to program with at-most-once `Clerk.Put`.\n\nNow you should modify your `kvsrv1/client.go` to continue in the face of dropped RPC requests and replies. A return value of `true` from the client's `ck.clnt.Call()` indicates that the client received an RPC reply from the server; a return value of `false` indicates that it did not receive a reply (more precisely, `Call()` waits for a reply message for a timeout interval, and returns false if no reply arrives within that time). Your `Clerk` should keep re-sending an RPC until it receives a reply. Keep in mind the discussion of `rpc.ErrMaybe` above. Your solution shouldn't require any changes to the server.\n\nAdd code to `Clerk` to retry if doesn't receive a reply. Your have completed this task if your code passes all tests in `kvsrv1/`, like this:\n\n```\n$ go test -v\n=== RUN   TestReliablePut\nOne client and reliable Put (reliable network)...\n  ... Passed --   0.0  1     5    0\n--- PASS: TestReliablePut (0.00s)\n=== RUN   TestPutConcurrentReliable\nTest: many clients racing to put values to the same key (reliable network)...\ninfo: linearizability check timed out, assuming history is ok\n  ... Passed --   3.1  1 106647 106647\n--- PASS: TestPutConcurrentReliable (3.09s)\n=== RUN   TestMemPutManyClientsReliable\nTest: memory use many put clients (reliable network)...\n  ... Passed --   8.0  1 100000    0\n--- PASS: TestMemPutManyClientsReliable (14.61s)\n=== RUN   TestUnreliableNet\nOne client (unreliable network)...\n  ... Passed --   7.6  1   251  208\n--- PASS: TestUnreliableNet (7.60s)\nPASS\nok  \t6.5840/kvsrv1\t25.319s\n```\n\n- Before the client retries, it should wait a little bit; you can use go's `time` package and call `time.Sleep(100 * time.Millisecond)`", "test_method": "cd src/kvsrv1 && go test -v", "test_results": "=== RUN   TestReliablePut\nOne client and reliable Put (reliable network)...\n  ... Passed --   0.0  1     5    0\n--- PASS: TestReliablePut (0.00s)\n=== RUN   TestPutConcurrentReliable\nTest: many clients racing to put values to the same key (reliable network)...\ninfo: linearizability check timed out, assuming history is ok\n  ... Passed --   3.1  1 106647 106647\n--- PASS: TestPutConcurrentReliable (3.09s)\n=== RUN   TestMemPutManyClientsReliable\nTest: memory use many put clients (reliable network)...\n  ... Passed --   8.0  1 100000    0\n--- PASS: TestMemPutManyClientsReliable (14.61s)\n=== RUN   TestUnreliableNet\nOne client (unreliable network)...\n  ... Passed --   7.6  1   251  208\n--- PASS: TestUnreliableNet (7.60s)\nPASS\nok  \t6.5840/kvsrv1\t25.319s", "difficulty": "moderate", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvsrv1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_16", "task_name": "problems/system_lab_16.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a key/value server for a single machine that ensures that each Put operation is executed *at-most-once* despite network failures and that the operations are *linearizable*. You will use this KV server to implement a lock. Later labs will replicate a server like this one to handle server crashes.\n\n### KV server\n\nEach client interacts with the key/value server using a *Clerk*, which sends RPCs to the server. Clients can send two different RPCs to the server: `Put(key, value, version)` and `Get(key)`. The server maintains an in-memory map that records for each key a (value, version) tuple. Keys and values are strings. The version number records the number of times the key has been written. `Put(key, value, version)` installs or replaces the value for a particular key in the map *only if* the `Put`'s version number matches the server's version number for the key. If the version numbers match, the server also increments the version number of the key. If the version numbers don't match, the server should return `rpc.ErrVersion`. A client can create a new key by invoking `Put` with version number 0 (and the resulting version stored by the server will be 1). If the version number of the `Put` is larger than 0 and the key doesn't exist, the server should return `rpc.ErrNoKey`.\n\n`Get(key)` fetches the current value for the key and its associated version. If the key doesn't exist at the server, the server should return `rpc.ErrNoKey`.\n\nMaintaining a version number for each key will be useful for implementing locks using `Put` and ensuring at-most-once semantics for `Put`'s when the network is unreliable and the client retransmits.\n\nWhen you've finished this lab and passed all the tests, you'll have a *linearizable* key/value service from the point of view of clients calling `Clerk.Get` and `Clerk.Put`. That is, if client operations aren't concurrent, each client `Clerk.Get` and `Clerk.Put` will observe the modifications to the state implied by the preceding sequence of operations. For concurrent operations, the return values and final state will be the same as if the operations had executed one at a time in some order. Operations are concurrent if they overlap in time: for example, if client X calls `Clerk.Put()`, and client Y calls `Clerk.Put()`, and then client X's call returns. An operation must observe the effects of all operations that have completed before the operation starts. See the FAQ on [linearizability](http://nil.csail.mit.edu/6.5840/2025/papers/linearizability-faq.txt) for more background.\n\nLinearizability is convenient for applications because it's the behavior you'd see from a single server that processes requests one at a time. For example, if one client gets a successful response from the server for an update request, subsequently launched reads from other clients are guaranteed to see the effects of that update. Providing linearizability is relatively easy for a single server.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvsrv1`. `kvsrv1/client.go` implements a Clerk that clients use to manage RPC interactions with the server; the Clerk provides `Put` and `Get` methods. `kvsrv1/server.go` contains the server code, including the `Put` and `Get` handlers that implement the server side of RPC requests. You will need to modify `client.go` and `server.go`. The RPC requests, replies, and error values are defined in the `kvsrv1/rpc` package in the file `kvsrv1/rpc/rpc.go`, which you should look at, though you don't have to modify `rpc.go`.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/kvsrv1\n$ go test -v\n=== RUN   TestReliablePut\nOne client and reliable Put (reliable network)...\n    kvsrv_test.go:25: Put err ErrNoKey\n...\n$\n```\n\n## The Code\n\n# Your Task \nModify your lock implementation to work correctly with your modified key/value client when the network is not reliable. You have completed this exercise when your code passes all the `kvsrv1/lock/` tests, including the unreliable ones:\n\n```\n$ cd lock\n$ go test -v\n=== RUN   TestOneClientReliable\nTest: 1 lock clients (reliable network)...\n  ... Passed --   2.0  1   968    0\n--- PASS: TestOneClientReliable (2.01s)\n=== RUN   TestManyClientsReliable\nTest: 10 lock clients (reliable network)...\n  ... Passed --   2.1  1 10789    0\n--- PASS: TestManyClientsReliable (2.12s)\n=== RUN   TestOneClientUnreliable\nTest: 1 lock clients (unreliable network)...\n  ... Passed --   2.3  1    70    0\n--- PASS: TestOneClientUnreliable (2.27s)\n=== RUN   TestManyClientsUnreliable\nTest: 10 lock clients (unreliable network)...\n  ... Passed --   3.6  1   908    0\n--- PASS: TestManyClientsUnreliable (3.62s)\nPASS\nok  \t6.5840/kvsrv1/lock\t10.033s\n```\n", "test_method": "cd src/kvsrv1/lock  && go test -v", "test_results": "=== RUN   TestOneClientReliable\nTest: 1 lock clients (reliable network)...\n  ... Passed --   2.0  1   968    0\n--- PASS: TestOneClientReliable (2.01s)\n=== RUN   TestManyClientsReliable\nTest: 10 lock clients (reliable network)...\n  ... Passed --   2.1  1 10789    0\n--- PASS: TestManyClientsReliable (2.12s)\n=== RUN   TestOneClientUnreliable\nTest: 1 lock clients (unreliable network)...\n  ... Passed --   2.3  1    70    0\n--- PASS: TestOneClientUnreliable (2.27s)\n=== RUN   TestManyClientsUnreliable\nTest: 10 lock clients (unreliable network)...\n  ... Passed --   3.6  1   908    0\n--- PASS: TestManyClientsUnreliable (3.62s)\nPASS\nok  \t6.5840/kvsrv1/lock\t10.033s", "difficulty": "easy", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvsrv1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_17", "task_name": "problems/system_lab_17.md", "task": "# Problem Context\n## Introduction\nThis is the first in a series of labs in which you'll build a fault-tolerant key/value storage system. In this lab you'll implement Raft, a replicated state machine protocol. In the next lab you'll build a key/value service on top of Raft. Then you will \u00c2\u0093shard\u00c2\u0094 your service over multiple replicated state machines for higher performance.\n\nA replicated service achieves fault tolerance by storing complete copies of its state (i.e., data) on multiple replica servers. Replication allows the service to continue operating even if some of its servers experience failures (crashes or a broken or flaky network). The challenge is that failures may cause the replicas to hold differing copies of the data.\n\nRaft organizes client requests into a sequence, called the log, and ensures that all the replica servers see the same log. Each replica executes client requests in log order, applying them to its local copy of the service's state. Since all the live replicas see the same log contents, they all execute the same requests in the same order, and thus continue to have identical service state. If a server fails but later recovers, Raft takes care of bringing its log up to date. Raft will continue to operate as long as at least a majority of the servers are alive and can talk to each other. If there is no such majority, Raft will make no progress, but will pick up where it left off as soon as a majority can communicate again.\n\nIn this lab you'll implement Raft as a Go object type with associated methods, meant to be used as a module in a larger service. A set of Raft instances talk to each other with RPC to maintain replicated logs. Your Raft interface will support an indefinite sequence of numbered commands, also called log entries. The entries are numbered with *index numbers*. The log entry with a given index will eventually be committed. At that point, your Raft should send the log entry to the larger service for it to execute.\n\nYou should follow the design in the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf), with particular attention to Figure 2. You'll implement most of what's in the paper, including saving persistent state and reading it after a node fails and then restarts. You will not implement cluster membership changes (Section 6).\n\nThis lab is due in four parts. You must submit each part on the corresponding due date.\n## Getiting Started\nDo a `git pull` to get the latest lab software.\n\nIf you have done Lab 1, you already have a copy of the lab source code. If not, you can find directions for obtaining the source via git in the [Lab 1 instructions](http://nil.csail.mit.edu/6.5840/2025/labs/lab-mr.html).\n\nWe supply you with skeleton code `src/raft/raft.go`. We also supply a set of tests, which you should use to drive your implementation efforts, and which we'll use to grade your submitted lab. The tests are in `src/raft/raft_test.go`.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should check that your code does not have races, by running the tests with the `-race` flag as you develop your solution.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/raft1\n$ go test\nTest (3A): initial election (reliable network)...\nFatal: expected one leader, got none\n--- FAIL: TestInitialElection3A (4.90s)\nTest (3A): election after network failure (reliable network)...\nFatal: expected one leader, got none\n--- FAIL: TestReElection3A (5.05s)\n...\n$\n```\n\n## The Code\nImplement Raft by adding code to `raft/raft.go`. In that file you'll find skeleton code, plus examples of how to send and receive RPCs.\n\nYour implementation must support the following interface, which the tester and (eventually) your key/value server will use. You'll find more details in comments in `raft.go`.\n\n```\n// create a new Raft server instance:\nrf := Make(peers, me, persister, applyCh)\n\n// start agreement on a new log entry:\nrf.Start(command interface{}) (index, term, isleader)\n\n// ask a Raft for its current term, and whether it thinks it is leader\nrf.GetState() (term, isLeader)\n\n// each time a new entry is committed to the log, each Raft peer\n// should send an ApplyMsg to the service (or tester).\ntype ApplyMsg\n```\n\nA service calls `Make(peers,me,\u00c2\u0085)` to create a Raft peer. The peers argument is an array of network identifiers of the Raft peers (including this one), for use with RPC. The `me` argument is the index of this peer in the peers array. `Start(command)` asks Raft to start the processing to append the command to the replicated log. `Start()` should return immediately, without waiting for the log appends to complete. The service expects your implementation to send an `ApplyMsg` for each newly committed log entry to the `applyCh` channel argument to `Make()`.\n\n`raft.go` contains example code that sends an RPC (`sendRequestVote()`) and that handles an incoming RPC (`RequestVote()`). Your Raft peers should exchange RPCs using the labrpc Go package (source in `src/labrpc`). The tester can tell `labrpc` to delay RPCs, re-order them, and discard them to simulate various network failures. While you can temporarily modify `labrpc`, make sure your Raft works with the original `labrpc`, since that's what we'll use to test and grade your lab. Your Raft instances must interact only with RPC; for example, they are not allowed to communicate using shared Go variables or files.\n\nSubsequent labs build on this lab, so it is important to give yourself enough time to write solid code.\n# Your Task \nImplement Raft leader election and heartbeats (`AppendEntries` RPCs with no log entries). The goal for Part 3A is for a single leader to be elected, for the leader to remain the leader if there are no failures, and for a new leader to take over if the old leader fails or if packets to/from the old leader are lost. Run `go test -run 3A `to test your 3A code.\n\n- You can't easily run your Raft implementation directly; instead you should run it by way of the tester, i.e. `go test -run 3A `.\n- Follow the paper's Figure 2. At this point you care about sending and receiving RequestVote RPCs, the Rules for Servers that relate to elections, and the State related to leader election,\n- Add the Figure 2 state for leader election to the `Raft` struct in `raft.go`. You'll also need to define a struct to hold information about each log entry.\n- Fill in the `RequestVoteArgs` and `RequestVoteReply` structs. Modify `Make()` to create a background goroutine that will kick off leader election periodically by sending out `RequestVote` RPCs when it hasn't heard from another peer for a while. Implement the `RequestVote()` RPC handler so that servers will vote for one another.\n- To implement heartbeats, define an `AppendEntries` RPC struct (though you may not need all the arguments yet), and have the leader send them out periodically. Write an `AppendEntries` RPC handler method.\n- The tester requires that the leader send heartbeat RPCs no more than ten times per second.\n- The tester requires your Raft to elect a new leader within five seconds of the failure of the old leader (if a majority of peers can still communicate).\n- The paper's Section 5.2 mentions election timeouts in the range of 150 to 300 milliseconds. Such a range only makes sense if the leader sends heartbeats considerably more often than once per 150 milliseconds (e.g., once per 10 milliseconds). Because the tester limits you tens of heartbeats per second, you will have to use an election timeout larger than the paper's 150 to 300 milliseconds, but not too large, because then you may fail to elect a leader within five seconds.\n- You may find Go's [rand](https://golang.org/pkg/math/rand/) useful.\n- You'll need to write code that takes actions periodically or after delays in time. The easiest way to do this is to create a goroutine with a loop that calls [time.Sleep()](https://golang.org/pkg/time/#Sleep); see the `ticker()` goroutine that `Make()` creates for this purpose. Don't use Go's `time.Timer` or `time.Ticker`, which are difficult to use correctly.\n- If your code has trouble passing the tests, read the paper's Figure 2 again; the full logic for leader election is spread over multiple parts of the figure.\n- Don't forget to implement `GetState()`.\n- The tester calls your Raft's `rf.Kill()` when it is permanently shutting down an instance. You can check whether `Kill()` has been called using `rf.killed()`. You may want to do this in all loops, to avoid having dead Raft instances print confusing messages.\n- Go RPC sends only struct fields whose names start with capital letters. Sub-structures must also have capitalized field names (e.g. fields of log records in an array). The `labgob` package will warn you about this; don't ignore the warnings.\n- The most challenging part of this lab may be the debugging. Spend some time making your implementation easy to debug. Refer to the [Guidance](http://nil.csail.mit.edu/6.5840/2025/labs/guidance.html) page for debugging tips.\n- If you fail a test, the tester produces a file that visualizes a timeline with events marked along it, including network partitions, crashed servers, and checks performed. Here's an [example of the visualization](http://nil.csail.mit.edu/6.5840/2025/labs/vis.html). Further, you can add your own annotations by writing, for example, `tester.Annotate(\"Server 0\", \"short description\", \"details\")`. This is a new feature we added this year, so if you have any feedback regarding the visualizer (e.g., bug reports, what annotation APIs that you think might be helpful, what information you want the visualizer to show, etc.), please let us know!\n\nBe sure you pass the 3A tests before submitting Part 3A, so that you see something like this:\n\n```\n$ go test -run 3A\nTest (3A): initial election (reliable network)...\n  ... Passed --   3.6  3   106    0\nTest (3A): election after network failure (reliable network)...\n  ... Passed --   7.6  3   304    0\nTest (3A): multiple elections (reliable network)...\n  ... Passed --   8.4  7   954    0\nPASS\nok      6.5840/raft1    19.834sak\n$\n```\n\nEach \"Passed\" line contains five numbers; these are the time that the test took in seconds, the number of Raft peers, the number of RPCs sent during the test, the total number of bytes in the RPC messages, and the number of log entries that Raft reports were committed. Your numbers will differ from those shown here. You can ignore the numbers if you like, but they may help you sanity-check the number of RPCs that your implementation sends. For all of labs 3, 4, and 5, the grading script will fail your solution if it takes more than 600 seconds for all of the tests (`go test`), or if any individual test takes more than 120 seconds.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should make sure that your code consistently passes the tests with the `-race` flag.", "test_method": "cd src/raft1 && go test -run 3A", "test_results": "Test (3A): initial election (reliable network)...\n  ... Passed --   3.6  3   106    0\nTest (3A): election after network failure (reliable network)...\n  ... Passed --   7.6  3   304    0\nTest (3A): multiple elections (reliable network)...\n  ... Passed --   8.4  7   954    0\nPASS\nok      6.5840/raft1    19.834sak", "difficulty": "moderate", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-raft1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_18", "task_name": "problems/system_lab_18.md", "task": "# Problem Context\n## Introduction\nThis is the first in a series of labs in which you'll build a fault-tolerant key/value storage system. In this lab you'll implement Raft, a replicated state machine protocol. In the next lab you'll build a key/value service on top of Raft. Then you will \u00c2\u0093shard\u00c2\u0094 your service over multiple replicated state machines for higher performance.\n\nA replicated service achieves fault tolerance by storing complete copies of its state (i.e., data) on multiple replica servers. Replication allows the service to continue operating even if some of its servers experience failures (crashes or a broken or flaky network). The challenge is that failures may cause the replicas to hold differing copies of the data.\n\nRaft organizes client requests into a sequence, called the log, and ensures that all the replica servers see the same log. Each replica executes client requests in log order, applying them to its local copy of the service's state. Since all the live replicas see the same log contents, they all execute the same requests in the same order, and thus continue to have identical service state. If a server fails but later recovers, Raft takes care of bringing its log up to date. Raft will continue to operate as long as at least a majority of the servers are alive and can talk to each other. If there is no such majority, Raft will make no progress, but will pick up where it left off as soon as a majority can communicate again.\n\nIn this lab you'll implement Raft as a Go object type with associated methods, meant to be used as a module in a larger service. A set of Raft instances talk to each other with RPC to maintain replicated logs. Your Raft interface will support an indefinite sequence of numbered commands, also called log entries. The entries are numbered with *index numbers*. The log entry with a given index will eventually be committed. At that point, your Raft should send the log entry to the larger service for it to execute.\n\nYou should follow the design in the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf), with particular attention to Figure 2. You'll implement most of what's in the paper, including saving persistent state and reading it after a node fails and then restarts. You will not implement cluster membership changes (Section 6).\n\nThis lab is due in four parts. You must submit each part on the corresponding due date.\n## Getiting Started\nDo a `git pull` to get the latest lab software.\n\nIf you have done Lab 1, you already have a copy of the lab source code. If not, you can find directions for obtaining the source via git in the [Lab 1 instructions](http://nil.csail.mit.edu/6.5840/2025/labs/lab-mr.html).\n\nWe supply you with skeleton code `src/raft/raft.go`. We also supply a set of tests, which you should use to drive your implementation efforts, and which we'll use to grade your submitted lab. The tests are in `src/raft/raft_test.go`.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should check that your code does not have races, by running the tests with the `-race` flag as you develop your solution.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/raft1\n$ go test\nTest (3A): initial election (reliable network)...\nFatal: expected one leader, got none\n--- FAIL: TestInitialElection3A (4.90s)\nTest (3A): election after network failure (reliable network)...\nFatal: expected one leader, got none\n--- FAIL: TestReElection3A (5.05s)\n...\n$\n```\n\n## The Code\nImplement Raft by adding code to `raft/raft.go`. In that file you'll find skeleton code, plus examples of how to send and receive RPCs.\n\nYour implementation must support the following interface, which the tester and (eventually) your key/value server will use. You'll find more details in comments in `raft.go`.\n\n```\n// create a new Raft server instance:\nrf := Make(peers, me, persister, applyCh)\n\n// start agreement on a new log entry:\nrf.Start(command interface{}) (index, term, isleader)\n\n// ask a Raft for its current term, and whether it thinks it is leader\nrf.GetState() (term, isLeader)\n\n// each time a new entry is committed to the log, each Raft peer\n// should send an ApplyMsg to the service (or tester).\ntype ApplyMsg\n```\n\nA service calls `Make(peers,me,\u00c2\u0085)` to create a Raft peer. The peers argument is an array of network identifiers of the Raft peers (including this one), for use with RPC. The `me` argument is the index of this peer in the peers array. `Start(command)` asks Raft to start the processing to append the command to the replicated log. `Start()` should return immediately, without waiting for the log appends to complete. The service expects your implementation to send an `ApplyMsg` for each newly committed log entry to the `applyCh` channel argument to `Make()`.\n\n`raft.go` contains example code that sends an RPC (`sendRequestVote()`) and that handles an incoming RPC (`RequestVote()`). Your Raft peers should exchange RPCs using the labrpc Go package (source in `src/labrpc`). The tester can tell `labrpc` to delay RPCs, re-order them, and discard them to simulate various network failures. While you can temporarily modify `labrpc`, make sure your Raft works with the original `labrpc`, since that's what we'll use to test and grade your lab. Your Raft instances must interact only with RPC; for example, they are not allowed to communicate using shared Go variables or files.\n\nSubsequent labs build on this lab, so it is important to give yourself enough time to write solid code.\n# Your Task \nImplement the leader and follower code to append new log entries, so that the `go test -run 3B `tests pass.\n\n- Run `git pull` to get the latest lab software.\n- Raft log is 1-indexed, but we suggest that you view it as 0-indexed, and starting out with an entry (at index=0) that has term 0. That allows the very first AppendEntries RPC to contain 0 as PrevLogIndex, and be a valid index into the log.\n- Your first goal should be to pass `TestBasicAgree3B()`. Start by implementing `Start()`, then write the code to send and receive new log entries via `AppendEntries` RPCs, following Figure 2. Send each newly committed entry on `applyCh` on each peer.\n- You will need to implement the election restriction (section 5.4.1 in the paper).\n- Your code may have loops that repeatedly check for certain events. Don't have these loops execute continuously without pausing, since that will slow your implementation enough that it fails tests. Use Go's [condition variables](https://golang.org/pkg/sync/#Cond), or insert a `time.Sleep(10 * time.Millisecond)` in each loop iteration.\n- Do yourself a favor for future labs and write (or re-write) code that's clean and clear. For ideas, re-visit our the [Guidance page](http://nil.csail.mit.edu/6.5840/2025/labs/guidance.html) with tips on how to develop and debug your code.\n- If you fail a test, look at `raft_test.go` and trace the test code from there to understand what's being tested.\n\nThe tests for upcoming labs may fail your code if it runs too slowly. You can check how much real time and CPU time your solution uses with the time command. Here's typical output:\n\n```\n$ time go test -run 3B\nTest (3B): basic agreement (reliable network)...\n  ... Passed --   1.3  3    18    0\nTest (3B): RPC byte count (reliable network)...\n  ... Passed --   2.8  3    56    0\nTest (3B): test progressive failure of followers (reliable network)...\n  ... Passed --   5.3  3   188    0\nTest (3B): test failure of leaders (reliable network)...\n  ... Passed --   6.4  3   378    0\nTest (3B): agreement after follower reconnects (reliable network)...\n  ... Passed --   5.9  3   176    0\nTest (3B): no agreement if too many followers disconnect (reliable network)...\n  ... Passed --   4.3  5   288    0\nTest (3B): concurrent Start()s (reliable network)...\n  ... Passed --   1.5  3    32    0\nTest (3B): rejoin of partitioned leader (reliable network)...\n  ... Passed --   5.3  3   216    0\nTest (3B): leader backs up quickly over incorrect follower logs (reliable network)...\n  ... Passed --  12.1  5  1528    0\nTest (3B): RPC counts aren't too high (reliable network)...\n  ... Passed --   3.1  3   106    0\nPASS\nok      6.5840/raft1    48.353s\ngo test -run 3B  1.37s user 0.74s system 4% cpu 48.865 total\n$\n```\n\nThe \"ok 6.5840/raft 35.557s\" means that Go measured the time taken for the 3B tests to be 35.557 seconds of real (wall-clock) time. The \"user 0m2.556s\" means that the code consumed 2.556 seconds of CPU time, or time spent actually executing instructions (rather than waiting or sleeping). If your solution uses much more than a minute of real time for the 3B tests, or much more than 5 seconds of CPU time, you may run into trouble later on. Look for time spent sleeping or waiting for RPC timeouts, loops that run without sleeping or waiting for conditions or channel messages, or large numbers of RPCs sent.", "test_method": "cd src/raft1 && time go test -run 3B", "test_results": "Test (3B): basic agreement (reliable network)...\n  ... Passed --   1.3  3    18    0\nTest (3B): RPC byte count (reliable network)...\n  ... Passed --   2.8  3    56    0\nTest (3B): test progressive failure of followers (reliable network)...\n  ... Passed --   5.3  3   188    0\nTest (3B): test failure of leaders (reliable network)...\n  ... Passed --   6.4  3   378    0\nTest (3B): agreement after follower reconnects (reliable network)...\n  ... Passed --   5.9  3   176    0\nTest (3B): no agreement if too many followers disconnect (reliable network)...\n  ... Passed --   4.3  5   288    0\nTest (3B): concurrent Start()s (reliable network)...\n  ... Passed --   1.5  3    32    0\nTest (3B): rejoin of partitioned leader (reliable network)...\n  ... Passed --   5.3  3   216    0\nTest (3B): leader backs up quickly over incorrect follower logs (reliable network)...\n  ... Passed --  12.1  5  1528    0\nTest (3B): RPC counts aren't too high (reliable network)...\n  ... Passed --   3.1  3   106    0\nPASS\nok      6.5840/raft1    48.353s\ngo test -run 3B  1.37s user 0.74s system 4% cpu 48.865 total", "difficulty": "hard", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-raft1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_19", "task_name": "problems/system_lab_19.md", "task": "# Problem Context\n## Introduction\nThis is the first in a series of labs in which you'll build a fault-tolerant key/value storage system. In this lab you'll implement Raft, a replicated state machine protocol. In the next lab you'll build a key/value service on top of Raft. Then you will \u00c2\u0093shard\u00c2\u0094 your service over multiple replicated state machines for higher performance.\n\nA replicated service achieves fault tolerance by storing complete copies of its state (i.e., data) on multiple replica servers. Replication allows the service to continue operating even if some of its servers experience failures (crashes or a broken or flaky network). The challenge is that failures may cause the replicas to hold differing copies of the data.\n\nRaft organizes client requests into a sequence, called the log, and ensures that all the replica servers see the same log. Each replica executes client requests in log order, applying them to its local copy of the service's state. Since all the live replicas see the same log contents, they all execute the same requests in the same order, and thus continue to have identical service state. If a server fails but later recovers, Raft takes care of bringing its log up to date. Raft will continue to operate as long as at least a majority of the servers are alive and can talk to each other. If there is no such majority, Raft will make no progress, but will pick up where it left off as soon as a majority can communicate again.\n\nIn this lab you'll implement Raft as a Go object type with associated methods, meant to be used as a module in a larger service. A set of Raft instances talk to each other with RPC to maintain replicated logs. Your Raft interface will support an indefinite sequence of numbered commands, also called log entries. The entries are numbered with *index numbers*. The log entry with a given index will eventually be committed. At that point, your Raft should send the log entry to the larger service for it to execute.\n\nYou should follow the design in the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf), with particular attention to Figure 2. You'll implement most of what's in the paper, including saving persistent state and reading it after a node fails and then restarts. You will not implement cluster membership changes (Section 6).\n\nThis lab is due in four parts. You must submit each part on the corresponding due date.\n## Getiting Started\nDo a `git pull` to get the latest lab software.\n\nIf you have done Lab 1, you already have a copy of the lab source code. If not, you can find directions for obtaining the source via git in the [Lab 1 instructions](http://nil.csail.mit.edu/6.5840/2025/labs/lab-mr.html).\n\nWe supply you with skeleton code `src/raft/raft.go`. We also supply a set of tests, which you should use to drive your implementation efforts, and which we'll use to grade your submitted lab. The tests are in `src/raft/raft_test.go`.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should check that your code does not have races, by running the tests with the `-race` flag as you develop your solution.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/raft1\n$ go test\nTest (3A): initial election (reliable network)...\nFatal: expected one leader, got none\n--- FAIL: TestInitialElection3A (4.90s)\nTest (3A): election after network failure (reliable network)...\nFatal: expected one leader, got none\n--- FAIL: TestReElection3A (5.05s)\n...\n$\n```\n\n## The Code\nImplement Raft by adding code to `raft/raft.go`. In that file you'll find skeleton code, plus examples of how to send and receive RPCs.\n\nYour implementation must support the following interface, which the tester and (eventually) your key/value server will use. You'll find more details in comments in `raft.go`.\n\n```\n// create a new Raft server instance:\nrf := Make(peers, me, persister, applyCh)\n\n// start agreement on a new log entry:\nrf.Start(command interface{}) (index, term, isleader)\n\n// ask a Raft for its current term, and whether it thinks it is leader\nrf.GetState() (term, isLeader)\n\n// each time a new entry is committed to the log, each Raft peer\n// should send an ApplyMsg to the service (or tester).\ntype ApplyMsg\n```\n\nA service calls `Make(peers,me,\u00c2\u0085)` to create a Raft peer. The peers argument is an array of network identifiers of the Raft peers (including this one), for use with RPC. The `me` argument is the index of this peer in the peers array. `Start(command)` asks Raft to start the processing to append the command to the replicated log. `Start()` should return immediately, without waiting for the log appends to complete. The service expects your implementation to send an `ApplyMsg` for each newly committed log entry to the `applyCh` channel argument to `Make()`.\n\n`raft.go` contains example code that sends an RPC (`sendRequestVote()`) and that handles an incoming RPC (`RequestVote()`). Your Raft peers should exchange RPCs using the labrpc Go package (source in `src/labrpc`). The tester can tell `labrpc` to delay RPCs, re-order them, and discard them to simulate various network failures. While you can temporarily modify `labrpc`, make sure your Raft works with the original `labrpc`, since that's what we'll use to test and grade your lab. Your Raft instances must interact only with RPC; for example, they are not allowed to communicate using shared Go variables or files.\n\nSubsequent labs build on this lab, so it is important to give yourself enough time to write solid code.\n# Your Task \nIf a Raft-based server reboots it should resume service where it left off. This requires that Raft keep persistent state that survives a reboot. The paper's Figure 2 mentions which state should be persistent.\n\nA real implementation would write Raft's persistent state to disk each time it changed, and would read the state from disk when restarting after a reboot. Your implementation won't use the disk; instead, it will save and restore persistent state from a `Persister` object (see `persister.go`). Whoever calls `Raft.Make()` supplies a `Persister` that initially holds Raft's most recently persisted state (if any). Raft should initialize its state from that `Persister`, and should use it to save its persistent state each time the state changes. Use the `Persister`'s `ReadRaftState()` and `Save()` methods.\n\nComplete the functions `persist()` and `readPersist()` in `raft.go` by adding code to save and restore persistent state. You will need to encode (or \"serialize\") the state as an array of bytes in order to pass it to the `Persister`. Use the `labgob` encoder; see the comments in `persist()` and `readPersist()`. `labgob` is like Go's `gob` encoder but prints error messages if you try to encode structures with lower-case field names. For now, pass `nil` as the second argument to `persister.Save()`. Insert calls to `persist()` at the points where your implementation changes persistent state. Once you've done this, and if the rest of your implementation is correct, you should pass all of the 3C tests.\n\nYou will probably need the optimization that backs up nextIndex by more than one entry at a time. Look at the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf) starting at the bottom of page 7 and top of page 8 (marked by a gray line). The paper is vague about the details; you will need to fill in the gaps. One possibility is to have a rejection message include:\n\n```\n    XTerm:  term in the conflicting entry (if any)\n    XIndex: index of first entry with that term (if any)\n    XLen:   log length\n```\n\nThen the leader's logic can be something like:\n\n```\n  Case 1: leader doesn't have XTerm:\n    nextIndex = XIndex\n  Case 2: leader has XTerm:\n    nextIndex = (index of leader's last entry for XTerm) + 1\n  Case 3: follower's log is too short:\n    nextIndex = XLen\n```\n\nA few other hints:\n\n- Run `git pull` to get the latest lab software.\n- The 3C tests are more demanding than those for 3A or 3B, and failures may be caused by problems in your code for 3A or 3B.\n\nYour code should pass all the 3C tests (as shown below), as well as the 3A and 3B tests.\n\n```\n$ go test -run 3C\nTest (3C): basic persistence (reliable network)...\n  ... Passed --   6.6  3   110    0\nTest (3C): more persistence (reliable network)...\n  ... Passed --  15.6  5   428    0\nTest (3C): partitioned leader and one follower crash, leader restarts (reliable network)...\n  ... Passed --   3.1  3    50    0\nTest (3C): Figure 8 (reliable network)...\n  ... Passed --  33.7  5   654    0\nTest (3C): unreliable agreement (unreliable network)...\n  ... Passed --   2.1  5  1076    0\nTest (3C): Figure 8 (unreliable) (unreliable network)...\n  ... Passed --  31.9  5  4400    0\nTest (3C): churn (reliable network)...\n  ... Passed --  16.8  5  4896    0\nTest (3C): unreliable churn (unreliable network)...\n  ... Passed --  16.1  5  7204    0\nPASS\nok      6.5840/raft1    126.054s\n$\n```\n\nIt is a good idea to run the tests multiple times before submitting and check that each run prints `PASS`.\n\n```\n$ for i in {0..10}; do go test; done\n```\n", "test_method": "cd src/raft1 && go test -run 3C", "test_results": "Test (3C): basic persistence (reliable network)...\n  ... Passed --   6.6  3   110    0\nTest (3C): more persistence (reliable network)...\n  ... Passed --  15.6  5   428    0\nTest (3C): partitioned leader and one follower crash, leader restarts (reliable network)...\n  ... Passed --   3.1  3    50    0\nTest (3C): Figure 8 (reliable network)...\n  ... Passed --  33.7  5   654    0\nTest (3C): unreliable agreement (unreliable network)...\n  ... Passed --   2.1  5  1076    0\nTest (3C): Figure 8 (unreliable) (unreliable network)...\n  ... Passed --  31.9  5  4400    0\nTest (3C): churn (reliable network)...\n  ... Passed --  16.8  5  4896    0\nTest (3C): unreliable churn (unreliable network)...\n  ... Passed --  16.1  5  7204    0\nPASS\nok      6.5840/raft1    126.054s", "difficulty": "hard", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-raft1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_20", "task_name": "problems/system_lab_20.md", "task": "# Problem Context\n## Introduction\nThis is the first in a series of labs in which you'll build a fault-tolerant key/value storage system. In this lab you'll implement Raft, a replicated state machine protocol. In the next lab you'll build a key/value service on top of Raft. Then you will \u00c2\u0093shard\u00c2\u0094 your service over multiple replicated state machines for higher performance.\n\nA replicated service achieves fault tolerance by storing complete copies of its state (i.e., data) on multiple replica servers. Replication allows the service to continue operating even if some of its servers experience failures (crashes or a broken or flaky network). The challenge is that failures may cause the replicas to hold differing copies of the data.\n\nRaft organizes client requests into a sequence, called the log, and ensures that all the replica servers see the same log. Each replica executes client requests in log order, applying them to its local copy of the service's state. Since all the live replicas see the same log contents, they all execute the same requests in the same order, and thus continue to have identical service state. If a server fails but later recovers, Raft takes care of bringing its log up to date. Raft will continue to operate as long as at least a majority of the servers are alive and can talk to each other. If there is no such majority, Raft will make no progress, but will pick up where it left off as soon as a majority can communicate again.\n\nIn this lab you'll implement Raft as a Go object type with associated methods, meant to be used as a module in a larger service. A set of Raft instances talk to each other with RPC to maintain replicated logs. Your Raft interface will support an indefinite sequence of numbered commands, also called log entries. The entries are numbered with *index numbers*. The log entry with a given index will eventually be committed. At that point, your Raft should send the log entry to the larger service for it to execute.\n\nYou should follow the design in the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf), with particular attention to Figure 2. You'll implement most of what's in the paper, including saving persistent state and reading it after a node fails and then restarts. You will not implement cluster membership changes (Section 6).\n\nThis lab is due in four parts. You must submit each part on the corresponding due date.\n## Getiting Started\nDo a `git pull` to get the latest lab software.\n\nIf you have done Lab 1, you already have a copy of the lab source code. If not, you can find directions for obtaining the source via git in the [Lab 1 instructions](http://nil.csail.mit.edu/6.5840/2025/labs/lab-mr.html).\n\nWe supply you with skeleton code `src/raft/raft.go`. We also supply a set of tests, which you should use to drive your implementation efforts, and which we'll use to grade your submitted lab. The tests are in `src/raft/raft_test.go`.\n\nWhen we grade your submissions, we will run the tests without the [`-race` flag](https://go.dev/blog/race-detector). However, you should check that your code does not have races, by running the tests with the `-race` flag as you develop your solution.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/raft1\n$ go test\nTest (3A): initial election (reliable network)...\nFatal: expected one leader, got none\n--- FAIL: TestInitialElection3A (4.90s)\nTest (3A): election after network failure (reliable network)...\nFatal: expected one leader, got none\n--- FAIL: TestReElection3A (5.05s)\n...\n$\n```\n\n## The Code\nImplement Raft by adding code to `raft/raft.go`. In that file you'll find skeleton code, plus examples of how to send and receive RPCs.\n\nYour implementation must support the following interface, which the tester and (eventually) your key/value server will use. You'll find more details in comments in `raft.go`.\n\n```\n// create a new Raft server instance:\nrf := Make(peers, me, persister, applyCh)\n\n// start agreement on a new log entry:\nrf.Start(command interface{}) (index, term, isleader)\n\n// ask a Raft for its current term, and whether it thinks it is leader\nrf.GetState() (term, isLeader)\n\n// each time a new entry is committed to the log, each Raft peer\n// should send an ApplyMsg to the service (or tester).\ntype ApplyMsg\n```\n\nA service calls `Make(peers,me,\u00c2\u0085)` to create a Raft peer. The peers argument is an array of network identifiers of the Raft peers (including this one), for use with RPC. The `me` argument is the index of this peer in the peers array. `Start(command)` asks Raft to start the processing to append the command to the replicated log. `Start()` should return immediately, without waiting for the log appends to complete. The service expects your implementation to send an `ApplyMsg` for each newly committed log entry to the `applyCh` channel argument to `Make()`.\n\n`raft.go` contains example code that sends an RPC (`sendRequestVote()`) and that handles an incoming RPC (`RequestVote()`). Your Raft peers should exchange RPCs using the labrpc Go package (source in `src/labrpc`). The tester can tell `labrpc` to delay RPCs, re-order them, and discard them to simulate various network failures. While you can temporarily modify `labrpc`, make sure your Raft works with the original `labrpc`, since that's what we'll use to test and grade your lab. Your Raft instances must interact only with RPC; for example, they are not allowed to communicate using shared Go variables or files.\n\nSubsequent labs build on this lab, so it is important to give yourself enough time to write solid code.\n# Your Task \nAs things stand now, a rebooting server replays the complete Raft log in order to restore its state. However, it's not practical for a long-running service to remember the complete Raft log forever. Instead, you'll modify Raft to cooperate with services that persistently store a \"snapshot\" of their state from time to time, at which point Raft discards log entries that precede the snapshot. The result is a smaller amount of persistent data and faster restart. However, it's now possible for a follower to fall so far behind that the leader has discarded the log entries it needs to catch up; the leader must then send a snapshot plus the log starting at the time of the snapshot. Section 7 of the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf) outlines the scheme; you will have to design the details.\n\nYour Raft must provide the following function that the service can call with a serialized snapshot of its state:\n\n```\nSnapshot(index int, snapshot []byte)\n```\n\nIn Lab 3D, the tester calls `Snapshot()` periodically. In Lab 4, you will write a key/value server that calls `Snapshot()`; the snapshot will contain the complete table of key/value pairs. The service layer calls `Snapshot()` on every peer (not just on the leader).\n\nThe `index` argument indicates the highest log entry that's reflected in the snapshot. Raft should discard its log entries before that point. You'll need to revise your Raft code to operate while storing only the tail of the log.\n\nYou'll need to implement the `InstallSnapshot` RPC discussed in the paper that allows a Raft leader to tell a lagging Raft peer to replace its state with a snapshot. You will likely need to think through how InstallSnapshot should interact with the state and rules in Figure 2.\n\nWhen a follower's Raft code receives an InstallSnapshot RPC, it can use the `applyCh` to send the snapshot to the service in an `ApplyMsg`. The `ApplyMsg` struct definition already contains the fields you will need (and which the tester expects). Take care that these snapshots only advance the service's state, and don't cause it to move backwards.\n\nIf a server crashes, it must restart from persisted data. Your Raft should persist both Raft state and the corresponding snapshot. Use the second argument to `persister.Save()` to save the snapshot. If there's no snapshot, pass `nil` as the second argument.\n\nWhen a server restarts, the application layer reads the persisted snapshot and restores its saved state.\n\nImplement `Snapshot()` and the InstallSnapshot RPC, as well as the changes to Raft to support these (e.g, operation with a trimmed log). Your solution is complete when it passes the 3D tests (and all the previous Lab 3 tests).\n\n- `git pull` to make sure you have the latest software.\n- A good place to start is to modify your code to so that it is able to store just the part of the log starting at some index X. Initially you can set X to zero and run the 3B/3C tests. Then make `Snapshot(index)` discard the log before `index`, and set X equal to `index`. If all goes well you should now pass the first 3D test.\n- A common reason for failing the first 3D test is that followers take too long to catch up to the leader.\n- Next: have the leader send an InstallSnapshot RPC if it doesn't have the log entries required to bring a follower up to date.\n- Send the entire snapshot in a single InstallSnapshot RPC. Don't implement Figure 13's `offset` mechanism for splitting up the snapshot.\n- Raft must discard old log entries in a way that allows the Go garbage collector to free and re-use the memory; this requires that there be no reachable references (pointers) to the discarded log entries.\n- A reasonable amount of time to consume for the full set of Lab 3 tests (3A+3B+3C+3D) without `-race` is 6 minutes of real time and one minute of CPU time. When running with `-race`, it is about 10 minutes of real time and two minutes of CPU time.\n\nYour code should pass all the 3D tests (as shown below), as well as the 3A, 3B, and 3C tests.\n\n```\n$ go test -run 3D\nTest (3D): snapshots basic (reliable network)...\n  ... Passed --   3.3  3   522    0\nTest (3D): install snapshots (disconnect) (reliable network)...\n  ... Passed --  48.4  3  2710    0\nTest (3D): install snapshots (disconnect) (unreliable network)...\n  ... Passed --  56.1  3  3025    0\nTest (3D): install snapshots (crash) (reliable network)...\n  ... Passed --  33.3  3  1559    0\nTest (3D): install snapshots (crash) (unreliable network)...\n  ... Passed --  38.1  3  1723    0\nTest (3D): crash and restart all servers (unreliable network)...\n  ... Passed --  11.2  3   296    0\nTest (3D): snapshot initialization after crash (unreliable network)...\n  ... Passed --   4.3  3    84    0\nPASS\nok      6.5840/raft1    195.006s\n```\n", "test_method": "cd src/raft1 && go test -run 3D", "test_results": "Test (3D): snapshots basic (reliable network)...\n  ... Passed --   3.3  3   522    0\nTest (3D): install snapshots (disconnect) (reliable network)...\n  ... Passed --  48.4  3  2710    0\nTest (3D): install snapshots (disconnect) (unreliable network)...\n  ... Passed --  56.1  3  3025    0\nTest (3D): install snapshots (crash) (reliable network)...\n  ... Passed --  33.3  3  1559    0\nTest (3D): install snapshots (crash) (unreliable network)...\n  ... Passed --  38.1  3  1723    0\nTest (3D): crash and restart all servers (unreliable network)...\n  ... Passed --  11.2  3   296    0\nTest (3D): snapshot initialization after crash (unreliable network)...\n  ... Passed --   4.3  3    84    0\nPASS\nok      6.5840/raft1    195.006s", "difficulty": "hard", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-raft1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_21", "task_name": "problems/system_lab_21.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a fault-tolerant key/value storage service using your Raft library from [Lab 3](http://nil.csail.mit.edu/6.5840/2025/labs/lab-raft1.html). To clients, the service looks similar to the server of [Lab 2](http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvsrv1.html). However, instead of a single server, the service consists of a set of servers that use Raft to help them maintain identical databases. Your key/value service should continue to process client requests as long as a majority of the servers are alive and can communicate, in spite of other failures or network partitions. After Lab 4, you will have implemented all parts (Clerk, Service, and Raft) shown in the [diagram of Raft interactions](http://nil.csail.mit.edu/6.5840/2025/figs/kvraft.pdf).\n\nClients will interact with your key/value service through a Clerk, as in Lab 2. A Clerk implements the `Put` and `Get` methods with the same semantics as Lab 2: Puts are at-most-once and the Puts/Gets must form a linearizable history.\n\nProviding linearizability is relatively easy for a single server. It is harder if the service is replicated, since all servers must choose the same execution order for concurrent requests, must avoid replying to clients using state that isn't up to date, and must recover their state after a failure in a way that preserves all acknowledged client updates.\n\nThis lab has three parts. In part A, you will implement a replicated-state machine package, `rsm`, using your raft implementation; `rsm` is agnostic of the requests that it replicates. In part B, you will implement a replicated key/value service using `rsm`, but without using snapshots. In part C, you will use your snapshot implementation from Lab 3D, which will allow Raft to discard old log entries. Please submit each part by the respective deadline.\n\nYou should review the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf), in particular Section 7 (but not 8). For a wider perspective, have a look at Chubby, Paxos Made Live, Spanner, Zookeeper, Harp, Viewstamped Replication, and [Bolosky et al.](http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf) Start early.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvraft1`. The skeleton code uses the skeleton package `src/kvraft1/rsm` to replicate a server. A server must implement the `StateMachine` interface defined in `rsm` to replicate itself using `rsm`. Most of your work will be implementing `rsm` to provide server-agnostic replication. You will also need to modify `kvraft1/client.go` and `kvraft1/server.go` to implement the server-specific parts. This split allows you to re-use `rsm` in the next lab. You may be able to re-use some of your Lab 2 code (e.g., re-using the server code by copying or importing the `\"src/kvsrv1\"` package) but it is not a requirement.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n..\n```\n\n## The Code\n\n# Your Task \n\n\n```\n$ cd src/kvraft1/rsm\n$ go test -v\n=== RUN   TestBasic\nTest RSM basic (reliable network)...\n..\n    config.go:147: one: took too long\n```\n\nIn the common situation of a client/server service using Raft for replication, the service interacts with Raft in two ways: the service leader submits client operations by calling `raft.Start()`, and all service replicas receive committed operations via Raft's `applyCh`, which they execute. On the leader, these two activities interact. At any given time, some server goroutines are handling client requests, have called `raft.Start()`, and each is waiting for its operation to commit and to find out what the result of executing the operation is. And as committed operations appear on the `applyCh`, each needs to be executed by the service, and the results need to be handed to the goroutine that called `raft.Start()` so that it can return the result to the client.\n\nThe `rsm` package encapsulates the above interaction. It sits as a layer between the service (e.g. a key/value database) and Raft. In `rsm/rsm.go` you will need to implement a \"reader\" goroutine that reads the `applyCh`, and a `rsm.Submit()` function that calls `raft.Start()` for a client operation and then waits for the reader goroutine to hand it the result of executing that operation.\n\nThe service that is using `rsm` appears to the `rsm` reader goroutine as a `StateMachine` object providing a `DoOp()` method. The reader goroutine should hand each committed operation to `DoOp()`; `DoOp()`'s return value should be given to the corresponding `rsm.Submit()` call for it to return. `DoOp()`'s argument and return value have type `any`; the actual values should have the same types as the argument and return values that the service passes to `rsm.Submit()`, respectively.\n\nThe service should pass each client operation to `rsm.Submit()`. To help the reader goroutine match `applyCh` messages with waiting calls to `rsm.Submit()`, `Submit()` should wrap each client operation in an `Op` structure along with a unique identifier. `Submit()` should then wait until the operation has committed and been executed, and return the result of execution (the value returned by `DoOp()`). If `raft.Start()` indicates that the current peer is not the Raft leader, `Submit()` should return an `rpc.ErrWrongLeader` error. `Submit()` should detect and handle the situation in which leadership changed just after it called `raft.Start()`, causing the operation to be lost (never committed).\n\nFor Part A, the `rsm` tester acts as the service, submitting operations that it interprets as increments on a state consisting of a single integer. In Part B you'll use `rsm` as part of a key/value service that implements `StateMachine` (and `DoOp()`), and calls `rsm.Submit()`.\n\nIf all goes well, the sequence of events for a client request is:\n\n- The client sends a request to the service leader.\n- The service leader calls `rsm.Submit()` with the request.\n- `rsm.Submit()` calls `raft.Start()` with the request, and then waits.\n- Raft commits the request and sends it on all peers' `applyCh`s.\n- The `rsm` reader goroutine on each peer reads the request from the `applyCh` and passes it to the service's `DoOp()`.\n- On the leader, the `rsm` reader goroutine hands the `DoOp()` return value to the `Submit()` goroutine that originally submitted the request, and `Submit()` returns that value.\n\nYour servers should not directly communicate; they should only interact with each other through Raft.\n\nImplement `rsm.go`: the `Submit()` method and a reader goroutine. You have completed this task if you pass the `rsm` 4A tests:\n\n```\n  $ cd src/kvraft1/rsm\n  $ go test -v -run 4A\n=== RUN   TestBasic4A\nTest RSM basic (reliable network)...\n  ... Passed --   1.2  3    48    0\n--- PASS: TestBasic4A (1.21s)\n=== RUN   TestLeaderFailure4A\n  ... Passed --  9223372036.9  3    31    0\n--- PASS: TestLeaderFailure4A (1.50s)\nPASS\nok      6.5840/kvraft1/rsm      2.887s\n```\n\n- You should not need to add any fields to the Raft `ApplyMsg`, or to Raft RPCs such as `AppendEntries`, but you are allowed to do so.\n- Your solution needs to handle an `rsm` leader that has called `Start()` for a request submitted with `Submit()` but loses its leadership before the request is committed to the log. One way to do this is for the `rsm` to detect that it has lost leadership, by noticing that Raft's term has changed or a different request has appeared at the index returned by `Start()`, and return `rpc.ErrWrongLeader` from `Submit()`. If the ex-leader is partitioned by itself, it won't know about new leaders; but any client in the same partition won't be able to talk to a new leader either, so it's OK in this case for the server to wait indefinitely until the partition heals.\n- The tester calls your Raft's `rf.Kill()` when it is shutting down a peer. Raft should close the `applyCh` so that your rsm learns about the shutdown, and can exit out of all loops.", "test_method": "cd src/kvraft1/rsm && go test -v -run 4A", "test_results": "=== RUN   TestBasic4A\nTest RSM basic (reliable network)...\n  ... Passed --   1.2  3    48    0\n--- PASS: TestBasic4A (1.21s)\n=== RUN   TestLeaderFailure4A\n  ... Passed --  9223372036.9  3    31    0\n--- PASS: TestLeaderFailure4A (1.50s)\nPASS\nok      6.5840/kvraft1/rsm      2.887s", "difficulty": "moderate/hard", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvraft1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_22", "task_name": "problems/system_lab_22.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a fault-tolerant key/value storage service using your Raft library from [Lab 3](http://nil.csail.mit.edu/6.5840/2025/labs/lab-raft1.html). To clients, the service looks similar to the server of [Lab 2](http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvsrv1.html). However, instead of a single server, the service consists of a set of servers that use Raft to help them maintain identical databases. Your key/value service should continue to process client requests as long as a majority of the servers are alive and can communicate, in spite of other failures or network partitions. After Lab 4, you will have implemented all parts (Clerk, Service, and Raft) shown in the [diagram of Raft interactions](http://nil.csail.mit.edu/6.5840/2025/figs/kvraft.pdf).\n\nClients will interact with your key/value service through a Clerk, as in Lab 2. A Clerk implements the `Put` and `Get` methods with the same semantics as Lab 2: Puts are at-most-once and the Puts/Gets must form a linearizable history.\n\nProviding linearizability is relatively easy for a single server. It is harder if the service is replicated, since all servers must choose the same execution order for concurrent requests, must avoid replying to clients using state that isn't up to date, and must recover their state after a failure in a way that preserves all acknowledged client updates.\n\nThis lab has three parts. In part A, you will implement a replicated-state machine package, `rsm`, using your raft implementation; `rsm` is agnostic of the requests that it replicates. In part B, you will implement a replicated key/value service using `rsm`, but without using snapshots. In part C, you will use your snapshot implementation from Lab 3D, which will allow Raft to discard old log entries. Please submit each part by the respective deadline.\n\nYou should review the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf), in particular Section 7 (but not 8). For a wider perspective, have a look at Chubby, Paxos Made Live, Spanner, Zookeeper, Harp, Viewstamped Replication, and [Bolosky et al.](http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf) Start early.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvraft1`. The skeleton code uses the skeleton package `src/kvraft1/rsm` to replicate a server. A server must implement the `StateMachine` interface defined in `rsm` to replicate itself using `rsm`. Most of your work will be implementing `rsm` to provide server-agnostic replication. You will also need to modify `kvraft1/client.go` and `kvraft1/server.go` to implement the server-specific parts. This split allows you to re-use `rsm` in the next lab. You may be able to re-use some of your Lab 2 code (e.g., re-using the server code by copying or importing the `\"src/kvsrv1\"` package) but it is not a requirement.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n..\n```\n\n## The Code\n\n# Your Task \n\n\n```\n$ cd src/kvraft1\n$ go test -v -run TestBasic4B\n=== RUN   TestBasic4B\nTest: one client (4B basic) (reliable network)...\n    kvtest.go:62: Wrong error \n$\n```\n\nNow you will use the `rsm` package to replicate a key/value server. Each of the servers (\"kvservers\") will have an associated rsm/Raft peer. Clerks send `Put()` and `Get()` RPCs to the kvserver whose associated Raft is the leader. The kvserver code submits the Put/Get operation to `rsm`, which replicates it using Raft and invokes your server's `DoOp` at each peer, which should apply the operations to the peer's key/value database; the intent is for the servers to maintain identical replicas of the key/value database.\n\nA `Clerk` sometimes doesn't know which kvserver is the Raft leader. If the `Clerk` sends an RPC to the wrong kvserver, or if it cannot reach the kvserver, the `Clerk` should re-try by sending to a different kvserver. If the key/value service commits the operation to its Raft log (and hence applies the operation to the key/value state machine), the leader reports the result to the `Clerk` by responding to its RPC. If the operation failed to commit (for example, if the leader was replaced), the server reports an error, and the `Clerk` retries with a different server.\n\nYour kvservers should not directly communicate; they should only interact with each other through Raft.\n\nYour first task is to implement a solution that works when there are no dropped messages, and no failed servers.\n\nFeel free to copy your client code from Lab 2 (`kvsrv1/client.go`) into `kvraft1/client.go`. You will need to add logic for deciding which kvserver to send each RPC to.\n\nYou'll also need to implement `Put()` and `Get()` RPC handlers in `server.go`. These handlers should submit the request to Raft using `rsm.Submit()`. As the `rsm` package reads commands from `applyCh`, it should invoke the `DoOp` method, which you will have to implement in `server.go`.\n\nYou have completed this task when you **reliably** pass the first test in the test suite, with `go test -v -run TestBasic4B`.\n\n- A kvserver should not complete a `Get()` RPC if it is not part of a majority (so that it does not serve stale data). A simple solution is to enter every `Get()` (as well as each `Put()`) in the Raft log using `Submit()`. You don't have to implement the optimization for read-only operations that is described in Section 8.\n- It's best to add locking from the start because the need to avoid deadlocks sometimes affects overall code design. Check that your code is race-free using `go test -race`.\n\nNow you should modify your solution to continue in the face of network and server failures. One problem you'll face is that a `Clerk` may have to send an RPC multiple times until it finds a kvserver that replies positively. If a leader fails just after committing an entry to the Raft log, the `Clerk` may not receive a reply, and thus may re-send the request to another leader. Each call to `Clerk.Put()` should result in just a single execution for a particular version number.\n\nAdd code to handle failures. Your `Clerk` can use a similar retry plan as in lab 2, including returning `ErrMaybe` if a response to a retried `Put` RPC is lost. You are done when your code reliably passes all the 4B tests, with `go test -v -run 4B`.\n\n- Recall that the rsm leader may lose its leadership and return `rpc.ErrWrongLeader` from `Submit()`. In this case you should arrange for the Clerk to re-send the request to other servers until it finds the new leader.\n- You will probably have to modify your Clerk to remember which server turned out to be the leader for the last RPC, and send the next RPC to that server first. This will avoid wasting time searching for the leader on every RPC, which may help you pass some of the tests quickly enough.\n\nYour code should now pass the Lab 4B tests, like this:\n\n```\n$ cd kvraft1\n$ go test -run 4B\nTest: one client (4B basic) ...\n  ... Passed --   3.2  5  1041  183\nTest: one client (4B speed) ...\n  ... Passed --  15.9  3  3169    0\nTest: many clients (4B many clients) ...\n  ... Passed --   3.9  5  3247  871\nTest: unreliable net, many clients (4B unreliable net, many clients) ...\n  ... Passed --   5.3  5  1035  167\nTest: unreliable net, one client (4B progress in majority) ...\n  ... Passed --   2.9  5   155    3\nTest: no progress in minority (4B) ...\n  ... Passed --   1.6  5   102    3\nTest: completion after heal (4B) ...\n  ... Passed --   1.3  5    67    4\nTest: partitions, one client (4B partitions, one client) ...\n  ... Passed --   6.2  5   958  155\nTest: partitions, many clients (4B partitions, many clients (4B)) ...\n  ... Passed --   6.8  5  3096  855\nTest: restarts, one client (4B restarts, one client 4B ) ...\n  ... Passed --   6.7  5   311   13\nTest: restarts, many clients (4B restarts, many clients) ...\n  ... Passed --   7.5  5  1223   95\nTest: unreliable net, restarts, many clients (4B unreliable net, restarts, many clients ) ...\n  ... Passed --   8.4  5   804   33\nTest: restarts, partitions, many clients (4B restarts, partitions, many clients) ...\n  ... Passed --  10.1  5  1308  105\nTest: unreliable net, restarts, partitions, many clients (4B unreliable net, restarts, partitions, many clients) ...\n  ... Passed --  11.9  5  1040   33\nTest: unreliable net, restarts, partitions, random keys, many clients (4B unreliable net, restarts, partitions, random keys, many clients) ...\n  ... Passed --  12.1  7  2801   93\nPASS\nok      6.5840/kvraft1  103.797s\n```\n\nThe numbers after each `Passed` are real time in seconds, number of peers, number of RPCs sent (including client RPCs), and number of key/value operations executed (`Clerk` Get/Put calls).", "test_method": "cd src/kvraft1 && go test -run 4B", "test_results": "Test: one client (4B basic) ...\n  ... Passed --   3.2  5  1041  183\nTest: one client (4B speed) ...\n  ... Passed --  15.9  3  3169    0\nTest: many clients (4B many clients) ...\n  ... Passed --   3.9  5  3247  871\nTest: unreliable net, many clients (4B unreliable net, many clients) ...\n  ... Passed --   5.3  5  1035  167\nTest: unreliable net, one client (4B progress in majority) ...\n  ... Passed --   2.9  5   155    3\nTest: no progress in minority (4B) ...\n  ... Passed --   1.6  5   102    3\nTest: completion after heal (4B) ...\n  ... Passed --   1.3  5    67    4\nTest: partitions, one client (4B partitions, one client) ...\n  ... Passed --   6.2  5   958  155\nTest: partitions, many clients (4B partitions, many clients (4B)) ...\n  ... Passed --   6.8  5  3096  855\nTest: restarts, one client (4B restarts, one client 4B ) ...\n  ... Passed --   6.7  5   311   13\nTest: restarts, many clients (4B restarts, many clients) ...\n  ... Passed --   7.5  5  1223   95\nTest: unreliable net, restarts, many clients (4B unreliable net, restarts, many clients ) ...\n  ... Passed --   8.4  5   804   33\nTest: restarts, partitions, many clients (4B restarts, partitions, many clients) ...\n  ... Passed --  10.1  5  1308  105\nTest: unreliable net, restarts, partitions, many clients (4B unreliable net, restarts, partitions, many clients) ...\n  ... Passed --  11.9  5  1040   33\nTest: unreliable net, restarts, partitions, random keys, many clients (4B unreliable net, restarts, partitions, random keys, many clients) ...\n  ... Passed --  12.1  7  2801   93\nPASS\nok      6.5840/kvraft1  103.797s", "difficulty": "moderate", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvraft1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_23", "task_name": "problems/system_lab_23.md", "task": "# Problem Context\n## Introduction\nIn this lab you will build a fault-tolerant key/value storage service using your Raft library from [Lab 3](http://nil.csail.mit.edu/6.5840/2025/labs/lab-raft1.html). To clients, the service looks similar to the server of [Lab 2](http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvsrv1.html). However, instead of a single server, the service consists of a set of servers that use Raft to help them maintain identical databases. Your key/value service should continue to process client requests as long as a majority of the servers are alive and can communicate, in spite of other failures or network partitions. After Lab 4, you will have implemented all parts (Clerk, Service, and Raft) shown in the [diagram of Raft interactions](http://nil.csail.mit.edu/6.5840/2025/figs/kvraft.pdf).\n\nClients will interact with your key/value service through a Clerk, as in Lab 2. A Clerk implements the `Put` and `Get` methods with the same semantics as Lab 2: Puts are at-most-once and the Puts/Gets must form a linearizable history.\n\nProviding linearizability is relatively easy for a single server. It is harder if the service is replicated, since all servers must choose the same execution order for concurrent requests, must avoid replying to clients using state that isn't up to date, and must recover their state after a failure in a way that preserves all acknowledged client updates.\n\nThis lab has three parts. In part A, you will implement a replicated-state machine package, `rsm`, using your raft implementation; `rsm` is agnostic of the requests that it replicates. In part B, you will implement a replicated key/value service using `rsm`, but without using snapshots. In part C, you will use your snapshot implementation from Lab 3D, which will allow Raft to discard old log entries. Please submit each part by the respective deadline.\n\nYou should review the [extended Raft paper](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf), in particular Section 7 (but not 8). For a wider perspective, have a look at Chubby, Paxos Made Live, Spanner, Zookeeper, Harp, Viewstamped Replication, and [Bolosky et al.](http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf) Start early.\n## Getiting Started\nWe supply you with skeleton code and tests in `src/kvraft1`. The skeleton code uses the skeleton package `src/kvraft1/rsm` to replicate a server. A server must implement the `StateMachine` interface defined in `rsm` to replicate itself using `rsm`. Most of your work will be implementing `rsm` to provide server-agnostic replication. You will also need to modify `kvraft1/client.go` and `kvraft1/server.go` to implement the server-specific parts. This split allows you to re-use `rsm` in the next lab. You may be able to re-use some of your Lab 2 code (e.g., re-using the server code by copying or importing the `\"src/kvsrv1\"` package) but it is not a requirement.\n\nTo get up and running, execute the following commands. Don't forget the `git pull` to get the latest software.\n\n```\n$ cd ~/6.5840\n$ git pull\n..\n```\n\n## The Code\n\n# Your Task \nAs things stand now, your key/value server doesn't call your Raft library's `Snapshot()` method, so a rebooting server has to replay the complete persisted Raft log in order to restore its state. Now you'll modify kvserver and `rsm` to cooperate with Raft to save log space and reduce restart time, using Raft's `Snapshot()` from Lab 3D.\n\nThe tester passes `maxraftstate` to your `StartKVServer()`, which passes it to `rsm`. `maxraftstate` indicates the maximum allowed size of your persistent Raft state in bytes (including the log, but not including snapshots). You should compare `maxraftstate` to `rf.PersistBytes()`. Whenever your `rsm` detects that the Raft state size is approaching this threshold, it should save a snapshot by calling Raft's `Snapshot`. `rsm` can create this snapshot by calling the `Snapshot` method of the `StateMachine` interface to obtain a snapshot of the kvserver. If `maxraftstate` is -1, you do not have to snapshot. The `maxraftstate` limit applies to the GOB-encoded bytes your Raft passes as the first argument to `persister.Save()`.\n\nYou can find the source for the `persister` object in `tester1/persister.go`.\n\nModify your rsm so that it detects when the persisted Raft state grows too large, and then hands a snapshot to Raft. When a `rsm` server restarts, it should read the snapshot with `persister.ReadSnapshot()` and, if the snapshot's length is greater than zero, pass the snapshot to the `StateMachine`'s `Restore()` method. You complete this task if you pass TestSnapshot4C in `rsm`.\n\n```\n$ cd kvraft1/rsm\n$ go test -run TestSnapshot4C\n=== RUN   TestSnapshot4C\n  ... Passed --  9223372036.9  3   230    0\n--- PASS: TestSnapshot4C (3.88s)\nPASS\nok      6.5840/kvraft1/rsm      3.882s\n```\n\n- Think about when `rsm` should snapshot its state and what should be included in the snapshot beyond just the server state. Raft stores each snapshot in the persister object using `Save()`, along with corresponding Raft state. You can read the latest stored snapshot using `ReadSnapshot()`.\n- Capitalize all fields of structures stored in the snapshot.\n\nImplement the `kvraft1/server.go` `Snapshot()` and `Restore()` methods, which `rsm` calls. Modify `rsm` to handle applyCh messages that contain snapshots.\n\n- You may have bugs in your Raft and rsm library that this task exposes. If you make changes to your Raft implementation make sure it continues to pass all of the Lab 3 tests.\n- A reasonable amount of time to take for the Lab 4 tests is 400 seconds of real time and 700 seconds of CPU time.\n\nYour code should pass the 4C tests (as in the example here) as well as the 4A+B tests (and your Raft must continue to pass the Lab 3 tests).\n\n```\n$ go test -run 4C\nTest: snapshots, one client (4C SnapshotsRPC) ...\nTest: InstallSnapshot RPC (4C) ...\n  ... Passed --   4.5  3   241   64\nTest: snapshots, one client (4C snapshot size is reasonable) ...\n  ... Passed --  11.4  3  2526  800\nTest: snapshots, one client (4C speed) ...\n  ... Passed --  14.2  3  3149    0\nTest: restarts, snapshots, one client (4C restarts, snapshots, one client) ...\n  ... Passed --   6.8  5   305   13\nTest: restarts, snapshots, many clients (4C restarts, snapshots, many clients ) ...\n  ... Passed --   9.0  5  5583  795\nTest: unreliable net, snapshots, many clients (4C unreliable net, snapshots, many clients) ...\n  ... Passed --   4.7  5   977  155\nTest: unreliable net, restarts, snapshots, many clients (4C unreliable net, restarts, snapshots, many clients) ...\n  ... Passed --   8.6  5   847   33\nTest: unreliable net, restarts, partitions, snapshots, many clients (4C unreliable net, restarts, partitions, snapshots, many clients) ...\n  ... Passed --  11.5  5   841   33\nTest: unreliable net, restarts, partitions, snapshots, random keys, many clients (4C unreliable net, restarts, partitions, snapshots, random keys, many clients) ...\n  ... Passed --  12.8  7  2903   93\nPASS\nok      6.5840/kvraft1  83.543s\n```\n", "test_method": "cd kvraft1/rsm && go test -run 4C", "test_results": "Test: snapshots, one client (4C SnapshotsRPC) ...\nTest: InstallSnapshot RPC (4C) ...\n  ... Passed --   4.5  3   241   64\nTest: snapshots, one client (4C snapshot size is reasonable) ...\n  ... Passed --  11.4  3  2526  800\nTest: snapshots, one client (4C speed) ...\n  ... Passed --  14.2  3  3149    0\nTest: restarts, snapshots, one client (4C restarts, snapshots, one client) ...\n  ... Passed --   6.8  5   305   13\nTest: restarts, snapshots, many clients (4C restarts, snapshots, many clients ) ...\n  ... Passed --   9.0  5  5583  795\nTest: unreliable net, snapshots, many clients (4C unreliable net, snapshots, many clients) ...\n  ... Passed --   4.7  5   977  155\nTest: unreliable net, restarts, snapshots, many clients (4C unreliable net, restarts, snapshots, many clients) ...\n  ... Passed --   8.6  5   847   33\nTest: unreliable net, restarts, partitions, snapshots, many clients (4C unreliable net, restarts, partitions, snapshots, many clients) ...\n  ... Passed --  11.5  5   841   33\nTest: unreliable net, restarts, partitions, snapshots, random keys, many clients (4C unreliable net, restarts, partitions, snapshots, random keys, many clients) ...\n  ... Passed --  12.8  7  2903   93\nPASS\nok      6.5840/kvraft1  83.543s", "difficulty": "moderate", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-kvraft1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_24", "task_name": "problems/system_lab_24.md", "task": "# Problem Context\n## Introduction\nYou can either do a [final project](http://nil.csail.mit.edu/6.5840/2025/project.html) based on your own ideas, or this lab.\n\nIn this lab you'll build a key/value storage system that \"shards,\" or partitions, the keys over a set of Raft-replicated key/value server groups (shardgrps). A shard is a subset of the key/value pairs; for example, all the keys starting with \"a\" might be one shard, all the keys starting with \"b\" another, etc. The reason for sharding is performance. Each shardgrp handles puts and gets for just a few of the shards, and the shardgrps operate in parallel; thus total system throughput (puts and gets per unit time) increases in proportion to the number of shardgrps.\n\n![shardkv design](http://nil.csail.mit.edu/6.5840/2025/labs/shardkv.png)\n\nThe sharded key/value service has the components shown above. Shardgrps (shown with blue squares) store shards with keys: shardgrp 1 holds a shard storing key \"a\", and shardgrp 2 holds a shard storing key \"b\". Clients of the sharded key/value service interact with the service through a clerk (shown with a green circle), which implements `Get` and `Put` methods. To find the shardgrp for a key passed to `Put`/`Get`, the clerk gets the configuration from the kvsrv (shown with a black square), which you implemented in Lab 2. The configuration (not shown) describes the mapping from shards to shardgrps (e.g., shard 1 is served by shardgrp 3).\n\nAn administrator (i.e., the tester) uses another client, the controller (shown with a purple circle), to add/remove shardgrps from the cluster and update which shardgrp should serve a shard. The controller has one main method: `ChangeConfigTo`, which takes as argument a new configuration and changes the system from the current configuration to the new configuration; this involves moving shards to new shardgrps that are joining the system and moving shards away from shardgrps that are leaving the system. To do so the controller 1) makes RPCs (`FreezeShard`, `InstallShard`, and `DeleteShard`) to shardgrps, and 2) updates the configuration stored in kvsrv.\n\nThe reason for the controller is that a sharded storage system must be able to shift shards among shardgrps. One reason is that some shardgrps may become more loaded than others, so that shards need to be moved to balance the load. Another reason is that shardgrps may join and leave the system: new shardgrps may be added to increase capacity, or existing shardgrps may be taken offline for repair or retirement.\n\nThe main challenges in this lab will be ensuring linearizability of `Get`/`Put` operations while handling 1) changes in the assignment of shards to shardgrps, and 2) recovering from a controller that fails or is partitioned during `ChangeConfigTo`.\n\n1. `ChangeConfigTo` moves shards from one shardgrp to another. A risk is that some clients might use the old shardgrp while other clients use the new shardgrp, which could break linearizability. You will need to ensure that at most one shardgrp is serving requests for each shard at any one time.\n2. If `ChangeConfigTo` fails while reconfiguring, some shards may be inaccessible if they have started but not completed moving from one shardgrp to another. To make forward progress, the tester starts a new controller, and your job is to ensure that the new one completes the reconfiguration that the previous controller started.\n\nThis lab uses \"configuration\" to refer to the assignment of shards to shardgrps. This is not the same as Raft cluster membership changes. You don't have to implement Raft cluster membership changes.\n\nA shardgrp server is a member of only a single shardgrp. The set of servers in a given shardgrp will never change.\n\nOnly RPC may be used for interaction among clients and servers. For example, different instances of your server are not allowed to share Go variables or files.\n\nIn Part A, you will implement a working `shardctrler`, which will store and retrieve configurations in a `kvsrv`. You will also implement the `shardgrp`, replicated with your Raft `rsm` package, and a corresponding `shardgrp` clerk. The `shardctrler` talks to the `shardgrp` clerks to move shards between different groups.\n\nIn Part B, you will modify your `shardctrler` to handle failures and partitions during config changes. In Part C, you will extend your `shardctrler` to allow for concurrent controllers without interfering with each other. Finally, in Part D, you will have the opportunity to extend your solution in any way you like.\n\nThis lab's sharded key/value service follows the same general design as Flat Datacenter Storage, BigTable, Spanner, FAWN, Apache HBase, Rosebud, Spinnaker, and many others. These systems differ in many details from this lab, though, and are also typically more sophisticated and capable. For example, the lab doesn't evolve the sets of peers in each Raft group; its data and query models are simple; and so on.\n\nLab 5 will use your `kvsrv` from Lab 2, and your `rsm` and `Raft` from Lab 4. Your Lab 5 and Lab 4 must use the same `rsm` and `Raft` implementations.\n\nYou may use late hours for Part A, but you may not use late hours for Parts B-D.\n## Getiting Started\nDo a `git pull` to get the latest lab software.\n\nWe supply you with tests and skeleton code in `src/shardkv1`:\n\n- `client.go` for the shardkv clerk\n- `shardcfg` package for computing shard configurations\n- `shardgrp` package: for the shardgrp clerk and server.\n- `shardctrler` package, which contains `shardctrler.go` with methods for the controller to change a configuration (`ChangeConfigTo`) and to get a configuration (`Query`)\n\nTo get up and running, execute the following commands:\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/shardkv1\n$ go test -v\n=== RUN  TestInitQuery5A\nTest (5A): Init and Query ... (reliable network)...\n    shardkv_test.go:46: Static wrong null 0\n...\n```\n\n## The Code\n\n# Your Task \nYour first job is to implement shardgrps and the `InitConfig`, `Query`, and `ChangeConfigTo` methods when there are no failures. We have given you the code for describing a configuration, in `shardkv1/shardcfg`. Each `shardcfg.ShardConfig` has a unique identifying number, a mapping from shard number to group number, and a mapping from group number to the list of servers replicating that group. There will usually be more shards than groups (so that each group serves more than one shard), in order that load can be shifted at a fairly fine granularity.\n\nImplement these two methods in `shardctrler/shardctrler.go`:\n\n- The `InitConfig` method receives the first configuration, passed to it by the tester as a `shardcfg.ShardConfig`. `InitConfig` should store the configuration in an instance of Lab 2's `kvsrv`.\n- The `Query` method returns the current configuration; it should read the configuration from `kvsrv`, previously stored there by `InitConfig`.\n\nImplement `InitConfig` and `Query`, and store the configuration in `kvsrv`. You're done when your code passes the first test. Note this task doesn't require any shardgrps.\n\n```\n$ cd ~/6.5840/src/shardkv1\n$ go test -run TestInitQuery5A                   \nTest (5A): Init and Query ... (reliable network)...\n  ... Passed --  time  0.0s #peers 1 #RPCs     3 #Ops    0\nPASS\nok      6.5840/shardkv1 0.197s\n$\n```\n\n- Implement `InitConfig` and `Query` by storing and reading the initial configuration from `kvsrv`: use the `Get`/`Put` methods of `ShardCtrler.IKVClerk` to talk to `kvsrv`, use the `String` method of `ShardConfig` to turn a `ShardConfig` into a string that you can pass to `Put`, and use the `shardcfg.FromString()` function to turn a string into a `ShardConfig`.\n\nImplement an initial version of `shardgrp` in `shardkv1/shardgrp/server.go` and a corresponding clerk in `shardkv1/shardgrp/client.go` by copying code from your Lab 4 `kvraft` solution.\n\nImplement a clerk in `shardkv1/client.go` that uses the `Query` method to find the shardgrp for a key, and then talks to that shardgrp. You're done when your code passes the `Static` test.\n\n```\n$ cd ~/6.5840/src/shardkv1\n$ go test -run Static\nTest (5A): one shard group ... (reliable network)...\n  ... Passed --  time  5.4s #peers 1 #RPCs   793 #Ops  180\nPASS\nok      6.5840/shardkv1 5.632s\n$\n```\n\n- Copy code from your `kvraft` client.go and server.go for `Put` and `Get`, and any other code you need from `kvraft`.\n- The code in `shardkv1/client.go` provides the `Put`/`Get` clerk for the overall system: it finds out which shardgrp holds the desired key's shard by invoking the `Query` method, and then talks to the shardgrp that holds that shard.\n- Implement `shardkv1/client.go`, including its `Put`/`Get` methods. Use `shardcfg.Key2Shard()` to find the shard number for a key. The tester passes a `ShardCtrler` object to `MakeClerk` in `shardkv1/client.go`. Retrieve the current configuration using the `Query` method.\n- To put/get a key from a shardgrp, the shardkv clerk should create a shardgrp clerk for the shardgrp by calling `shardgrp.MakeClerk`, passing in the servers found in the configuration and the shardkv clerk's `ck.clnt`. Use the `GidServers()` method from `ShardConfig` to get the group for a shard.\n- `shardkv1/client.go`'s Put must return `ErrMaybe` when the reply was maybe lost, but this Put invokes `shardgrp`'s Put to talk a particular shardgrp. The inner Put can signal this with an error.\n- Upon creation, the first shardgrp (`shardcfg.Gid1`) should initialize itself to own all shards.\n\nNow you should support movement of shards among groups by implementing the `ChangeConfigTo` method, which changes from an old configuration to a new configuration. The new configuration may include new shardgrps that are not present in the old configuration, and may exclude shardgrps that were present in the old configuration. The controller should move shards (the key/value data) so that the set of shards stored by each shardgrp matches the new configuration.\n\nThe approach we suggest for moving a shard is for `ChangeConfigTo` to first \"freeze\" the shard at the source shardgrp, causing that shardgrp to reject `Put`'s for keys in the moving shard. Then, copy (install) the shard to the destination shardgrp; then delete the frozen shard. Finally, post a new configuration so that clients can find the moved shard. A nice property of this approach is that it avoids any direct interactions among the shardgrps. It also supports serving shards that are not affected by an ongoing configuration change.\n\nTo be able to order changes to the configuration, each configuration has a unique number `Num` (see `shardcfg/shardcfg.go`). The tester in Part A invokes `ChangeConfigTo` sequentially, and the configuration passed to `ChangeConfigTo` will have a `Num` one larger than the previous one; thus, a configuration with a higher `Num` is newer than one with a lower `Num`.\n\nThe network may delay RPCs, and RPCs may arrive out of order at the shardgrps. To reject old `FreezeShard`, `InstallShard`, and `DeleteShard` RPCs, they should include `Num` (see `shardgrp/shardrpc/shardrpc.go`), and shardgrps must remember the largest `Num` they have seen for each shard.\n\nImplement `ChangeConfigTo` (in `shardctrler/shardctrler.go`) and extend `shardgrp` to support freeze, install, and delete. `ChangeConfigTo` should always succeed in Part A because the tester doesn't induce failures in this part. You will need to implement `FreezeShard`, `InstallShard`, and `DeleteShard` in `shardgrp/client.go` and `shardgrp/server.go` using the RPCs in the `shardgrp/shardrpc` package, and reject old RPCs based on `Num`. You will also need modify the shardkv clerk in `shardkv1/client.go` to handle `ErrWrongGroup`, which a shardgrp should return if it isn't responsible for the shard.\n\nYou have completed this task when you pass the `JoinBasic` and `DeleteBasic` tests. These tests focus on adding shardgrps; you don't have to worry about shardgrps leaving just yet.\n\n- A shardgrp should respond with an `ErrWrongGroup` error to a client `Put`/`Get` with a key that the shardgrp isn't responsible for (i.e., for a key whose shard is not assigned to the shardgrp). You will have to modify `shardkv1/client.go` to reread the configuration and retry the `Put`/`Get`.\n- Note that you will have to run `FreezeShard`, `InstallShard`, and `DeleteShard` through your `rsm` package, just like `Put` and `Get`.\n- You can send an entire map as your state in an RPC request or reply, which may help keep the code for shard transfer simple.\n- If one of your RPC handlers includes in its reply a map (e.g. a key/value map) that's part of your server's state, you may get bugs due to races. The RPC system has to read the map in order to send it to the caller, but it isn't holding a lock that covers the map. Your server, however, may proceed to modify the same map while the RPC system is reading it. The solution is for the RPC handler to include a copy of the map in the reply.\n\nExtend `ChangeConfigTo` to handle shard groups that leave; i.e., shardgrps that are present in the current configuration but not in the new one. Your solution should pass `TestJoinLeaveBasic5A` now. (You may have handled this scenario already in the previous task, but the previous tests didn't test for shardgrps leaving.)\n\nMake your solution pass all Part A tests, which check that your sharded key/value service supports many groups joining and leaving, shardgrps restarting from snapshots, processing `Get`s while some shards are offline or involved in a configuration change, and linearizability when many clients interact with the service while the tester concurrently invokes the controller's `ChangeConfigTo` to rebalance shards.\n\n```\n$ cd ~/6.5840/src/shardkv1\n$ go test -run 5A\nTest (5A): Init and Query ... (reliable network)...\n  ... Passed --  time  0.0s #peers 1 #RPCs     3 #Ops    0\nTest (5A): one shard group ... (reliable network)...\n  ... Passed --  time  5.1s #peers 1 #RPCs   792 #Ops  180\nTest (5A): a group joins... (reliable network)...\n  ... Passed --  time 12.9s #peers 1 #RPCs  6300 #Ops  180\nTest (5A): delete ... (reliable network)...\n  ... Passed --  time  8.4s #peers 1 #RPCs  1533 #Ops  360\nTest (5A): basic groups join/leave ... (reliable network)...\n  ... Passed --  time 13.7s #peers 1 #RPCs  5676 #Ops  240\nTest (5A): many groups join/leave ... (reliable network)...\n  ... Passed --  time 22.1s #peers 1 #RPCs  3529 #Ops  180\nTest (5A): many groups join/leave ... (unreliable network)...\n  ... Passed --  time 54.8s #peers 1 #RPCs  5055 #Ops  180\nTest (5A): shutdown ... (reliable network)...\n  ... Passed --  time 11.7s #peers 1 #RPCs  2807 #Ops  180\nTest (5A): progress ... (reliable network)...\n  ... Passed --  time  8.8s #peers 1 #RPCs   974 #Ops   82\nTest (5A): progress ... (reliable network)...\n  ... Passed --  time 13.9s #peers 1 #RPCs  2443 #Ops  390\nTest (5A): one concurrent clerk reliable... (reliable network)...\n  ... Passed --  time 20.0s #peers 1 #RPCs  5326 #Ops 1248\nTest (5A): many concurrent clerks reliable... (reliable network)...\n  ... Passed --  time 20.4s #peers 1 #RPCs 21688 #Ops 10500\nTest (5A): one concurrent clerk unreliable ... (unreliable network)...\n  ... Passed --  time 25.8s #peers 1 #RPCs  2654 #Ops  176\nTest (5A): many concurrent clerks unreliable... (unreliable network)...\n  ... Passed --  time 25.3s #peers 1 #RPCs  7553 #Ops 1896\nPASS\nok      6.5840/shardkv1 243.115s\n$\n```\n\nYour solution must continue serving shards that are not affected by an ongoing configuration change.", "test_method": "cd src/shardkv1 && go test -run 5A", "test_results": "Test (5A): Init and Query ... (reliable network)...\n  ... Passed --  time  0.0s #peers 1 #RPCs     3 #Ops    0\nTest (5A): one shard group ... (reliable network)...\n  ... Passed --  time  5.1s #peers 1 #RPCs   792 #Ops  180\nTest (5A): a group joins... (reliable network)...\n  ... Passed --  time 12.9s #peers 1 #RPCs  6300 #Ops  180\nTest (5A): delete ... (reliable network)...\n  ... Passed --  time  8.4s #peers 1 #RPCs  1533 #Ops  360\nTest (5A): basic groups join/leave ... (reliable network)...\n  ... Passed --  time 13.7s #peers 1 #RPCs  5676 #Ops  240\nTest (5A): many groups join/leave ... (reliable network)...\n  ... Passed --  time 22.1s #peers 1 #RPCs  3529 #Ops  180\nTest (5A): many groups join/leave ... (unreliable network)...\n  ... Passed --  time 54.8s #peers 1 #RPCs  5055 #Ops  180\nTest (5A): shutdown ... (reliable network)...\n  ... Passed --  time 11.7s #peers 1 #RPCs  2807 #Ops  180\nTest (5A): progress ... (reliable network)...\n  ... Passed --  time  8.8s #peers 1 #RPCs   974 #Ops   82\nTest (5A): progress ... (reliable network)...\n  ... Passed --  time 13.9s #peers 1 #RPCs  2443 #Ops  390\nTest (5A): one concurrent clerk reliable... (reliable network)...\n  ... Passed --  time 20.0s #peers 1 #RPCs  5326 #Ops 1248\nTest (5A): many concurrent clerks reliable... (reliable network)...\n  ... Passed --  time 20.4s #peers 1 #RPCs 21688 #Ops 10500\nTest (5A): one concurrent clerk unreliable ... (unreliable network)...\n  ... Passed --  time 25.8s #peers 1 #RPCs  2654 #Ops  176\nTest (5A): many concurrent clerks unreliable... (unreliable network)...\n  ... Passed --  time 25.3s #peers 1 #RPCs  7553 #Ops 1896\nPASS\nok      6.5840/shardkv1 243.115s", "difficulty": "hard", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-shard1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_25", "task_name": "problems/system_lab_25.md", "task": "# Problem Context\n## Introduction\nYou can either do a [final project](http://nil.csail.mit.edu/6.5840/2025/project.html) based on your own ideas, or this lab.\n\nIn this lab you'll build a key/value storage system that \"shards,\" or partitions, the keys over a set of Raft-replicated key/value server groups (shardgrps). A shard is a subset of the key/value pairs; for example, all the keys starting with \"a\" might be one shard, all the keys starting with \"b\" another, etc. The reason for sharding is performance. Each shardgrp handles puts and gets for just a few of the shards, and the shardgrps operate in parallel; thus total system throughput (puts and gets per unit time) increases in proportion to the number of shardgrps.\n\n![shardkv design](http://nil.csail.mit.edu/6.5840/2025/labs/shardkv.png)\n\nThe sharded key/value service has the components shown above. Shardgrps (shown with blue squares) store shards with keys: shardgrp 1 holds a shard storing key \"a\", and shardgrp 2 holds a shard storing key \"b\". Clients of the sharded key/value service interact with the service through a clerk (shown with a green circle), which implements `Get` and `Put` methods. To find the shardgrp for a key passed to `Put`/`Get`, the clerk gets the configuration from the kvsrv (shown with a black square), which you implemented in Lab 2. The configuration (not shown) describes the mapping from shards to shardgrps (e.g., shard 1 is served by shardgrp 3).\n\nAn administrator (i.e., the tester) uses another client, the controller (shown with a purple circle), to add/remove shardgrps from the cluster and update which shardgrp should serve a shard. The controller has one main method: `ChangeConfigTo`, which takes as argument a new configuration and changes the system from the current configuration to the new configuration; this involves moving shards to new shardgrps that are joining the system and moving shards away from shardgrps that are leaving the system. To do so the controller 1) makes RPCs (`FreezeShard`, `InstallShard`, and `DeleteShard`) to shardgrps, and 2) updates the configuration stored in kvsrv.\n\nThe reason for the controller is that a sharded storage system must be able to shift shards among shardgrps. One reason is that some shardgrps may become more loaded than others, so that shards need to be moved to balance the load. Another reason is that shardgrps may join and leave the system: new shardgrps may be added to increase capacity, or existing shardgrps may be taken offline for repair or retirement.\n\nThe main challenges in this lab will be ensuring linearizability of `Get`/`Put` operations while handling 1) changes in the assignment of shards to shardgrps, and 2) recovering from a controller that fails or is partitioned during `ChangeConfigTo`.\n\n1. `ChangeConfigTo` moves shards from one shardgrp to another. A risk is that some clients might use the old shardgrp while other clients use the new shardgrp, which could break linearizability. You will need to ensure that at most one shardgrp is serving requests for each shard at any one time.\n2. If `ChangeConfigTo` fails while reconfiguring, some shards may be inaccessible if they have started but not completed moving from one shardgrp to another. To make forward progress, the tester starts a new controller, and your job is to ensure that the new one completes the reconfiguration that the previous controller started.\n\nThis lab uses \"configuration\" to refer to the assignment of shards to shardgrps. This is not the same as Raft cluster membership changes. You don't have to implement Raft cluster membership changes.\n\nA shardgrp server is a member of only a single shardgrp. The set of servers in a given shardgrp will never change.\n\nOnly RPC may be used for interaction among clients and servers. For example, different instances of your server are not allowed to share Go variables or files.\n\nIn Part A, you will implement a working `shardctrler`, which will store and retrieve configurations in a `kvsrv`. You will also implement the `shardgrp`, replicated with your Raft `rsm` package, and a corresponding `shardgrp` clerk. The `shardctrler` talks to the `shardgrp` clerks to move shards between different groups.\n\nIn Part B, you will modify your `shardctrler` to handle failures and partitions during config changes. In Part C, you will extend your `shardctrler` to allow for concurrent controllers without interfering with each other. Finally, in Part D, you will have the opportunity to extend your solution in any way you like.\n\nThis lab's sharded key/value service follows the same general design as Flat Datacenter Storage, BigTable, Spanner, FAWN, Apache HBase, Rosebud, Spinnaker, and many others. These systems differ in many details from this lab, though, and are also typically more sophisticated and capable. For example, the lab doesn't evolve the sets of peers in each Raft group; its data and query models are simple; and so on.\n\nLab 5 will use your `kvsrv` from Lab 2, and your `rsm` and `Raft` from Lab 4. Your Lab 5 and Lab 4 must use the same `rsm` and `Raft` implementations.\n\nYou may use late hours for Part A, but you may not use late hours for Parts B-D.\n## Getiting Started\nDo a `git pull` to get the latest lab software.\n\nWe supply you with tests and skeleton code in `src/shardkv1`:\n\n- `client.go` for the shardkv clerk\n- `shardcfg` package for computing shard configurations\n- `shardgrp` package: for the shardgrp clerk and server.\n- `shardctrler` package, which contains `shardctrler.go` with methods for the controller to change a configuration (`ChangeConfigTo`) and to get a configuration (`Query`)\n\nTo get up and running, execute the following commands:\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/shardkv1\n$ go test -v\n=== RUN  TestInitQuery5A\nTest (5A): Init and Query ... (reliable network)...\n    shardkv_test.go:46: Static wrong null 0\n...\n```\n\n## The Code\n\n# Your Task \nThe controller is a short-lived command, which an administrator invokes: it moves shards and then exits. But, it may fail or lose network connectivity while moving shards. The main task in this part of the lab is recovering from a controller that fails to complete `ChangeConfigTo`. The tester starts a new controller and invokes its `ChangeConfigTo` after partitioning the first controller; you have to modify the controller so that the new one finishes the reconfiguration. The tester calls `InitController` when starting a controller; you can modify that function to check whether an interrupted configuration change needs to be completed.\n\nA good approach to allowing a controller to finish a reconfiguration that a previous one started is to keep two configurations: a current one and a next one, both stored in the controller's kvsrv. When a controller starts a reconfiguration, it stores the next configuration. Once a controller completes the reconfiguration, it makes the next configuration the current one. Modify `InitController` to first check if there is a stored next configuration with a higher configuration number than the current one, and if so, complete the shard moves necessary to reconfigure to the next one.\n\nModify shardctrler to implement the above approach. A controller that picks up the work from a failed controller may repeat `FreezeShard`, `InstallShard`, and `Delete` RPCs; shardgrps can use `Num` to detect duplicates and reject them. You have completed this task if your solution passes the Part B tests.\n\n```\n$ cd ~/6.5840/src/shardkv1\n$ go test -run 5B\nTest (5B): Join/leave while a shardgrp is down... (reliable network)...\n  ... Passed --  time  9.2s #peers 1 #RPCs   899 #Ops  120\nTest (5B): recover controller ... (reliable network)...\n  ... Passed --  time 26.4s #peers 1 #RPCs  3724 #Ops  360\nPASS\nok      6.5840/shardkv1 35.805s\n$\n```\n\n- The tester calls `InitController` when starting a controller; you can implement recovery in that method in `shardctrler/shardctrler.go`.", "test_method": "cd src/shardkv1 && go test -run 5B", "test_results": "Test (5B): Join/leave while a shardgrp is down... (reliable network)...\n  ... Passed --  time  9.2s #peers 1 #RPCs   899 #Ops  120\nTest (5B): recover controller ... (reliable network)...\n  ... Passed --  time 26.4s #peers 1 #RPCs  3724 #Ops  360\nPASS\nok      6.5840/shardkv1 35.805s", "difficulty": "easy", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-shard1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
{"task_id": "system_lab_26", "task_name": "problems/system_lab_26.md", "task": "# Problem Context\n## Introduction\nYou can either do a [final project](http://nil.csail.mit.edu/6.5840/2025/project.html) based on your own ideas, or this lab.\n\nIn this lab you'll build a key/value storage system that \"shards,\" or partitions, the keys over a set of Raft-replicated key/value server groups (shardgrps). A shard is a subset of the key/value pairs; for example, all the keys starting with \"a\" might be one shard, all the keys starting with \"b\" another, etc. The reason for sharding is performance. Each shardgrp handles puts and gets for just a few of the shards, and the shardgrps operate in parallel; thus total system throughput (puts and gets per unit time) increases in proportion to the number of shardgrps.\n\n![shardkv design](http://nil.csail.mit.edu/6.5840/2025/labs/shardkv.png)\n\nThe sharded key/value service has the components shown above. Shardgrps (shown with blue squares) store shards with keys: shardgrp 1 holds a shard storing key \"a\", and shardgrp 2 holds a shard storing key \"b\". Clients of the sharded key/value service interact with the service through a clerk (shown with a green circle), which implements `Get` and `Put` methods. To find the shardgrp for a key passed to `Put`/`Get`, the clerk gets the configuration from the kvsrv (shown with a black square), which you implemented in Lab 2. The configuration (not shown) describes the mapping from shards to shardgrps (e.g., shard 1 is served by shardgrp 3).\n\nAn administrator (i.e., the tester) uses another client, the controller (shown with a purple circle), to add/remove shardgrps from the cluster and update which shardgrp should serve a shard. The controller has one main method: `ChangeConfigTo`, which takes as argument a new configuration and changes the system from the current configuration to the new configuration; this involves moving shards to new shardgrps that are joining the system and moving shards away from shardgrps that are leaving the system. To do so the controller 1) makes RPCs (`FreezeShard`, `InstallShard`, and `DeleteShard`) to shardgrps, and 2) updates the configuration stored in kvsrv.\n\nThe reason for the controller is that a sharded storage system must be able to shift shards among shardgrps. One reason is that some shardgrps may become more loaded than others, so that shards need to be moved to balance the load. Another reason is that shardgrps may join and leave the system: new shardgrps may be added to increase capacity, or existing shardgrps may be taken offline for repair or retirement.\n\nThe main challenges in this lab will be ensuring linearizability of `Get`/`Put` operations while handling 1) changes in the assignment of shards to shardgrps, and 2) recovering from a controller that fails or is partitioned during `ChangeConfigTo`.\n\n1. `ChangeConfigTo` moves shards from one shardgrp to another. A risk is that some clients might use the old shardgrp while other clients use the new shardgrp, which could break linearizability. You will need to ensure that at most one shardgrp is serving requests for each shard at any one time.\n2. If `ChangeConfigTo` fails while reconfiguring, some shards may be inaccessible if they have started but not completed moving from one shardgrp to another. To make forward progress, the tester starts a new controller, and your job is to ensure that the new one completes the reconfiguration that the previous controller started.\n\nThis lab uses \"configuration\" to refer to the assignment of shards to shardgrps. This is not the same as Raft cluster membership changes. You don't have to implement Raft cluster membership changes.\n\nA shardgrp server is a member of only a single shardgrp. The set of servers in a given shardgrp will never change.\n\nOnly RPC may be used for interaction among clients and servers. For example, different instances of your server are not allowed to share Go variables or files.\n\nIn Part A, you will implement a working `shardctrler`, which will store and retrieve configurations in a `kvsrv`. You will also implement the `shardgrp`, replicated with your Raft `rsm` package, and a corresponding `shardgrp` clerk. The `shardctrler` talks to the `shardgrp` clerks to move shards between different groups.\n\nIn Part B, you will modify your `shardctrler` to handle failures and partitions during config changes. In Part C, you will extend your `shardctrler` to allow for concurrent controllers without interfering with each other. Finally, in Part D, you will have the opportunity to extend your solution in any way you like.\n\nThis lab's sharded key/value service follows the same general design as Flat Datacenter Storage, BigTable, Spanner, FAWN, Apache HBase, Rosebud, Spinnaker, and many others. These systems differ in many details from this lab, though, and are also typically more sophisticated and capable. For example, the lab doesn't evolve the sets of peers in each Raft group; its data and query models are simple; and so on.\n\nLab 5 will use your `kvsrv` from Lab 2, and your `rsm` and `Raft` from Lab 4. Your Lab 5 and Lab 4 must use the same `rsm` and `Raft` implementations.\n\nYou may use late hours for Part A, but you may not use late hours for Parts B-D.\n## Getiting Started\nDo a `git pull` to get the latest lab software.\n\nWe supply you with tests and skeleton code in `src/shardkv1`:\n\n- `client.go` for the shardkv clerk\n- `shardcfg` package for computing shard configurations\n- `shardgrp` package: for the shardgrp clerk and server.\n- `shardctrler` package, which contains `shardctrler.go` with methods for the controller to change a configuration (`ChangeConfigTo`) and to get a configuration (`Query`)\n\nTo get up and running, execute the following commands:\n\n```\n$ cd ~/6.5840\n$ git pull\n...\n$ cd src/shardkv1\n$ go test -v\n=== RUN  TestInitQuery5A\nTest (5A): Init and Query ... (reliable network)...\n    shardkv_test.go:46: Static wrong null 0\n...\n```\n\n## The Code\n\n# Your Task \nIn this part of the lab you will modify the controller to allow for concurrent controllers. When a controller crashes or is partitioned, the tester will start a new controller, which must finish any work that the old controller might have in progress (i.e., finishing moving shards like in Part B). This means that several controllers may run concurrently and send RPCs to the shardgrps and the `kvsrv` that stores configurations.\n\nThe main challenge is to ensure these controllers don't step on each other. In Part A you already fenced all the shardgrp RPCs with `Num` so that old RPCs are rejected. Even if several controllers pick up the work of an old controller concurrently, one of them succeeds and the others repeat all the RPCs, the shardgrps will ignore them.\n\nThus the challenging case left is to ensure that only one controller updates the next configuration to avoid that two controllers (e.g., a partitioned one and a new one) put different configurations in the next one. To stress this scenario, the tester runs several controllers concurrently and each one computes the next configuration by reading the current configuration and updating it for a shardgrp that left or joined, and then the tester invokes `ChangeConfigTo`; thus multiple controllers may invoke `ChangeConfigTo` with different configuration with the same `Num`. You can use the version number of a key and versioned `Put`s to ensure that only one controller updates the next configuration and that the other invocations return without doing anything.\n\nModify your controller so that only one controller can post a next configuration for a configuration `Num`. The tester will start many controllers but only one should start `ChangeConfigTo` for a new configuation. You have completed this task if you pass the concurrent tests of Part C:\n\n```\n$ cd ~/6.5840/src/shardkv1\n$ go test -run TestConcurrentReliable5C\nTest (5C): Concurrent ctrlers ... (reliable network)...\n  ... Passed --  time  8.2s #peers 1 #RPCs  1753 #Ops  120\nPASS\nok      6.5840/shardkv1 8.364s\n$ go test -run TestAcquireLockConcurrentUnreliable5C\nTest (5C): Concurrent ctrlers ... (unreliable network)...\n  ... Passed --  time 23.8s #peers 1 #RPCs  1850 #Ops  120\nPASS\nok      6.5840/shardkv1 24.008s\n$\n```\n\n- See `concurCtrler` in `test.go` to see how the tester runs controllers concurrently.\n\nIn this exercise you will put recovery of an old controller together with a new controller: a new controller should perform recovery from Part B. If the old controller was partitioned during `ChangeConfigTo`, you will have to make sure that the old controller doesn't interfere with the new controller. If all the controller's updates are already properly fenced with `Num` checks from Part B, you don't have to write extra code. You have completed this task if you pass the `Partition` tests.\n\n```\n$ cd ~/6.5840/src/shardkv1\n$ go test -run Partition\nTest (5C): partition controller in join... (reliable network)...\n  ... Passed --  time  7.8s #peers 1 #RPCs   876 #Ops  120\nTest (5C): controllers with leased leadership ... (reliable network)...\n  ... Passed --  time 36.8s #peers 1 #RPCs  3981 #Ops  360\nTest (5C): controllers with leased leadership ... (unreliable network)...\n  ... Passed --  time 52.4s #peers 1 #RPCs  2901 #Ops  240\nTest (5C): controllers with leased leadership ... (reliable network)...\n  ... Passed --  time 60.2s #peers 1 #RPCs 27415 #Ops 11182\nTest (5C): controllers with leased leadership ... (unreliable network)...\n  ... Passed --  time 60.5s #peers 1 #RPCs 11422 #Ops 2336\nPASS\nok      6.5840/shardkv1 217.779s\n$\n```\n\nYou have completed implementing a highly-available sharded key/value service with many shard groups for scalability, reconfiguration to handle changes in load, and with a fault-tolerant controller; congrats!\n\nRerun all tests to check that your recent changes to the controller haven't broken earlier tests.\n\nGradescope will rerun the Lab 3A-D and Lab 4A-C tests on your submission, in addition to the 5C tests. Before submitting, double check that your solution works:\n\n```\n$ go test ./raft1\n$ go test ./kvraft1\n$ go test ./shardkv1\n```\n", "test_method": "cd src/shardkv1 && go test -run Partition", "test_results": "Test (5C): partition controller in join... (reliable network)...\n  ... Passed --  time  7.8s #peers 1 #RPCs   876 #Ops  120\nTest (5C): controllers with leased leadership ... (reliable network)...\n  ... Passed --  time 36.8s #peers 1 #RPCs  3981 #Ops  360\nTest (5C): controllers with leased leadership ... (unreliable network)...\n  ... Passed --  time 52.4s #peers 1 #RPCs  2901 #Ops  240\nTest (5C): controllers with leased leadership ... (reliable network)...\n  ... Passed --  time 60.2s #peers 1 #RPCs 27415 #Ops 11182\nTest (5C): controllers with leased leadership ... (unreliable network)...\n  ... Passed --  time 60.5s #peers 1 #RPCs 11422 #Ops 2336\nPASS\nok      6.5840/shardkv1 217.779s", "difficulty": "moderate", "link": "http://nil.csail.mit.edu/6.5840/2025/labs/lab-shard1.html", "docker_env": "xuafeng/swe-go-python:latest", "repo_url": "git://g.csail.mit.edu/6.5840-golabs-2025", "repo_path": "projects/6.5840-golabs-2025"}
